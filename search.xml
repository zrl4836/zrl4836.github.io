<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2018开放学术数据挖掘大赛总结]]></title>
    <url>%2F2019%2F01%2F01%2F2018%E5%BC%80%E6%94%BE%E5%AD%A6%E6%9C%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%A4%A7%E8%B5%9B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、赛题理解 本次竞赛[首页]针对学术领域的论文同名作者消歧问题，根据提供的AMiner中大量有挑战性的作者同名消歧数据集，表现在每个姓名关联的论文和学者数量有所增加。本次竞赛的任务是识别出哪些同名作者的论文属于同一个人。 常见的解决方案： 1. 基于规则：利用文献之间和作者关系、机构关系，通过人为设定一些规则将待消歧文献归类到相应已有类簇中。 2. 无监督聚类：按照设定的相似度度量方法，计算待消歧数据集中所有样本彼此之间的相似度，得到样本间相似度矩阵，利用计算出的相似度矩阵进行聚类。 3. 半监督聚类：利用已标注数据数据集，构建二分类训练样本，即标签为两个文献是否属于同一个作者或者两者之间的距离。通过训练样本训练模型，得到样本之间的距离函数模型。根据已训练的模型在待消歧数据集的预测结果，即样本之间的距离矩阵，运用聚类算法得到最终的聚类类簇，也就是消歧后的结果。 二、算法流程 针对该比赛，我们尝试了上述三类方法，最终采用第三种半监督聚类的方法在测试集上效果最好。具体算法流程包括：数据预处理、构造训练集、训练距离模型、聚类预测。 1. 数据预处理 1.1 作者姓名处理 将文献的所有作者姓名处理为统一的格式 1.2 机构名称处理 同名替换，通过观察数据，找出机构相同但书写或者表达不同的组织机构，构建一个同义词字典，根据字典将相同机构进行同义词替换，如下面的一些表达： 71834 Unit No.71834 Unit Unit 71834 Henan Agriculture University Henan Agricultural University State Key Laboratory for Magnetism State Key Laboratory of Magnetism Shanghai Inst. Opt./Fine Mechanics Shanghai Inst. of Opt./Fine Mech. 缩写还原，通过观察数发现，相同机构的表达中，有的包含缩写，有的为全称，所以需要对缩写单词进行复原，统一表达。下面为一些数据中常见的缩写及对应的原型： 缩写—原型 缩写—原型 缩写—原型 缩写—原型 Sch.—School Dept.—Department Coll.—College Mech.—Mechanics Inst.—Institute Univ.—University Natl.—National Chem.—Chemistry Sci.——Science Technol.——Technology Tech.——Technology Res.—Research 2. 构造训练集 根据已标注数据集，构建同一姓名下的文献对数据集，标签为两者之间的距离，包含0和1，0表示两个文献属于同一作者，1表示两文献不属于同一作者。为了处理数据不均衡问题，将正负样本按比例随机采样，构造用于训练距离函数模型的数据集。 3. 训练距离模型 该步骤主要利用2中训练集，提取特征，训练分类器模型，即主要包含特征工程和模型选择。 3.1 特征工程 机构相似度：TF-IDF的余弦相似度、Jaro–Winkler 距离 合作者相似度：合作者交集数量、合作者交并比 合作机构相似度：合作机构交并比 标题相似度：文献标题的TF-IDF余弦相似度 关键词相似度：关键词的TF-IDF余弦相似度、关键词交并比 期刊相似度：是否为同一期刊 日期：年份差距 3.2 模型选择 由于时间原因，只采用了GBDT分类器模型，没有尝试效果更佳的XGBoost或者LightGBM模型。同时，参数也没有进行进一步调优、具体设置为：n_estimators=1500, max_depth=9, max_features=’sqrt’, learning_rate=0.1。 4. 聚类预测 根据上述训练得到的距离模型，在测试数据上进行预测，得到测试集的距离关联矩阵，采用层次聚类算法对每一个姓名下的文献进行聚类，得到消歧结果。 算法流程结构如下图所示： 算法流程结构图 三、代码说明 beard： 数据对操作及一些工具类，参考自here data：数据目录 tmp：距离模型、训练pair集存储路径 sampling.py ：pair集采样生成代码 distance.py ：特征提取、距离模型训练代码 clustering.py：测试聚类输出结果 utils.py：部分工具函数 执行过程：sampling.py————》distance.py————》clustering.py（最终得分：0.7077） rule.py：基于规则方法（最终得分：0.6384） 参考文献 Louppe G , Alnatsheh H , Susik M , et al. Ethnicity sensitive author disambiguation using semi-supervised learning[J]. 2015.相关报道 http://cs.cqupt.edu.cn/info/1034/6778.htm https://cloud.tencent.com/developer/news/370291 颁奖合影留念 与王国胤院长合影]]></content>
      <tags>
        <tag>比赛</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN经典模型总结]]></title>
    <url>%2F2018%2F09%2F04%2FCNN%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、前言 CNN模型在图像处理上取得的突破，推动了CV的进一步发展，同时也掀起了人工神经网络（也称深度学习）的第三次浪潮，前两次分别为1958年的感知机模型与1986年反向传播算法。尤其是ImageNet使得CNN模型不断推陈出新，造就了一个个经典的CNN结构，其在ImageNet数据集上预训练的模型，成为了在小数据集上finetune的首选。本文简单梳理常见的CNN结构，旨在对所学的知识进行回顾，以便以后复习查看。 二、经典CNN CNN从90年代Lenet开始，用于手写字体识别，奠定了现代卷积神经网络的基石之作。沉寂十年后，12年AlexNet在ILSVRC（ImageNet Large Scale Visual Recognition Challenge）比赛分类项目获得冠军，错误率低于传统方法近10个百分点。从此CNN模型发展一发不可收拾，从ZFNet到VGG，GoogLeNet再到ResNet和最近的DenseNet，网络越来越深，架构越来越复杂，解决梯度消失、过拟合、模型退化的方法也越来越巧妙。 CNN一些计算公式：卷积核大小K，输入通道C_in，输出通道C_out，步长S，padding宽度P feature map 大小计算公式：$ \lfloor{(I-k+2 * p)}\rfloor / S + 1$ 感受野大小计算公式：最后一层感受野为前一层卷积核大小，前一层RF = (RF - 1) * S + K，不断迭代到第一层。 一层卷积参数计算：$C_{in} * K * K * C_{out} + C_{out}(偏置个数)$ 1. 入门LeNet 提出：LeCun在1998年提出，用于解决手写数字(MNIST数据集)识别的视觉任务。如今MNIST数据集是每一个CVer入门必使用的数据，相当于Deep Learning的Hello world。论文《Gradient-Based Learning Applied to Document Recognition》首页 创新：定义了CNN的基本组件：卷积、激活函数、池化、全连接，是CNN模型的鼻祖。 LeNet-5：5个可学习层的卷积神经网络（包含可训练参数的层数：2conv + 3FC） 输入层：尺寸统一归一化为32×32的单通道灰度值图像，矩阵表示为[32, 32, 1]。传统上，不将输入层视为网络层次结构之一 卷积层C1：输入图片I:32 x 32, 卷积核大小K：5 x 5, 卷积核个数O：6 卷积核矩阵表示为[1, 5, 5, 6], 步长S为1， padding为VALID 输出featuremap大小计算公式：32-5+1 = 28，神经元数量：28 * 28 *6 可训练参数：（5 *5+1) * 6（每个滤波器5 *5=25个unit参数和一个bias参数，一共6个滤波器） 池化层S2：输入：28 *28, 采样区域：2 * 2，步长S为2。 输出大小：14 * 14 其余层计算方式同上。 keras实现：激活函数改为最新的relu。12345678910def LeNet(): model = Sequential() model.add(Conv2D(filters=6, kernel_size=(5,5), padding='valid', input_shape=(32, 32, 1), activation='relu')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='relu')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dense(120, activation='relu')) model.add(Dense(84, activation='relu')) model.add(Dense(10, activation='softmax')) 2. 开创AlexNet 提出：AlexNet在2012年取得ImageNet竞赛冠军，开创深度学习和卷积神经网络研究的新局面。论文《ImageNet Classification with Deep Convolution Neural Networks》 创新：1. 更深的网络，2. 数据增广， 3. ReLu激活函数， 4. Dropout， 5 LRN（被证明没啥用，被后续结构抛弃），5. 两个GPU并行训练 AlexNet总共包含8层，5层卷积层 + 3层全连接层，最终softmax输出是1000类 1. AlexNet共8层，相比于LeNet-5层数有所增加，卷积神经网络总的流程没有变化。 2. AlexNet针对ImageNet数据集共1000类进行分类，输入图片大小为256 x 256 x 3的三通道彩色图片， 为了提高模型泛化能力，同时避免过拟合，作者使用了随机裁剪的思路对原来256×256的图像进行随机裁剪，得到尺寸为224×224x3的图像，并进行随机翻转，相当于把训练集扩大了32x32x2=2048倍，再输入到网络训练。 3. 使用ReLU代替Sigmoid来加快SGD的收敛速度，同时避免梯度消失或梯度爆炸的问题。 4. Dropout原理类似于浅层学习算法的中集成算法，该方法通过让全连接层的神经元（该模型在前两个全连接层引入Dropout）以一定的概率失去活性（比如0.5）失活的神经元不再参与前向和反向传播，相当于约有一半的神经元不再起作用，前向传播时，神经元的输出将被设置为0，在误差反向传播时，传播到该神经元的值也为0。而在下次迭代中，所有神经元将会根据keep_prob被重新随机dropout。相当于每次迭代，神经网络的拓扑结构都会有所不同，这就会迫使神经网络不会过度依赖某几个神经元或者说某些特征，因此，神经元会被迫去学习更具有鲁棒性的特征。在测试的时候，所有的keep_prob都为1.0，也即关闭dropout，并让所有神经元的输出乘0.5。即：$w_{test}^{(l)} = pW^{(l)}$(主要是为了使得测试数据和训练数据是大致一样的。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。)，Dropout的引用，有效缓解了模型的过拟合，dropout只用于全连接层。 没有Dropout的网络计算公式： 采用Dropout的网络计算公式： 5. Hinton等人认为LRN层模仿生物神经系统的侧抑制机制，对局部神经元的活动创建竞争机制，使得响应比较大的值相对更大，提高模型泛化能力。但是，后来的论文比如Very Deep Convolution Networks for Large-Scale Image Recognition（也就是提出VGG网络的文章）中证明，LRN对CNN并没有什么作用，反而增加了计算复杂度，因此，这一技术也不再使用了。 输入层：[227, 227, 3], (论文图224x224x3)有误 卷积层：卷积核大小K=11，个数：96，表示为: [3, 11, 11, 96], 步长S=4，padding：VALID，输出大小：(227 - 11) / 4 + 1 = 55， tensor维度：[55, 55, 96] 池化层：核大小为3，步长为2,(overlapping pooling，论文中提到，使用这种池化可以一定程度上减小过拟合现象。)，输出：(55 - 3) / 2 + 1 = 27，tensor维度：[27, 27, 96] 其余层计算方式同上。 keras实现：12345678910111213141516def AlexNet(): model = Sequential() model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(227,227,3),padding='valid',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2))) model.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2))) model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2))) model.add(Flatten()) model.add(Dense(4096,activation='relu')) model.add(Dropout(0.5)) model.add(Dense(4096,activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1000,activation='softmax')) 3. 调参ZFNet 提出：2013年ImageNet冠军模型，基本就是在AlexNet基础上进行了一些细节的改动，网络结构上并没有太大的突破。论文名称：《Visualizing and Understanding Convolutional Networks 》 创新：提出了一个新颖的可视化技术，“理解”中间的特征层和最后的分类器层，并且找到改进神经网络的结构的方法。 Conv1从步长为4，大小为11的卷积核改变为步长为2，大小为7的卷积核，（224-7）/2 +1 = 110 Conv3,4,5由384,384,256改变为512,1024,512。 12345678910111213141516def ZFNet(): model = Sequential() model.add(Conv2D(96,(7,7),strides=(2,2),input_shape=(224,224,3),padding='valid',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2))) model.add(Conv2D(256,(5,5),strides=(2,2),padding='same',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2))) model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2))) model.add(Flatten()) model.add(Dense(4096,activation='relu')) model.add(Dropout(0.5)) model.add(Dense(4096,activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1000,activation='softmax')) 4. 加宽GoogleNet 提出：2014的ImageNet分类任务上击败了VGG-Nets夺得冠军，除了单纯的加深网络深度（22层），参考Network in Network思想，引入了Inception结构代替了单纯的卷积+激活的传统操作，拓宽了网络的宽度。论文《Going deeper with convolutions》 创新：利用inception结构，在不增加计算负载的情况下，增加网络的宽度和深度。 1x1卷积的使用来增加网络深度，同时达到降维和限制网络尺寸的作用。 对inception结构中的所有滤波器都进行学习，固定的多个Gabor滤波器来进行多尺度处理的方法。 运用Hebbian原理，后面的全连接层全部替换为简单的全局平均pooling，把全连接的网络变为稀疏连接。 结构中包含3个LOSS单元，为了帮助网络的收敛，在中间层加入辅助计算的LOSS单元，让低层的特征也有很好的区分能力，这两个辅助LOSS单元的计算被乘以0.3，然后和最后的LOSS相加作为最终的损失函数来训练网络。 Inception结构：卷积stride都是1，为了保持特征响应图大小一致，都用了零填充，每个卷积层后面都立刻接了个ReLU层。 1. 通过3×3的池化、以及1×1、3×3和5×5这三种不同尺度的卷积核，一共4种方式对输入的特征响应图做了特征提取。 2. 为了降低计算量。同时让信息通过更少的连接传递以达到更加稀疏的特性，采用1×1卷积核来实现降维（通道压缩 ＋ 参数较少），同时还增加了ReLU非线性激层。 整个网络的参数表：可以看到参数总量并不大，但是计算次数是非常大的。 GoogLeNet的Keras实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def Conv2d_BN(x, nb_filter,kernel_size, padding='same',strides=(1,1),name=None): if name is not None: bn_name = name + '_bn' conv_name = name + '_conv' else: bn_name = None conv_name = None x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x) x = BatchNormalization(axis=3,name=bn_name)(x) return xdef Inception(x,nb_filter): branch1x1 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None) branch3x3 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None) branch3x3 = Conv2d_BN(branch3x3,nb_filter,(3,3), padding='same',strides=(1,1),name=None) branch5x5 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None) branch5x5 = Conv2d_BN(branch5x5,nb_filter,(1,1), padding='same',strides=(1,1),name=None) branchpool = MaxPooling2D(pool_size=(3,3),strides=(1,1),padding='same')(x) branchpool = Conv2d_BN(branchpool,nb_filter,(1,1),padding='same',strides=(1,1),name=None) x = concatenate([branch1x1,branch3x3,branch5x5,branchpool],axis=3) return xdef GoogLeNet(): inpt = Input(shape=(224,224,3)) #padding = 'same'，填充为(步长-1）/2,还可以用ZeroPadding2D((3,3)) x = Conv2d_BN(inpt,64,(7,7),strides=(2,2),padding='same') x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) x = Conv2d_BN(x,192,(3,3),strides=(1,1),padding='same') x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) x = Inception(x,64)#256 x = Inception(x,120)#480 x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) x = Inception(x,128)#512 x = Inception(x,128) x = Inception(x,128) x = Inception(x,132)#528 x = Inception(x,208)#832 x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) x = Inception(x,208) x = Inception(x,256)#1024 x = AveragePooling2D(pool_size=(7,7),strides=(7,7),padding='same')(x) x = Dropout(0.4)(x) x = Dense(1000,activation='relu')(x) x = Dense(1000,activation='softmax')(x) model = Model(inpt,x,name='inception') return model 5. 堆砌VggNet 提出：VGGNet是由牛津大学VGG（Visual Geometry Group）提出，是2014年ImageNet竞赛定位任务的第一名和分类任务的第二名的中的基础网络。相当于加深版的AlexNet，论文《Very Deep Convolutional Networks for Large-Scale Visual Recognition》。 创新：VGG采用Pre-training的方式，这种方式在经典的神经网络中经常见得到，就是先训练一部分小网络，然后再确保这部分网络稳定之后，再在这基础上逐渐加深。表1从左到右体现的就是这个过程，并且当网络处于D阶段的时候，效果是最优的，因此D阶段的网络也就是VGG-16了！E阶段得到的网络就是VGG-19了！VGG-16的16指的是conv+fc的总层数是16，是不包括max pool的层数！ 所有卷积层有相同的配置，更小的卷积核大小，更小的步长，即卷积核大小为3x3，步长为1，填充为1；共有5个最大池化层，大小都为2x2，步长为2；共有三个全连接层，前两层都有4096通道，第三层共1000路及代表1000个标签类别；最后一层为softmax层；所有隐藏层后都带有ReLU非线性激活函数； 为什么用3x3的滤波器尺寸？ 1. 这是能捕捉到各个方向(8领域)的最小尺寸。 2. 大的感受野可以通过3x3卷积层堆叠而得，如5x5的感受野可以由两层3x3卷积得到。 3. 层数增加，同时增加了Relu层，增强模型非线性拟合能力，提升了判别函数的识别能力。 4. 同样的感受野减少了参数。如3个3x3的卷积核，通道数为C，则参数为$3*(3*3*C*C)=27C^2$，而一个7x7的卷积核，通道数也为C，则参数为$(7 *7*C*C)=49C^2$。 下面这个图就是VGG-16的网络结构： VGG-16的结构非常整洁，深度较AlexNet深得多，里面包含多个conv-&gt;conv-&gt;max_pool这类的结构,VGG的卷积层都是same的卷积，即卷积过后的输出图像的尺寸与输入是一致的，它的下采样完全是由max pooling来实现。VGG网络后接3个全连接层，filter的个数（卷积后的输出通道数）从64开始，然后没接一个pooling后其成倍的增加，128、512，VGG的注意贡献是使用小尺寸的filter，及有规则的卷积-池化操作。 VGG-16的Keras实现：12345678910111213141516171819202122232425262728293031323334def VGG_16(): model = Sequential() model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(224,224,3),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Conv2D(128,(3,2),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform')) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dense(4096,activation='relu')) model.add(Dropout(0.5)) model.add(Dense(4096,activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1000,activation='softmax')) return model 6. 更深ResNet 提出：2015年何恺明推出的ResNet在ISLVRC和COCO上横扫所有选手，获得冠军。ResNet在网络结构上做了大创新，而不再是简单的堆积层数，ResNet在卷积神经网络的新思路，绝对是深度学习发展历程上里程碑式的事件。 创新：层数非常深，已经超过百层 引入残差单元来解决退化问题 两种mapping：1. identity mapping,称为shortcut connection 2. residual mapping 最后输出：$y=F(x)+x$ 如果F(x)和x的channel个数不同怎么办？相同时：$y = F(x) + x$, 不相同时：$y = F(x) + Wx$,其中W是卷积操作，用来调整x的channel维度。 残差模块：左图是常规残差模块，有两个3×3卷积核卷积核组成，但是随着网络进一步加深，这种残差结构在实践中并不是十分有效。针对这问题，右图的“瓶颈残差模块”（bottleneck residual block）可以有更好的效果，它依次由1×1、3×3、1×1这三个卷积层堆积而成，这里的1×1的卷积能够起降维或升维的作用，从而令3×3的卷积可以在相对较低维度的输入上进行，以达到提高计算效率的目的。 模型结构：VGG19, plain net, resnet(shortcut connection实线为通道数相同，虚线为通道数不同) ResNet50和ResNet101：检测，分割，识别等领域常使用的模型。 ResNet101:101层可学习层：3 + 4 + 23 + 3 = 33个building block，每个block为3层，所以有33 x 3 = 99层，最后有个fc层(用于分类)，所以1（第一层） + 99 + 1 = 101层。 ResNet101在Faster RCNN中RPN和Fast RCNN的使用： ResNet-50的Keras实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def Conv2d_BN(x, nb_filter, kernel_size, strides=(1, 1), padding='same', name=None): if name is not None: bn_name = name + '_bn' conv_name = name + '_conv' else: bn_name = None conv_name = None x = Conv2D(nb_filter, kernel_size, padding=padding, strides=strides, activation='relu', name=conv_name)(x) x = BatchNormalization(axis=3, name=bn_name)(x) return xdef Conv_Block(inpt, nb_filter, kernel_size, strides=(1, 1), with_conv_shortcut=False): x = Conv2d_BN(inpt, nb_filter=nb_filter[0], kernel_size=(1, 1), strides=strides, padding='same') x = Conv2d_BN(x, nb_filter=nb_filter[1], kernel_size=(3, 3), padding='same') x = Conv2d_BN(x, nb_filter=nb_filter[2], kernel_size=(1, 1), padding='same') if with_conv_shortcut: shortcut = Conv2d_BN(inpt, nb_filter=nb_filter[2], strides=strides, kernel_size=kernel_size) x = add([x, shortcut]) return x else: x = add([x, inpt]) return xdef ResNet50(): inpt = Input(shape=(224, 224, 3)) x = ZeroPadding2D((3, 3))(inpt) x = Conv2d_BN(x, nb_filter=64, kernel_size=(7, 7), strides=(2, 2), padding='valid') x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x) x = Conv_Block(x, nb_filter=[64, 64, 256], kernel_size=(3, 3), strides=(1, 1), with_conv_shortcut=True) x = Conv_Block(x, nb_filter=[64, 64, 256], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[64, 64, 256], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[128, 128, 512], kernel_size=(3, 3), strides=(2, 2), with_conv_shortcut=True) x = Conv_Block(x, nb_filter=[128, 128, 512], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[128, 128, 512], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[128, 128, 512], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[256, 256, 1024], kernel_size=(3, 3), strides=(2, 2), with_conv_shortcut=True) x = Conv_Block(x, nb_filter=[256, 256, 1024], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[256, 256, 1024], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[256, 256, 1024], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[256, 256, 1024], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[256, 256, 1024], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[512, 512, 2048], kernel_size=(3, 3), strides=(2, 2), with_conv_shortcut=True) x = Conv_Block(x, nb_filter=[512, 512, 2048], kernel_size=(3, 3)) x = Conv_Block(x, nb_filter=[512, 512, 2048], kernel_size=(3, 3)) x = AveragePooling2D(pool_size=(7, 7))(x) x = Flatten()(x) x = Dense(1000, activation='softmax')(x) model = Model(inputs=inpt, outputs=x) return model 7. 密集DenseNet 提出：CVPR2017的oral，作者从feature入手，将feature利用到了极致，可以说DenseNet吸收了ResNet最精华的部分，并在此上做了更加创新的工作，使得网络性能进一步提升。论文：《Densely Connected Convolutional Networks》github 创新：减轻了vanishing-gradient（梯度消失） 加强了feature的传递 更有效地利用了feature 一定程度上较少了参数数量 DenseNet结构： 如上结构，DenseNet主要包含DenseBlock和transition layer两个组成模块， Dense Block结构：在传统的卷积神经网络中，如果你有L层，那么就会有L个连接，但是在DenseNet中，会有L(L+1)/2个连接，因为其每一层的输入都来自前面所有层的输出。与ResNet中的BottleNeck基本一致：BN-ReLU-Conv(1×1)-BN-ReLU-Conv(3×3) ，而一个DenseNet则由多个这种block组成。每个DenseBlock的之间层称为transition layers。 transition layers，由BN−&gt;Conv(1×1)−&gt;averagePooling(2×2)组成。 DenseNet-121的Keras实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160def DenseNet121(nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, classes=1000, weights_path=None): '''Instantiate the DenseNet 121 architecture, # Arguments nb_dense_block: number of dense blocks to add to end growth_rate: number of filters to add per dense block nb_filter: initial number of filters reduction: reduction factor of transition blocks. dropout_rate: dropout rate weight_decay: weight decay factor classes: optional number of classes to classify images weights_path: path to pre-trained weights # Returns A Keras model instance. ''' eps = 1.1e-5 # compute compression factor compression = 1.0 - reduction # Handle Dimension Ordering for different backends global concat_axis if K.image_dim_ordering() == 'tf': concat_axis = 3 img_input = Input(shape=(224, 224, 3), name='data') else: concat_axis = 1 img_input = Input(shape=(3, 224, 224), name='data') # From architecture for ImageNet (Table 1 in the paper) nb_filter = 64 nb_layers = [6,12,24,16] # For DenseNet-121 # Initial convolution x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input) x = Convolution2D(nb_filter, 7, 7, subsample=(2, 2), name='conv1', bias=False)(x) x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv1_bn')(x) x = Scale(axis=concat_axis, name='conv1_scale')(x) x = Activation('relu', name='relu1')(x) x = ZeroPadding2D((1, 1), name='pool1_zeropadding')(x) x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x) # Add dense blocks for block_idx in range(nb_dense_block - 1): stage = block_idx+2 x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay) # Add transition_block x = transition_block(x, stage, nb_filter, compression=compression, dropout_rate=dropout_rate, weight_decay=weight_decay) nb_filter = int(nb_filter * compression) final_stage = stage + 1 x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate, weight_decay=weight_decay) x = BatchNormalization(epsilon=eps, axis=concat_axis, name='conv'+str(final_stage)+'_blk_bn')(x) x = Scale(axis=concat_axis, name='conv'+str(final_stage)+'_blk_scale')(x) x = Activation('relu', name='relu'+str(final_stage)+'_blk')(x) x = GlobalAveragePooling2D(name='pool'+str(final_stage))(x) x = Dense(classes, name='fc6')(x) x = Activation('softmax', name='prob')(x) model = Model(img_input, x, name='densenet') if weights_path is not None: model.load_weights(weights_path) return modeldef conv_block(x, stage, branch, nb_filter, dropout_rate=None, weight_decay=1e-4): '''Apply BatchNorm, Relu, bottleneck 1x1 Conv2D, 3x3 Conv2D, and option dropout # Arguments x: input tensor stage: index for dense block branch: layer index within each dense block nb_filter: number of filters dropout_rate: dropout rate weight_decay: weight decay factor ''' eps = 1.1e-5 conv_name_base = 'conv' + str(stage) + '_' + str(branch) relu_name_base = 'relu' + str(stage) + '_' + str(branch) # 1x1 Convolution (Bottleneck layer) inter_channel = nb_filter * 4 x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x1_bn')(x) x = Scale(axis=concat_axis, name=conv_name_base+'_x1_scale')(x) x = Activation('relu', name=relu_name_base+'_x1')(x) x = Convolution2D(inter_channel, 1, 1, name=conv_name_base+'_x1', bias=False)(x) if dropout_rate: x = Dropout(dropout_rate)(x) # 3x3 Convolution x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_x2_bn')(x) x = Scale(axis=concat_axis, name=conv_name_base+'_x2_scale')(x) x = Activation('relu', name=relu_name_base+'_x2')(x) x = ZeroPadding2D((1, 1), name=conv_name_base+'_x2_zeropadding')(x) x = Convolution2D(nb_filter, 3, 3, name=conv_name_base+'_x2', bias=False)(x) if dropout_rate: x = Dropout(dropout_rate)(x) return xdef transition_block(x, stage, nb_filter, compression=1.0, dropout_rate=None, weight_decay=1E-4): ''' Apply BatchNorm, 1x1 Convolution, averagePooling, optional compression, dropout # Arguments x: input tensor stage: index for dense block nb_filter: number of filters compression: calculated as 1 - reduction. Reduces the number of feature maps in the transition block. dropout_rate: dropout rate weight_decay: weight decay factor ''' eps = 1.1e-5 conv_name_base = 'conv' + str(stage) + '_blk' relu_name_base = 'relu' + str(stage) + '_blk' pool_name_base = 'pool' + str(stage) x = BatchNormalization(epsilon=eps, axis=concat_axis, name=conv_name_base+'_bn')(x) x = Scale(axis=concat_axis, name=conv_name_base+'_scale')(x) x = Activation('relu', name=relu_name_base)(x) x = Convolution2D(int(nb_filter * compression), 1, 1, name=conv_name_base, bias=False)(x) if dropout_rate: x = Dropout(dropout_rate)(x) x = AveragePooling2D((2, 2), strides=(2, 2), name=pool_name_base)(x) return xdef dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True): ''' Build a dense_block where the output of each conv_block is fed to subsequent ones # Arguments x: input tensor stage: index for dense block nb_layers: the number of layers of conv_block to append to the model. nb_filter: number of filters growth_rate: growth rate dropout_rate: dropout rate weight_decay: weight decay factor grow_nb_filters: flag to decide to allow number of filters to grow ''' eps = 1.1e-5 concat_feat = x for i in range(nb_layers): branch = i+1 x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate, weight_decay) concat_feat = merge([concat_feat, x], mode='concat', concat_axis=concat_axis, name='concat_'+str(stage)+'_'+str(branch)) if grow_nb_filters: nb_filter += growth_rate return concat_feat, nb_filter 三、参考链接 CNN网络架构演进：从LeNet到DenseNet]]></content>
      <tags>
        <tag>总结</tag>
        <tag>面试</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BAT机器学习面试题库(201-300)]]></title>
    <url>%2F2018%2F09%2F01%2FBAT%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E5%BA%93%EF%BC%88200-300%EF%BC%89%2F</url>
    <content type="text"><![CDATA[为了求职笔试面试，需恶补基础、算法原理，于是仔细研读了七月在线发布的BAT机器学习面试1000题系列，也添加了一些自己的理解或来自其他博客的答案，以下内容均来自BAT机器学习面试1000题系列。该文为本人的阅读笔记，主要是为了记忆和自查。 201. 为什么网络够深(Neurons 足够多)的时候，总是可以避开较差Local Optima？ 参见：The Loss Surfaces of Multilayer Networks 202. 机器学习中的正负样本 在分类问题中，这个问题相对好理解一点，比如人脸识别中的例子，正样本很好理解，就是人脸的图片，负样本的选取就与问题场景相关，具体而言，如果你要进行教室中学生的人脸识别，那么负样本就是教室的窗子、墙等等，也就是说，不能是与你要研究的问题毫不相关的乱七八糟的场景图片，这样的负样本并没有意义。负样本可以根据背景生成，有时候不需要寻找额外的负样本。一般3000-10000的正样本需要5，000,000-100,000,000的负样本来学习，在互金领域一般在入模前将正负比例通过采样的方法调整到3:1-5:1。 203. 机器学习中，有哪些特征选择的工程方法？ 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已 1. 计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了； 2. 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征； 3. 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验； 4. 训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型； 5. 通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。 6. 通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 204. 在一个n维的空间中， 最好的检测outlier(离群点)的方法是（）机器学习 ML基础 易 A. 作正态分布概率图 B. 作盒形图 C. 马氏距离 D. 作散点图 马氏距离是基于卡方分布的，度量多元outlier离群点的统计方法。 有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为： $D(x) = \sqrt{(x - \mu)^T S^{-1} (x - \mu)}$ (协方差矩阵中每个元素是各个矢量元素之间的协方差Cov(X,Y)，Cov(X,Y) = E{ [X-E(X)] [Y-E(Y)]}，其中E为数学期望) 而其中向量Xi与Xj之间的马氏距离定义为： D(x_I,y_i) = \sqrt{(x_i - y_i)^TS^{-1}(x_i - y_i)} 若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了： D(x_I,y_i) = \sqrt{(x_i - y_i)^T(x_i - y_i)} 也就是欧氏距离了。 若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。 (2)马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。 更多请详见：这里和”各种距离“。 205. 对数几率回归（logistics regression）和一般回归分析有什么区别？机器学习 ML基础 易 A. 对数几率回归是设计用来预测事件可能性的 B. 对数几率回归可以用来度量模型拟合程度 C. 对数几率回归可以用来估计回归系数 D. 以上所有 A: 对数几率回归其实是设计用来解决分类问题的这篇文章 B: 对数几率回归可以用来检验模型对数据的拟合度 C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。 206. bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）机器学习 ML模型 易 A. 有放回地从总共M个特征中抽样m个特征 B. 无放回地从总共M个特征中抽样m个特征 C. 有放回地从总共N个样本中抽样n个样本 D. 无放回地从总共N个样本中抽样n个样本 答案：C。boostrap是提鞋自举的意思(武侠小说作者所说的左脚踩右脚腾空而起). 它的过程是对样本(而不是特征)进行有放回的抽样, 抽样次数等同于样本总数. 这个随机抽样过程决定了最终抽样出来的样本, 去除重复之后, 占据原有样本的1/e比例。 207. “过拟合”只在监督学习中出现，在非监督学习中，没有“过拟合”，这是（）机器学习 ML基础 易 A. 对的 B. 错的 答案：我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score） 208. 对于k折交叉验证, 以下对k的说法正确的是（）机器学习 ML基础 易 A. k越大, 不一定越好, 选择大的k会加大评估时间 B. 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集) C. 在选择k时, 要最小化数据集之间的方差 D. 以上所有 答案：k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差. 如果不明白bias和variance的概念, 务必参考下面链接: Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning Understanding the Bias-Variance Tradeoff 209. 回归模型中存在多重共线性, 你如何解决这个问题？机器学习 ML模型 中 1. 去除这两个共线性变量 2. 我们可以先去除一个共线性变量 3. 计算VIF(方差膨胀因子), 采取相应措施 4. 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归. 以下哪些是对的: A. 1 B. 2 C. 2和3 D. 2, 3和4 解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值&lt;=4说明相关性不是很高, VIF值&gt;=10说明相关性较高. 我们也可以用岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。 210. 模型的高bias是什么意思, 我们如何降低它 ？机器学习 ML基础 易 A. 在特征空间中减少特征 B. 在特征空间中增加特征 C. 增加数据点 D. B和C E. 以上所有 bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 ! 211. 训练决策树模型, 属性节点的分裂, 具有最大信息增益的图是下图的哪一个（）机器学习 ML模型 易 A. Outlook B. Humidity C. Windy D. Temperature 答案: A 信息增益, 增加平均子集纯度, 详细研究, 请戳下面链接: A Complete Tutorial on Tree Based Modeling from Scratch (in R &amp; Python) Lecture 4 Decision Trees (2): Entropy, Information Gain, Gain Ratio 212. 对于信息增益, 决策树分裂节点, 下面说法正确的是（）机器学习 ML模型 易 1. 纯度高的节点需要更多的信息去区分 2. 信息增益可以用”1比特-熵”获得 3. 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的 A. 1 B. 2 C.2和3 D. 所有以上 详细研究, 请戳下面链接: A Complete Tutorial on Tree Based Modeling from Scratch (in R &amp; Python) Lecture 4 Decision Trees (2): Entropy, Information Gain, Gain Ratio 213. 如果SVM模型欠拟合, 以下方法哪些可以改进模型 （） 机器学习 ML模型 中 A. 增大惩罚参数C的值 B. 减小惩罚参数C的值 C. 减小核系数(gamma参数) 如果SVM模型欠拟合, 我们可以调高参数C的值, 使得模型复杂度上升.LibSVM中，SVM的目标函数是： 而gamma参数是你选择径向基函数作为kernel后,该函数自带的一个参数.隐含地决定了数据映射到新的特征空间后的分布. gamma参数与C参数无关. gamma参数越高, 模型越复杂. 214. 下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3, 下面大小比较正确的是： A. g1 &gt; g2 &gt; g3 B. g1 = g2 = g3 C. g1 &lt; g2 &lt; g3 D. g1 &gt;= g2 &gt;= g3E. g1 &lt;= g2 &lt;= g3 215. 假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值, 那么现在关于模型说法, 正确的是 : 1. 模型分类的召回率会降低或不变 2. 模型分类的召回率会升高 3. 模型分类准确率会升高或不变 4. 模型分类准确率会降低 A. 1 B. 2 C.1和3 D. 2和4 E. 以上都不是 这篇文章讲述了阈值对准确率和召回率影响 : Confidence Splitting Criterions Can Improve Precision And Recall in Random Forest Classifiers 216. “点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是 : A. 模型预测准确率已经很高了, 我们不需要做什么了 B. 模型预测准确率不高, 我们需要做点什么改进模型 C. 无法下结论 D. 以上都不对 99%的预测准确率可能说明, 你预测的没有点进去的人很准确 (因为有99%的人是不会点进去的, 这很好预测). 不能说明你的模型对点进去的人预测准确, 所以, 对于这样的非平衡数据集, 我们要把注意力放在小部分的数据上, 即那些点击进去的人. 217. 使用k=1的knn算法, 下图二类分类问题, “+” 和 “o” 分别代表两个类, 那么, 用仅拿出一个测试样本的交叉验证方法, 交叉验证的错误率是多少： A. 0% B. 100% C. 0% 到 100 D. 以上都不是 knn算法就是, 在样本周围看k个样本, 其中大多数样本的分类是A类, 我们就把这个样本分成A类. 显然, k=1 的knn在上图不是一个好选择, 分类的错误率始终是100% 219. 我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以： A. 增加树的深度 B. 增加学习率 (learning rate) C. 减少树的深度 D. 减少树的数量 增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间. 220. 决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法) 决策树只有一棵树, 不是随机森林.对于神经网络的说法, 下面正确的是 : 1. 增加神经网络层数, 可能会增加测试数据集的分类错误率 2. 减少神经网络层数, 总是能减小测试数据集的分类错误率 3. 增加神经网络层数, 总是能减小训练数据集的分类错误率 A. 1 B. 1 和 3 C. 1 和 2 D. 2 221. 假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？ A. 设C=1 B. 设C=0 C. 设C=无穷大 D. 以上都不对 C无穷大保证了所有的线性不可分都是可以忍受的. 222. 训练完SVM模型后, 不是支持向量的那些样本我们可以丢掉, 也可以继续分类: A. 正确 B. 错误 223. SVM模型中, 真正影响决策边界的是支持向量，以下哪些算法, 可以用神经网络去构造: 1. KNN 2. 线性回归 3. 对数几率回归 A. 1和 2 B. 2 和 3 C. 1, 2 和 3 D. 以上都不是 1. KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙 2. 最简单的神经网络, 感知器, 其实就是线性回归的训练 3. 我们可以用一层的神经网络构造对数几率回归 224. 请选择下面可以应用隐马尔科夫(HMM)模型的选项: A. 基因序列数据集 B. 电影浏览数据集 C. 股票市场数据集 D. 所有以上 只要是和时间序列问题有关的 , 都可以试试HMM 225. 我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练: A. 我们随机抽取一些样本, 在这些少量样本之上训练 B. 我们可以试用在线机器学习算法 C. 我们应用PCA算法降维, 减少特征数 D. B 和 C E. A 和 B F. 以上所有 226. 我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案 : 1. 使用前向特征选择方法 2. 使用后向特征排除方法 3. 我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征. 4. 查看相关性表, 去除相关性最高的一些特征 A. 1 和 2 B. 2, 3和4 C. 1, 2和4 D. All 1.前向特征选择方法和后向特征排除方法是我们特征选择的常用方法 2.如果前向特征选择方法和后向特征排除方法在大数据上不适用, 可以用这里第三种方法. 3.用相关性的度量去删除多余特征, 也是一个好方法 所有D是正确的 227. 对于随机森林和GradientBoosting Trees, 下面说法正确的是: 1.在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting Trees中的单个树之间是没有依赖的。 2.这两个模型都使用随机特征子集, 来生成许多单个的树。 3.我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的, GradientBoosting Trees训练模型的表现总是比随机森林好。 A. 2 B. 1 and 2 C. 1, 3 and 4 D. 2 and 4 1.随机森林是基于bagging的, 而Gradient Boosting trees是基于boosting的, 所有说反了,在随机森林的单个树中, 树和树之间是没有依赖的, 而GradientBoosting Trees中的单个树之间是有依赖关系. 2.这两个模型都使用随机特征子集, 来生成许多单个的树. 228. 对于PCA(主成分分析)转化过的特征, 朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 : A. 正确的 B. 错误的 这个说法是错误的, 首先, “不依赖”和”不相关”是两回事, 其次, 转化过的特征, 也可能是相关的 229. 对于PCA说法正确的是 : 1. 我们必须在使用PCA前规范化数据 2. 我们应该选择使得模型有最大variance的主成分 3. 我们应该选择使得模型有最小variance的主成分 4. 我们可以使用PCA在低维度上做数据可视化 A. 1, 2 and 4 B. 2 and 4 C. 3 and 4 D. 1 and 3 E. 1, 3 and 4 1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分). 2）我们总是应该选择使得模型有最大variance的主成分 3）有时在低维度上左图是需要PCA的降维帮助的 230. 对于下图, 最好的主成分选择是多少 ? : A. 7 B. 30 C. 35 D. Can’t Say 主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。 231. 数据科学家可能会同时使用多个算法（模型）进行预测，并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是 : A. 单个模型之间有高相关性 B. 单个模型之间有低相关性 C. 在集成学习中使用“平均权重”而不是“投票”会比较好 D. 单个模型都是用的一个算法 详细请参考下面文章:Basics of Ensemble Learning Explained in Simple English Kaggle Ensemble Guide 5 Easy questions on Ensemble Modeling everyone should know 232. 在有监督学习中， 我们如何使用聚类方法？ : 1. 我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习 2. 我们可以使用聚类“类别id”作为一个新的特征项，然后再用监督学习分别进行学习 3. 在进行监督学习之前， 我们不能新建聚类类别 4. 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习 A. 2 和 4 B. 1 和 2 C. 3 和 4 D. 1 和 3 我们可以为每个聚类构建不同的模型， 提高预测准确率。“类别id”作为一个特征项去训练， 可以有效地总结了数据特征。 233. 以下说法正确的是 : 1. 一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的 2. 如果增加模型复杂度， 那么模型的测试错误率总是会降低 3. 如果增加模型复杂度， 那么模型的训练错误率总是会降低 4. 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习 A. 1 B. 2 C. 3 D. 1 and 3 234. 对应GradientBoosting tree算法， 以下说法正确的是 : 1. 当增加最小样本分裂个数，我们可以抵制过拟合 2. 当增加最小样本分裂个数，会导致过拟合 3. 当我们减少训练单个学习器的样本个数，我们可以降低variance 4. 当我们减少训练单个学习器的样本个数，我们可以降低bias A. 2 和 4 B. 2 和 3 C. 1 和 3 D. 1 和 4 最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。 第二点是靠bias和variance概念的。 235. 以下哪个图是KNN算法的训练边界 : A) B B) A C) D D) C E) 都不是 KNN算法肯定不是线性的边界， 所以直的边界就不用考虑了。另外这个算法是看周围最近的k个样本的分类用以确定分类，所以边界一定是坑坑洼洼的。 236. 如果一个训练好的模型在测试集上有100%的准确率， 这是不是意味着在一个新的数据集上，也会有同样好的表现？ : A. 是的，这说明这个模型的范化能力已经足以支持新的数据集合了 B. 不对，依然后其他因素模型没有考虑到，比如噪音数据 没有一个模型是可以总是适应新数据的。我们不可能可到100%准确率。 237. 下面的交叉验证方法: i. 有放回的Bootstrap方法 ii. 留一个测试样本的交叉验证 iii. 5折交叉验证 iv. 重复两次的5折教程验证 当样本是1000时，下面执行时间的顺序，正确的是： A. i &gt; ii &gt; iii &gt; iv B. ii &gt; iv &gt; iii &gt; i C. iv &gt; i &gt; ii &gt; iii D. ii &gt; iii &gt; iv &gt; i Boostrap方法是传统地随机抽样，验证一次的验证方法，只需要训练1次模型，所以时间最少。 留一个测试样本的交叉验证，需要n次训练过程（n是样本个数），这里，要训练1000个模型。 5折交叉验证需要训练5个模型。 重复2次的5折交叉验证，需要训练10个模型。 238. 变量选择是用来选择最好的判别器子集，如果要考虑模型效率，我们应该做哪些变量选择的考虑？ : 1. 多个变量其实有相同的用处 2. 变量对于模型的解释有多大作用 3. 特征携带的信息 4. 交叉验证 A. 1 和 4 B. 1, 2 和 3 C. 1,3 和 4 D. 以上所有 注意， 这题的题眼是考虑模型效率，所以不要考虑选项2. 239. 对于线性回归模型，包括附加变量在内，以下的可能正确的是 : 1. R-Squared 和 Adjusted R-squared都是递增的 2. R-Squared 是常量的，Adjusted R-squared是递增的 3. R-Squared 是递减的， Adjusted R-squared 也是递减的 4. R-Squared 是递减的， Adjusted R-squared是递增的 A. 1 和 2 B. 1 和 3 C. 2 和 4 D. 以上都不是 R-squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-squared有R-squared 和 predicted R-squared 所没有的问题。 每次你为模型加入预测器，R-squared递增或不变.详细请看这个链接：discussion. 240. 对于下面三个模型的训练情况， 下面说法正确的是 : 1. 第一张图的训练错误与其余两张图相比，是最大的 2. 最后一张图的训练效果最好，因为训练错误最小 3. 第二张图比第一和第三张图鲁棒性更强，是三个里面表现最好的模型 4. 第三张图相对前两张图过拟合了 5. 三个图表现一样，因为我们还没有测试数据集 A. 1 和 3 B. 1 和 3 C. 1, 3 和 4 D. 5 241. 对于线性回归，我们应该有以下哪些假设？ 1. 找到利群点很重要, 因为线性回归对利群点很敏感 2. 线性回归要求所有变量必须符合正态分布 3. 线性回归假设数据没有多重线性相关性 A. 1 和 2 B. 2 和 3 C. 1,2 和 3 D. 以上都不是 利群点要着重考虑, 第一点是对的 不是必须的, 当然, 如果是正态分布, 训练效果会更好 有少量的多重线性相关性是可以的, 但是我们要尽量避免 242. 当我们构造线性模型时, 我们注意变量间的相关性. 在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论: 1. Var1和Var2是非常相关的 2. 因为Var1和Var2是非常相关的, 我们可以去除其中一个 3. Var3和Var1的1.23相关系数是不可能的 A. 1 and 3 B. 1 and 2 C. 1,2 and 3 D. 1 Var1和Var2相关系数是负的, 所以这是多重线性相关, 我们可以考虑去除其中一个. 一般地, 如果相关系数大于0.7或者小于-0.7, 是高相关的 相关性系数范围应该是 [-1,1] 243. 如果在一个高度非线性并且复杂的一些变量中, 一个树模型可能比一般的回归模型效果更好. 只是: A. 对的 B. 错的 244. 对于维度极低的特征，选择线性还是非线性分类器？ 非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。 1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。 245.246.247.248.249.250.251.252.253.254.255.256.]]></content>
      <tags>
        <tag>面试</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS31n学习笔记损失函数与最优化]]></title>
    <url>%2F2018%2F08%2F22%2FCS31n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89----%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9C%80%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1、前言 损失函数（Loss function）或者代价函数（Cost function）是用来估量你模型的预测值 $f(x)$ 与真实值 Y 的不一致程度，也就是当前W取值下的不理想程度。它是一个非负实值函数，通常用 $L(Y,f(x))$ 来表示。损失函数越小，模型的鲁棒性就越好。损失函数(一次预测好坏)是经验风险函数（平均意义下模型好坏）的核心部分，也是结构风险函数的重要组成部分。 一些资源：CS231n 2016 通关 第三章-SVM与Softmax 2、损失函数2.1 0-1 Loss 分类错误时损失为1，分类正确 L(Y, f(x))=\begin{cases} 1, & Y \neq f(x)\\ 0, & Y = f(x) \end{cases}2.2 Hinge Loss 折页损失，也称铰链损失，常用于SVM最大间隔法中。将所有不正确分类得分与正确类别（标签）得分之差加1，再与0比较，取最大值求和。1为安全系数，当然可以任意设置，W成比例缩放，损失也会成比例变化，所以分数与W的度量相关。 L(Y, f(x))=\frac{1}{N}\sum_i=1^N\sum_{j\neq y_i}\max{(0, f(x_i)_j - f(x_i)_{y_i}+1)} 问题1：如果在求loss时，允许j=y_i会怎么样？（L加1） 问题2：如果对1个样本做loss时使用对loss做平均，而不是求和，会怎样？（损失绝对值减小，相当于sum乘以常系数，问题1,2均不会影响最终的W） 问题3：若取平方会怎么样？（这其实是二次的hinge loss，在某些情况下会使用。并且某些情况下结果比一次的hinge loss更好，此处使用一次形式。） 问题4：hinge loss的最大最小损失是多少？（0，无穷大） 问题5：通常会以较小较小的值初始化参数w，此时得到的scores接近于0，那么这时候的loss是？(此时正确score与错误score的差接近于0，对于N类，loss的结果是N-1。)存在的问题：若w是损失函数为0，w按比例变化，损失函数值不变。解决办法：引入正则项，对损失函数进行约束$R(w)$，$R(w)$可以衡量w的好坏，因此不仅要求更好的拟合数据，也希望优化w， 同时可以防止过拟合，使得在测试集上效果更好。 L(Y, f(x))=\frac{1}{N}\sum_i=1^N\sum_{j\neq y_i}\max{(0, f(x_i)_j - f(x_i)_{y_i}+1)} + \lambda R(w) 常见的正则项有： L2正则项： L1正则项： 弹性网络（Elastic Net）L1+L2： max-norm regularization： Dropout： 加入正则，对w进行约束，常用的正则有L1、L2。，L1趋于选取稀疏的参数，起到特征选择的作用L2趋于选取数值较小且离散的参数，尽可能的考虑的大部分特征。SVM目标函数即：L2 + hinge loss 2.3 Softmax Loss 在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数，softmax只是其在多维上的推广，适用于多分类。 得分函数f(x, w)的值score看作是未标准化的对数概率，于是得到每一类的未标准化概率为$e^score$,进一步标准化得到0-1之间的概率，且所以的概率之和为1.大的score代表此score对应图像属于的某一个class的概率大。该函数就是softmax函数： softmax = P(Y = y_i|X = x_i)= \frac{e^{sy_i}}{\sum_j e^{s_j}} 我们要是正确类别对数概率最大，根据损失函数，要是负的正确分类概率最小。 使用似然估计作为loss，本来是似然估计越大越好，但通常loss使用越小时更直观： L_i = -\log{P(Y = y_i|X = x_i)} 即最终表达式：L_i = -\log{\frac{e^{sy_i}}{\sum_j e^{s_j}}} L(Y, f(x))=\frac{1}{N}\sum_{i=1}^N{y_i\dot \log{f(x_i) + (1 - y_i)\log(1-f(x_i))}} 问题1：损失函数最大值最小值？（0，无穷） 问题2：初始化w为很小值时，损失值？（$-log{\frac{1}{CLASSES}}$） softmax与SVM的区别：SVM只考虑支持向量对分类结果的影响，softmax考虑所有数据对结果的影响。 2.5 Absolute Loss 常用于回归任务中。 L(Y, f(x))=\frac{1}{N}\sum_i^N{|f(x_i) - y_i|}2.5 Square Loss 或者均方误差（MSE），常用于回归任务中。 L(Y, f(x))=\frac{1}{N}\sum_i^N{|f(x_i) - y_i|^2}2.6 Exponential Loss 指数损失常用于boosting算法中。 L(Y, f(x))=\sum_i^N{e^{-{y_i}{f(x_i)}}}3、最优化 学习就是个最优化问题，不断更新w，使得损失最小。 3.1 最朴素的思想：随机搜索 计算量大，效果不行，不可取。 3.2 梯度下降 一维函数梯度： \frac{df(x)}{d_x} = \lim_{1 \to \infty}\frac{f(x+h) - f(x)}{h} 梯度检验：，计算梯度的方式有两种：数值梯度（numerical gradient）、解析梯度（analytic gradient）。 数值梯度：根据公式右边计算，优点是容易编程实现，不要求函数可微，然而，缺点也很明显，通常是近似解，同时求解速度很慢，针对多维数据，需反复计算求每一维的梯度值。因此在设计机器学习目标函数时，通常设计成可微的函数，可以快速地通过微积分求解其解析梯度，同时这个梯度是确切解。 解析梯度：通过微积分公式直接对损失函数求导进行计算，无需反复计算，效率高，但易出错，毕竟涉及复杂数学问题。 梯度下降（BGD）、随机梯度下降（SGD）、Mini-batch Gradient Descent、带Mini-batch的SGD 梯度下降（BGD）：通过梯度的负值 * 步长一点一点更新w值，使得损失减少。步长也叫学习率，是一个非常重要的超参数（手工设置，模型不能学）。基于全部数据更新参数，得到全局最优解。 随机梯度下降（SGD）：训练集中随机选取一个样本，更新参数，局部最优解。通常batch_size大小根据GPU内存大小设置。 小批量批梯度下降 (mini-batch GD)：训练集中，选取小批量数据更新参数。 小批量随机梯度下降（mini-batch SGD）：上述两者的结合。]]></content>
      <tags>
        <tag>笔记</tag>
        <tag>cs231n</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BAT机器学习面试题库(101-200)]]></title>
    <url>%2F2018%2F08%2F20%2FBAT%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E5%BA%93%EF%BC%88100-200%EF%BC%89%2F</url>
    <content type="text"><![CDATA[为了求职笔试面试，需恶补基础、算法原理，于是仔细研读了七月在线发布的BAT机器学习面试1000题系列，也添加了一些自己的理解或来自其他博客的答案，以下内容均来自BAT机器学习面试1000题系列。该文为本人的阅读笔记，主要是为了记忆和自查。 101. 解释对偶的概念。机器学习 ML基础 易 一个优化问题可以从两个角度进行考察，一个是primal(原始)问题，一个是dual(对偶)问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。 102. 如何进行特征选择？机器学习 ML基础 中 特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解。 常见的特征选择方式： 1. 去除方差较小的特征 2. 正则化。Ｌ1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。 3. 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。 4. 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。 103. 数据预处理。机器学习 ML基础 易 1. 缺失值，填充缺失值fillna： i. 离散：None, ii. 连续：均值。 iii. 缺失值太多，则直接去除该列。 2. 连续值：离散化。有的模型（如决策树）需要离散值 3. 对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。 4. 皮尔逊相关系数，去除高度相关的列 104. 简单说说特征工程。机器学习 ML基础 中 105. 你知道有哪些数据处理和特征工程的处理？机器学习 ML应用 中 106. 请对比下Sigmoid、Tanh、ReLu这三个激活函数。深度学习 DL基础 中 sigmoid函数又称logistic函数，应用在Logistic回归中。logistic回归的目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。 假设函数：$h_\theta (x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}$ 其中x是n维特征向量，函数g就是logistic函数。而$g(z) = \frac{1}{e^{-z}}$函数图像： 可以看到，将无穷映射到了(0,1)。 而假设函数就是特征属于y=1的概率。 P(y=1\| x; \theta) = h_\theta(x) \\ P(y= 0|x;\theta) = 1 - h_\theta(x) 从而，当我们要判别一个新来的特征属于哪个类时，只需求$h_\theta(x)$即可，若$h_\theta(x)$大于0.5就是y=1的类，反之属于y=0类。 tanh激活函数： relu激活函数： 但sigmoid函数有如下几个缺点： a). 正向计算包含指数，反向传播的导数也包含指数计算和除法运算，因而计算复杂度很高。 b). 输出的均值非0。这样使得网络容易发生梯度消失或梯度爆炸。这也是batch normalization要解决的问题。 c). 假如sigmoid函数为f(x)，那么f’(x)=f(x)(1-f(x))，因为f(x)输出在0-1之间，那么f’(x)恒大于0。 这就导致全部的梯度的正负号都取决于损失函数上的梯度。这样容易导致训练不稳定，参数一荣俱荣一损俱损。 d). 同样的，f’(x)=f(x)(1-f(x))，因为f(x)输出在0-1之间，那么f’(x)输出也在0-1之间，当层次比较深时，底层的导数就是很多在0-1之间的数相乘，从而导致了梯度消失问题。 对于tanh来说，同sigmoid类似，但是输出值在-1到1之间，均值为0，是其相对于sigmoid的提升。但是因为输出在-1，1之间，因而输出不能被看做是概率。 更多参考资源 | 从ReLU到Sinc，26种神经网络激活函数可视化 对于ReLU来说，相对于sigmoid和tanh来说，有如下优点： a). 计算量下，没有指数和除法运算。 b). 不会饱和，因为在x&gt;0的情况下，导数恒等于1 d). 收敛速度快，在实践中可以得知，它的收敛速度是sigmoid的6倍。 e). Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生 但是Relu也有缺点，缺点在于， 如果有一个特别大的导数经过神经单元使得输入变得小于0，这样会使得这个单元永远得不到参数更新，因为输入小于0时导数也是0. 这就形成了很多dead cell(失活)。 107. Sigmoid、Tanh、ReLu这三个激活函数有什么缺点或不足，有没改进的激活函数。深度学习 DL基础 中 sigmoid、Tanh、ReLU的缺点在见上题，@张雨石：为了解决ReLU的dead cell的情况，发明了Leaky Relu， 即在输入小于0时不让输出为0，而是乘以一个较小的系数，从而保证有导数存在。其他改进如：PRelu、RRelu、RReLU同样的目的，还有一个ELU，函数示意图如下。 还有一个激活函数是Maxout，即使用两套w,b参数，输出较大值。本质上Maxout可以看做Relu的泛化版本，因为如果一套w,b全都是0的话，那么就是普通的ReLU。Maxout可以克服Relu的缺点，但是参数数目翻倍。 面试笔试整理3：深度学习机器学习面试问题准备（必会） 108. 怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感。机器学习 ML模型 中怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感? 109. 为什么引入非线性激励函数？深度学习 DL基础 中 @张雨石：第一，对于神经网络来说，网络的每一层相当于f(wx+b)=f(w’x)，对于线性函数，其实相当于f(x)=x，那么在线性激活函数下，每一层相当于用一个矩阵去乘以x，那么多层就是反复的用矩阵去乘以输入。根据矩阵的乘法法则，多个矩阵相乘得到一个大矩阵。所以线性激励函数下，多层网络与一层网络相当。比如，两层的网络f(W1f(W2x))=W1W2x=Wx。 第二，非线性变换是深度学习有效的原因之一。原因在于非线性相当于对空间进行变换，变换完成后相当于对*问题空间进行简化，原来线性不可解的问题现在变得可以解了。 下图可以很形象的解释这个问题，左图用一根线是无法划分的。经过一系列变换后，就变成线性可解的问题了。 请问人工神经网络中的activation function的作用具体是什么？为什么ReLu要好过于tanh和sigmoid function? 110. 请问人工神经网络中为什么ReLu要好过于tanh和sigmoid function？深度学习 DL基础 中 见以上问题 111. 为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数？深度学习 DL模型 难 为什么不是选择统一一种sigmoid或者tanh，而是混合使用呢？这样的目的是什么？为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数？ 二者目的不一样 a). sigmoid 用在了各种gate上，产生0~1之间的值，这个一般只有sigmoid最直接了。 b). tanh 用在了状态和输出上，是对数据的处理，这个用其他激活函数或许也可以。 112. 衡量分类器的好坏？机器学习 ML基础 中 面试笔试整理3：深度学习机器学习面试问题准备（必会）这里首先要知道TP、FN（真的判成假的）、FP（假的判成真）、TN四种（可以画一个表格）。 几种常用的指标： 准确率: precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量） 召回率: recall = TP/(TP+FN) = TP/ P F1值: 2/F1 = 1/recall + 1/precision ROC曲线: ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N AUC: ROC曲线与坐标轴围成的面积。 113. 机器学习和统计里面的auc的物理意义是啥？机器学习 ML基础 中 机器学习和统计里面的auc怎么理解？ 114. 观察增益gain, alpha和gamma越大，增益越小？机器学习 ML基础 中 @AntZ：xgboost寻找分割点的标准是最大化gain. 考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中计算Gain按最大值找出最佳的分割点。它的计算公式分为四项, 可以由正则化项参数调整(lamda为叶子权重平方和的系数, gama为叶子数量): 第一项是假设分割的左孩子的权重分数, 第二项为右孩子, 第三项为不分割总体分数, 最后一项为引入一个节点的复杂度损失 由公式可知, gama越大gain越小, lamda越大, gain可能小也可能大. 原问题是alpha而不是lambda, 这里paper上没有提到, xgboost实现上有这个参数. 上面是我从paper上理解的答案,下面是搜索到的: https://zhidao.baidu.com/question/2121727290086699747.html?fr=iks&amp;word=xgboost+lamda&amp;ie=gbk lambda[默认1]权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。11、alpha[默认1]权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。 gamma[默认0]在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。 115. 什麽造成梯度消失问题? 推导一下。深度学习 DL基础 中 Yes you should understand backdrop－Andrej Karpathy How does the ReLu solve the vanishing gradient problem? 神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。 梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0，造成学习停止。 116. 什么是梯度消失和梯度爆炸？深度学习 DL基础 中 @寒小阳，反向传播中链式法则带来的连乘，如果有数很小趋于0，结果就会特别小（梯度消失）；如果数都比较大，可能结果会很大（梯度爆炸）。 @单车，下段来源：神经网络训练中的梯度消失与梯度爆炸 层数比较多的神经网络模型在训练时也是会出现一些问题的，其中就包括梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。 例如，对于下图所示的含有3个隐藏层的神经网络，梯度消失问题发生时，接近于输出层的hidden layer 3等的权值更新相对正常，但前面的hidden layer 1的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。 而这种问题为何会产生呢？以下图的反向传播为例（假设每一层只有一个神经元且对于每一层$y_i=\sigma\left(z_i\right)=\sigma\left(w_ix_i+b_i\right)$，其中$\sigma$为sigmoid函数） 可以推导出 \begin{align} &\frac{\partial C}{\partial b_1}=\frac{\partial C}{\partial y_4}\frac{\partial y_4}{\partial z_4}\frac{\partial z_4}{\partial x_4}\frac{\partial x_4}{\partial z_3}\frac{\partial z_3}{\partial x_3}\frac{\partial x_3}{\partial z_2}\frac{\partial z_2}{\partial x_2}\frac{\partial x_2}{\partial z_1}\frac{\partial z_1}{\partial b_1}\\ &=\frac{\partial C}{\partial y_4}\sigma'\left(z_4\right)w_4\sigma'\left(z_3\right)w_3\sigma'\left(z_2\right)w_2\sigma'\left(z_1\right) \end{align} 而sigmoid的导数$\sigma’\left(x\right)$如下图： 可见，$\sigma’\left(x\right)$的最大值为$\frac{1}{4}$，而我们初始化的网络权值|w|通常都小于1，因此$|\sigma’\left(z\right)w|\leq\frac{1}{4}$，因此对于上面的链式求导，层数越多，求导结果$\frac{\partial C}{\partial b_1}$越小，因而导致 梯度消失的情况出现。 这样，梯度爆炸问题的出现原因就显而易见了，即$|\sigma’\left(z\right)w|&gt;1$，也就是w比较大的情况。但对于使用sigmoid激活函数来说，这种情况比较少。因为$\sigma’\left(z\right)$的大小也与w有关$（z=wx+b）$，除非该层的输入值x在一直一个比较小的范围内。 其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。另外，LSTM的结构设计也可以改善RNN中的梯度消失问题。117. 如何解决梯度消失和梯度膨胀？深度学习 DL基础 中 （1）梯度消失： 根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0 可以采用ReLU激活函数有效的解决梯度消失的情况，也可以用Batch Normalization解决这个问题。关于深度学习中 Batch Normalization为什么效果好？参见：深度学习中 Batch Normalization为什么效果好？ （2）梯度膨胀 根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大 可以通过激活函数来解决，或用Batch Normalization解决这个问题。 118. 推导下反向传播Backpropagation。深度学习 DL基础 难 @我愛大泡泡，来源：面试笔试整理3：深度学习机器学习面试问题准备（必会） 首先，要理解反向传播的基本原理，那就是求导的链式法则。 \frac{d_y}{d_x} = \frac{d_y}{d_u} \cdot \frac{d_u}{d_x}\\ \frac{d_y}{d_z} = \frac{d_y}{d_u} \cdot \frac{d_u}{d_x} \cdot \frac{d_x}{d_z} 反映到神经网络里： 下面从损失函数开始用公式进行推导。 反向传播是在求解损失函数L对参数w求导时候用到的方法，目的是通过链式法则对参数进行一层一层的求导。这里重点强调：要将参数进行随机初始化而不是全部置0，否则所有隐层的数值都会与输入相关，这称为对称失效。 大致过程是: a). 首先前向传导计算出所有节点的激活值和输出值， $z^{(l +1)} = W^{(l)}a^{(l)} + b^{(l)}$ $ａ^{(l +1)} = f(z^{(l +1)})$ b). 计算整体损失函数： c). 然后针对第L层的每个节点计算出残差（这里是因为UFLDL中说的是残差，本质就是整体损失函数对每一层激活值Z的导数），所以要对W求导只要再乘上激活函数对W的导数即可 119. SVD和PCA。机器学习 ML模型 中 PCA的理念是使得数据投影后的方差最大，找到这样一个投影向量，满足方差最大的条件即可。而经过了去除均值的操作之后，就可以用SVD分解来求解这样一个投影向量，选择特征值最大的方向。 PCA的本质是对于一个以矩阵为参数的分布进行似然估计，而SVD是矩阵近似的有效手段。 详见：PCA数据降维和SVD降维有什么区别？ 120. 数据不平衡问题。机器学习 ML基础 易 这主要是由于数据分布不平衡造成的。解决方法如下： 1. 采样，对小样本加噪声采样，对大样本进行下采样 2. 数据生成，利用已知样本生成新的样本 3. 进行特殊的加权，如在Adaboost中或者SVM中 4. 采用对不平衡数据集不敏感的算法 5. 改变评价标准：用AUC/ROC来进行评价 6. 采用Bagging/Boosting/ensemble等方法 7. 在设计模型的时候考虑数据的先验分布 121. 简述神经网络的发展历史。深度学习 DL基础 中 1949年Hebb提出了神经心理学学习范式——Hebbian学习理论 1952年，IBM的Arthur Samuel写出了西洋棋程序 1957年，Rosenblatt的感知器算法是第二个有着神经系统科学背景的机器学习模型. 3年之后，Widrow因发明Delta学习规则而载入ML史册，该规则马上就很好的应用到了感知器的训练中 感知器的热度在1969被Minskey一盆冷水泼灭了。他提出了著名的XOR问题，论证了感知器在类似XOR问题的线性不可分数据的无力。 尽管BP的思想在70年代就被Linnainmaa以“自动微分的翻转模式”被提出来，但直到1981年才被Werbos应用到多层感知器(MLP)中，NN新的大繁荣。 1991年的Hochreiter和2001年的Hochreiter的工作，都表明在使用BP算法时，NN单元饱和之后会发生梯度损失。又发生停滞。 时间终于走到了当下，随着计算资源的增长和数据量的增长。一个新的NN领域——深度学习出现了。 简言之，MP模型+sgn—-&gt;单层感知机（只能线性）+sgn— Minsky 低谷 —&gt;多层感知机+BP+sigmoid—- (低谷) —&gt;深度学习+pre-training+ReLU/sigmoid 122. 深度学习常用方法。深度学习 DL基础 中 @SmallisBig，来源：机器学习岗位面试问题汇总 之 深度学习 a). 全连接DNN（相邻层相互连接、层内无连接）： b). AutoEncoder(尽可能还原输入)、Sparse Coding（在AE上加入L1规范）、RBM（解决概率问题）—–&gt;特征探测器——&gt;栈式叠加 贪心训练 c). RBM—-&gt;DBN d). 解决全连接DNN的全连接问题—–&gt;CNN e). 解决全连接DNN的无法对时间序列上变化进行建模的问题—–&gt;RNN—解决时间轴上的梯度消失问题——-&gt;LSTM @张雨石：现在在应用领域应用的做多的是DNN，CNN和RNN。 DNN是传统的全连接网络，可以用于广告点击率预估，推荐等。其使用embedding的方式将很多离散的特征编码到神经网络中，可以很大的提升结果。 CNN主要用于计算机视觉(Computer Vision)领域，CNN的出现主要解决了DNN在图像领域中参数过多的问题。同时，CNN特有的卷积、池化、batch normalization、Inception、ResNet、DeepNet等一系列的发展也使得在分类、物体检测、人脸识别、图像分割等众多领域有了长足的进步。同时，CNN不仅在图像上应用很多，在自然语言处理上也颇有进展，现在已经有基于CNN的语言模型能够达到比LSTM更好的效果。在最新的AlphaZero中，CNN中的ResNet也是两种基本算法之一。 GAN是一种应用在生成模型的训练方法，现在有很多在CV方面的应用，例如图像翻译，图像超分辨率、图像修复等等。 RNN主要用于自然语言处理(Natural Language Processing，NLP)领域，用于处理序列到序列的问题。普通RNN会遇到梯度爆炸和梯度消失的问题。所以现在在NLP领域，一般会使用LSTM模型。在最近的机器翻译领域，Attention作为一种新的手段，也被引入进来。 除了DNN、RNN和CNN外， 自动编码器(AutoEncoder)、稀疏编码(Sparse Coding)、深度信念网络(DBM)、限制玻尔兹曼机(RBM)也都有相应的研究。 123. 神经网络模型（Neural Network）因受人类大脑的启发而得名。深度学习 DL基础 易124. 神经网络由许多神经元（Neuron）组成，每个神经元接受一个输入，对输入进行处理后给出一个输出，如下图所示。请问下列关于神经元的描述中，哪一项是正确的？ A 每个神经元可以有一个输入和一个输出 B 每个神经元可以有多个输入和一个输出 C 每个神经元可以有一个输入和多个输出 D 每个神经元可以有多个输入和多个输出 E 上述都正确 答案：（E） 每个神经元可以有一个或多个输入，和一个或多个输出。 125. 下图是一个神经元的数学表示。深度学习 DL基础 易 - x1, x2,…, xN：表示神经元的输入。可以是输入层的实际观测值，也可以是某一个隐藏层（Hidden Layer）的中间值 - w1, w2,…,wN：表示每一个输入的权重 - bi：表示偏差单元/偏移量（bias unit）。作为常数项加到激活函数的输入当中，类似截距（Intercept） - a：作为神经元的激励函数（Activation），可以表示为 a=f(\sum_{i=1}^{N}w_ix_i) - y：神经元输出 考虑上述标注，线性等式（y = mx + c）可以被认为是属于神经元吗：(是) 126. 在一个神经网络中，知道每一个神经元的权重和偏差是最重要的一步。如果知道了神经元准确的权重和偏差，便可以近似任何函数，但怎么获知每个神经的权重和偏移呢？深度学习 DL基础 易 A 搜索每个可能的权重和偏差组合，直到得到最佳值 B 赋予一个初始值，然后检查跟最佳值的差值，不断迭代调整权重 C 随机赋值，听天由命 D 以上都不正确的 选项B是对梯度下降的描述。 127. 梯度下降算法的正确步骤是什么？深度学习 DL基础 易 1. 计算预测值和真实值之间的误差 2. 重复迭代，直至得到网络权重的最佳值 3. 把输入传入网络，得到输出值 4. 用随机值初始化权重和偏差 5. 对每一个产生误差的神经元，调整相应的（权重）值以减小误差 答案：4, 3, 1, 5, 2 128. 已知：- 大脑是有很多个叫做神经元的东西构成，神经网络是对大脑的简单的数学表达。- 每一个神经元都有输入、处理函数和输出。- 神经元组合起来形成了网络，可以拟合任何函数。- 为了得到最佳的神经网络，我们用梯度下降方法不断更新模型。给定上述关于神经网络的描述，什么情况下神经网络模型被称为深度学习模型？深度学习 DL基础 易 A 加入更多层，使神经网络的深度增加 B 有维度更高的数据 C 当这是一个图形识别的问题时 D 以上都不正确 更多层意味着网络更深。没有严格的定义多少层的模型才叫深度模型，目前如果有超过2层的隐层，那么也可以及叫做深度模型。 129. 使用CNN时，是否需要对输入进行旋转、平移、缩放等预处理？深度学习 DL基础 易 （需要）把数据传入神经网络之前需要做一系列数据预处理（也就是旋转、平移、缩放）工作，神经网络本身不能完成这些变换。 130. 下面哪项操作能实现跟神经网络中Dropout的类似效果？（B）深度学习 DL基础 易 A Boosting B Bagging C Stacking D Mapping Dropout可以认为是一种极端的Bagging，每一个模型都在单独的数据上训练，同时，通过和其他模型对应参数的共享，从而实现模型参数的高度正则化。 131. 下列哪一项在神经网络中引入了非线性？深度学习 DL基础 易 A 随机梯度下降 B 修正线性单元（ReLU） C 卷积函数 D 以上都不正确 修正线性单元（ReLU）是非线性的激活函数。 132. 在训练神经网络时，损失函数(loss)在最初的几个epochs时没有下降，可能的原因是？深度学习 DL基础 易 A 学习率(learning rate)太低 B 正则参数太高 C 陷入局部最小值 D 以上都有可能 133. 下列哪项关于模型能力（model capacity）的描述是正确的？（指神经网络模型能拟合复杂函数的能力）深度学习 DL基础 易 A 隐藏层层数增加，模型能力增加 B Dropout的比例增加，模型能力增加 C 学习率增加，模型能力增加 D 都不正确 134. 如果增加多层感知机（Multilayer Perceptron）的隐藏层层数，分类误差便会减小。这种陈述正确还是错误？深度学习 DL基础 易 （错误）并不总是正确。层数增加可能导致过拟合，从而可能引起错误增加。模型退化问题 135. 构建一个神经网络，将前一层的输出和它自身作为输入。深度学习 DL模型 易 下列哪一种架构有反馈连接？ A 循环神经网络 B 卷积神经网络 C 限制玻尔兹曼机 D 都不是 136. 在感知机中（Perceptron）的任务顺序是什么？深度学习 DL基础 易 1. 随机初始化感知机的权重 2. 去到数据集的下一批（batch） 3. 如果预测值和输出不一致，则调整权重 4. 对一个输入样本，计算输出值 答案：1-&gt;4-&gt;3-&gt;2 137. 假设你需要调整参数来最小化代价函数（cost function），会使用下列哪项技术？深度学习 DL基础 易 A． 穷举搜索 B． 随机搜索 C． Bayesian优化 D． 梯度下降 138. 在下面哪种情况下，一阶梯度下降不一定正确工作（可能会卡住）？深度学习 DL基础 易 D. 以上都不正确 答案：B 这是鞍点（Saddle Point）的梯度下降的经典例子。另，本题来源于：Questions to test a data scientist on basics of Deep Learning (along with solution)。 139. 图显示了训练过的3层卷积神经网络准确度，与参数数量(特征核的数量)的关系。深度学习 DL基础 易 从图中趋势可见，如果增加神经网络的宽度，精确度会增加到一个特定阈值后，便开始降低。造成这一现象的可能原因是什么？ A 即使增加卷积核的数量，只有少部分的核会被用作预测 B 当卷积核数量增加时，神经网络的预测能力（Power）会降低 C 当卷积核数量增加时，导致过拟合 D 以上都不正确 140. 假设我们有一个如下图所示的隐藏层。隐藏层在这个网络中起到了一定的降纬作用。假如现在我们用另一种维度下降的方法，比如说主成分分析法(PCA)来替代这个隐藏层。 深度学习 DL基础 易 那么，这两者的输出效果是一样的吗？(不一样)，PCA 提取的是数据分布方差比较大的方向，隐藏层可以提取有预测能力的特征 141. 下列哪个函数不可以做激活函数？深度学习 DL基础 易 A. y = tanh(x) B. y = sin(x) C. y = max(x,0) D. y = 2x 激活函数特性：非线性、可导、特定区域。 142. 下列哪个神经网络结构会发生权重共享？深度学习 DL模型 易 A.卷积神经网络 B.循环神经网络 C.全连接神经网络 D.选项A和B 143. 批规范化(Batch Normalization)的好处都有啥？深度学习 DL基础 中 A.在将所有的输入传递到下一层之前对其进行归一化（更改） B.它将权重的归一化平均值和标准差 C.它是一种非常有效的反向传播(BP)方法 D.这些均不是 144. 在一个神经网络中，下面哪种方法可以用来处理过拟合？ 深度学习 DL基础 易 A Dropout B 分批归一化(Batch Normalization) C 正则化(regularization) D 都可以 145. 如果我们用了一个过大的学习速率会发生什么？深度学习 DL基础 易 A 神经网络会收敛 B 不好说 C 都不对 D 神经网络不会收敛 146. 下图所示的网络用于训练识别字符H和T，如下所示（深度学习 DL基础 易）： 网络的输出是什么？ D.可能是A或B，取决于神经网络的权重设置 不知道神经网络的权重和偏差是什么，则无法判定它将会给出什么样的输出。 147. 假设我们已经在ImageNet数据集(物体识别)上训练好了一个卷积神经网络。然后给这张卷积神经网络输入一张全白的图片。对于这个输入的输出结果为任何种类的物体的可能性都是一样的，对吗？深度学习 DL模型 中 A 对的 B 不知道 C 看情况 D 不对 答案：D，已经训练好的卷积神经网络, 各个神经元已经精雕细作完工, 对于全白图片的输入, 其j层层激活输出给最后的全连接层的值几乎不可能恒等, 再经softmax转换之后也不会相等, 所以”输出结果为任何种类的等可能性一样”也就是softmax的每项均相等, 这个概率是极低的。 148. 当在卷积神经网络中加入池化层(pooling layer)时，变换的不变性会被保留，是吗？深度学习 DL模型 中 答案：（C） 池化算法比如取最大值/取平均值等, 都是输入数据旋转后结果不变, 所以多层叠加后也有这种不变性。(降维/尺度不变性) 149. 当数据过大以至于无法在RAM中同时处理时，哪种梯度下降方法更加有效？ 深度学习 DL基础 易 A 随机梯度下降法(Stochastic Gradient Descent) B 不知道 C 整批梯度下降法(Full Batch Gradient Descent) D 都不是 梯度下降法分随机梯度下降（SGD）(每次用一个样本)、小批量梯度下降法(每次用一小批样本算出总损失, 因而反向传播的梯度折中)、全批量梯度下降法则一次性使用全部样本。这三个方法, 对于全体样本的损失函数曲面来说, 梯度指向一个比一个准确. 但是在工程应用中,受到内存/磁盘IO的吞吐性能制约, 若要最小化梯度下降的实际运算时间, 需要在梯度方向准确性和数据传输性能之间取得最好的平衡. 所以, 对于数据过大以至于无法在RAM中同时处理时, RAM每次只能装一个样本, 那么只能选随机梯度下降法。 150. 下图是一个利用sigmoid函数作为激活函数的含四个隐藏层的神经网络训练的梯度下降图。这个神经网络遇到了梯度消失的问题。下面哪个叙述是正确的？ 深度学习 DL基础 中 A. 第一隐藏层对应D，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应A B. 第一隐藏层对应A，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应D C. 第一隐藏层对应A，第二隐藏层对应B，第三隐藏层对应C，第四隐藏层对应D D. 第一隐藏层对应B，第二隐藏层对应D，第三隐藏层对应C，第四隐藏层对应A 由于反向传播算法进入起始层，学习能力降低，这就是梯度消失。换言之，梯度消失是梯度在前向传播中逐渐减为0, 按照图标题所说, 四条曲线是4个隐藏层的学习曲线, 那么第一层梯度最高(损失函数曲线下降明显), 最后一层梯度几乎为零(损失函数曲线变成平直线). 所以D是第一层, A是最后一层。 151. 对于一个分类任务，如果开始时神经网络的权重不是随机赋值的，而是都设成0，下面哪个叙述是正确的？ 深度学习 DL基础 易 A 其他选项都不对 B 没啥问题，神经网络会正常开始训练 C 神经网络可以训练，但是所有的神经元最后都会变成识别同样的东西 D 神经网络不会开始训练，因为没有梯度改变 令所有权重都初始化为0这个一个听起来还蛮合理的想法也许是一个我们假设中最好的一个假设了, 但结果是错误的，因为如果神经网络计算出来的输出值都一个样，那么反向传播算法计算出来的梯度值一样，并且参数更新值也一样(w=w−α∗dw)。更一般地说，如果权重初始化为同一个值，网络即是对称的, 最终所有的神经元最后都会变成识别同样的东西。 152. 下图显示，当开始训练时，误差一直很高，这是因为神经网络在往全局最小值前进之前一直被卡在局部最小值里。为了避免这种情况，我们可以采取下面哪种策略？深度学习 DL基础 易 A 改变学习速率，比如一开始的几个训练周期不断更改学习速率 B 一开始将学习速率减小10倍，然后用动量项(momentum) C 增加参数数目，这样神经网络就不会卡在局部最优处 D 其他都不对 选项A可以将陷于局部最小值的神经网络提取出来。 153. 对于一个图像识别问题(在一张照片里找出一只猫)，下面哪种神经网络可以更好地解决这个问题？ 深度学习 DL基础 易 A 循环神经网络 B 感知机 C 多层感知机 D 卷积神经网络 卷积神经网络将更好地适用于图像相关问题，因为考虑到图像附近位置变化的固有性质(局部相关性)。 154. 假设在训练中我们突然遇到了一个问题，在几次循环之后，误差瞬间降低 你认为数据有问题，于是你画出了数据并且发现也许是数据的偏度过大造成了这个问题。 你打算怎么做来处理这个问题？深度学习 DL基础 易 A 对数据作归一化 B 对数据取对数变化 C 都不对 D 对数据作主成分分析(PCA)和归一化 首先您将相关的数据去掉，然后将其置零。具体来说，误差瞬间降低, 一般原因是多个数据样本有强相关性且突然被拟合命中, 或者含有较大方差数据样本突然被拟合命中. 所以对数据作主成分分析(PCA)和归一化能够改善这个问题。 155. 下面那个决策边界是神经网络生成的？ 深度学习 DL基础 易 A A B D C C D B E 以上都有 神经网络可以逼近方式拟合任意函数, 所以以上图都可能由神经网络通过监督学习训练得到决策边界。 156. 在下图中，我们可以观察到误差出现了许多小的”涨落”。 这种情况我们应该担心吗？深度学习 DL基础 易 A 需要，这也许意味着神经网络的学习速率存在问题 B 不需要，只要在训练集和交叉验证集上有累积的下降就可以了 C 不知道 D 不好说 选项B是正确的，为了减少这些“起伏”，可以尝试增加批尺寸(batch size)。具体来说，在曲线整体趋势为下降时, 为了减少这些“起伏”，可以尝试增加批尺寸(batch size)以缩小batch综合梯度方向摆动范围. 当整体曲线趋势为平缓时出现可观的“起伏”, 可以尝试降低学习率以进一步收敛. “起伏”不可观时应该提前终止训练以免过拟合。 157. 在选择神经网络的深度时，下面那些参数需要考虑？深度学习 DL基础 易 1 神经网络的类型(如MLP,CNN) 2 输入数据 3 计算能力(硬件和软件能力决定) 4 学习速率 5 映射的输出函数 A 1,2,4,5 B 2,3,4,5 C 都需要考虑 D 1,3,4,5 所有上述因素对于选择神经网络模型的深度都是重要的。特征抽取所需分层越多, 输入数据维度越高, 映射的输出函数非线性越复杂, 所需深度就越深. 另外为了达到最佳效果, 增加深度所带来的参数量增加, 也需要考虑硬件计算能力和学习速率以设计合理的训练时间。 158. 考虑某个具体问题时，你可能只有少量数据来解决这个问题。不过幸运的是你有一个类似问题已经预先训练好的神经网络。可以用下面哪种方法来利用这个预先训练好的网络？ 深度学习 DL基础 易 A 把除了最后一层外所有的层都冻住，重新训练最后一层 B 对新数据重新训练整个模型 C 只对最后几层进行调参(fine tune) D 对每一层模型进行评估，选择其中的少数来用 如果有个预先训练好的神经网络, 就相当于网络各参数有个很靠谱的先验代替随机初始化. 若新的少量数据来自于先前训练数据(或者先前训练数据量很好地描述了数据分布, 而新数据采样自完全相同的分布), 则冻结前面所有层而重新训练最后一层即可; 但一般情况下, 新数据分布跟先前训练集分布有所偏差, 所以先验网络不足以完全拟合新数据时, 可以冻结大部分前层网络, 只对最后几层进行训练调参(这也称之为fine tune)。 159. 增加卷积核的大小对于改进卷积神经网络的效果是必要的吗？（C）深度学习 DL基础 易 答案：（不是），增加核函数的大小不一定会提高性能。这个问题在很大程度上取决于数据集。 160. 请简述神经网络的发展史。深度学习 DL基础 易 @SIY.Z。本题解析来源：浅析 Hinton 最近提出的 Capsule 计划 sigmoid会饱和，造成梯度消失。于是有了ReLU。 ReLU负半轴是死区，造成梯度变0。于是有了LeakyReLU，PReLU。 强调梯度和权值分布的稳定性，由此有了ELU，以及较新的SELU。 太深了，梯度传不下去，于是有了highway。 干脆连highway的参数都不要，直接变残差，于是有了ResNet。 强行稳定参数的均值和方差，于是有了BatchNorm。 在梯度流中增加噪声，于是有了 Dropout。 RNN梯度不稳定，于是加几个通路和门控，于是有了LSTM。 LSTM简化一下，有了GRU。 GAN的JS散度有问题，会导致梯度消失或无效，于是有了WGAN。 WGAN对梯度的clip有问题，于是有了WGAN-GP。 161. 说说spark的性能调优。大数据 Hadoop/spark 中 手把手教你 Spark 性能调优 162. 常见的分类算法有哪些？ 机器学习 ML基础 易 SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯 163. 常见的监督学习算法有哪些？ 机器学习 ML基础 易 感知机、svm、人工神经网络、决策树、逻辑回归 164. 在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）机器学习 ML基础 易 A. 增加训练集量 B. 减少神经网络隐藏层节点数 C. 删除稀疏的特征 D. SVM算法中使用高斯核/RBF核代替线性核 @刘炫320，本题题目及解析来源：机器学习习题集 一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。 B.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合 D.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数 之一。 165. 下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。机器学习 ML模型 易 A.AR模型 B.MA模型 C.ARMA模型 D.GARCH模型 @刘炫320，本题题目及解析来源：机器学习习题集 AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。 MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。 ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。 GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。 166. 以下()属于线性分类器最佳准则？机器学习 ML模型 易 A.感知准则函 B.贝叶斯分类 C.支持向量机 D.Fisher准则 @刘炫320，本题题目及解析来源：机器学习习题集 线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。 感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。 支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题） Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。 根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。 167. 基于二次准则函数的H-K算法较之于感知器算法的优点是()？深度学习 DL基础 易 A.计算量小 B.可以判别问题是否线性可分 C.其解完全适用于非线性可分的情况 D.其解的适应性更好 @刘炫320，本题题目及解析来源：机器学习习题集 H-K算法思想很朴实,就是在最小均方误差准则下求得权矢量. 他相对于感知器算法的优点在于,他适用于线性可分和非线性可分得情况,对于线性可分的情况,给出最优权矢量,对于非线性可分得情况,能够判别出来,以退出迭代过程. 168. 以下说法中正确的是() 机器学习 ML模型 中 A.SVM对噪声(如来自其他分布的噪声样本)鲁棒 B.在AdaBoost算法中,所有被分错的样本的权重更新比例相同 C.Boosting和Bagging都是组合多个分类器投票的方法,二都是根据单个分类器的正确率决定其权重 D.给定n个数据点,如果其中一半用于训练,一般用于测试,则训练误差和测试误差之间的差别会随着n的增加而减少 @刘炫320，本题题目及解析来源：机器学习习题集 A、SVM本身对噪声具有一定的鲁棒性，但实验证明，是当噪声率低于一定水平的噪声对SVM没有太大影响，但随着噪声率的不断增加，分类器的识别率会降低。 B、AdaBoost算法中不同的训练集是通过调整每个样本对应的权重来实现的。开始时，每个样本对应的权重是相同的，即其中n为样本个数，在此样本分布下训练出一弱分类器。对于分类错误的样本，加大其对应的权重；而对于分类正确的样本，降低其权重，这样分错的样本就被凸显出来，从而得到一个新的样本分布。在新的样本分布下，再次对样本进行训练，得到弱分类器。以此类推，将所有的弱分类器重叠加起来，得到强分类器。 C、Bagging与Boosting的区别： a). 取样方式不同：Bagging采用均匀取样，而Boosting根据错误率取样。 b). 权重不同：Bagging的各个预测函数没有权重，而Boosting是有权重的。 c). 可并行性：Bagging的各个预测函数可以并行生成，而Boosing的各个预测函数只能顺序生成。 d). 决策不同：bagging简单取投票结果，而Boosing根据误分类率决定分类器权重。 @AntZ A. SVM解决的是结构风险最小, 经验风险处理较弱, 所以对数据噪声敏感. B. AdaBoost算法中, 每个迭代训练一个学习器并按其误分类率得到该学习器的权重alpha, 这个学习器的权重算出两个更新比例去修正全部样本的权重: 正样本是exp(-alpha), 负样本是exp(alpha). 所以所有被分错的样本的权重更新比例相同. C. bagging的学习器之间无权重不同, 简单取投票结果; Boosting的adaboost根据误分类率决定权重, boosting的gbdt则是固定小权重(也称学习率), 用逼近伪残差函数本身代替权重. D: 根据中心极限定律, 随着n的增加, 训练误差和测试误差之间的差别必然减少 — 这就是大数据训练的由来 169. 在spss的基础分析模块中，作用是“以行列表的形式揭示数据之间的关系”的是（ ）大数据 Hadoop/spark 易 A. 数据描述 B. 相关 C. 交叉表 D. 多重相应 170. 一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求：（）机器学习 ML基础 易 A. 二分类问题 B. 多分类问题 C. 层次聚类问题 D. k-中心点聚类问题 E. 回归问题 F. 结构分析问题 @刘炫320，本题题目及解析来源：机器学习习题集 二分类：每个分类器只能把样本分为两类。监狱里的样本分别为狱警、小偷、送餐员、其他。二分类肯 定行不通。瓦普尼克95年提出来基础的支持向量机就是个二分类的分类器，这个分类器学习过 程就是解一个基于正负二分类推导而来的一个最优规划问题（对偶问题），要解决多分类问题 就要用决策树把二分类的分类器级联，VC维的概念就是说的这事的复杂度。 层次聚类： 创建一个层次等级以分解给定的数据集。监狱里的对象分别是狱警、小偷、送餐员、或者其他，他们等级应该是平等的，所以不行。此方法分为自上而下（分解）和自下而上（合并）两种操作方式。 K-中心点聚类：挑选实际对象来代表簇，每个簇使用一个代表对象。它是围绕中心点划分的一种规则，所以这里并不合适。 回归分析：处理变量之间具有相关性的一种统计方法，这里的狱警、小偷、送餐员、其他之间并没有什 么直接关系。 结构分析： 结构分析法是在统计分组的基础上，计算各组成部分所占比重，进而分析某一总体现象的内部结构特征、总体的性质、总体内部结构依时间推移而表现出的变化规律性的统计方法。结构分析法的基本表现形式，就是计算结构指标。这里也行不通。 多分类问题： 针对不同的属性训练几个不同的弱分类器，然后将它们集成为一个强分类器。这里狱警、 小偷、送餐员 以及他某某，分别根据他们的特点设定依据，然后进行区分识别。 171. 关于 logit回归和SVM不正确的是（）机器学习 ML模型 易 A.Logit回归目标函数是最小化后验概率 B. Logit回归可以用于预测事件发生概率的大小 C. SVM目标是结构风险最小化 D.SVM可以有效避免模型过拟合 @刘炫320，本题题目及解析来源：机器学习习题集 A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误 B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确 C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。 D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。 172. 有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是()机器学习 ML基础 易 A. 2x+y=4 B. x+2y=5 C. x+2y=3 D. 2x-y=0 解析：这道题简化了，对于两个点来说，最大间隔就是垂直平分线，因此求出垂直平分线即可。斜率是两点连线的斜率的负倒数-1/((-1-3)/(0-2)) = -1/2, 可得y=-(1/2)x + c, 过中点((0+2)/2, (-1+3)/2) = (1, 1), 可得c=3/2, 故选C. 173.下面有关分类算法的准确率，召回率，F1值的描述，错误的是？机器学习 ML基础 易 A.准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率 B.召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率 C.正确率、召回率和F值取值都在0和1之间，数值越接近0，查准率或查全率就越高 D.为了解决准确率和召回率冲突问题，引入了F1分数 解析： 对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作： TP——将正类预测为正类数 FN——将正类预测为负类数 FP——将负类预测为正类数 TN——将负类预测为负类数 由此： 精准率定义为：$P = \frac{TP}{(TP + FP)}$ 召回率定义为：$R = \frac{TP}{(TP + FN)}$ F1值定义为： $F1 = \frac{2PR}{(P + R)}$ 精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。 174. 以下几种模型方法属于判别式模型(Discriminative Model)的有() 机器学习 ML模型 易 A 混合高斯模型 B 条件随机场模型 C 区分度训练 D 隐马尔科夫模型 @刘炫320，本题题目及解析来源：机器学习习题集 常见的判别式模型有： Logistic regression（logistical 回归） Linear discriminant analysis（线性判别分析） Supportvector machines（支持向量机） Boosting（集成学习） Conditional random fields（条件随机场） Linear regression（线性回归） Neural networks（神经网络） 常见的生成式模型有: Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型） Hidden Markov model（隐马尔可夫） NaiveBayes（朴素贝叶斯） AODE（平均单依赖估计） Latent Dirichlet allocation（LDA主题模型） Restricted Boltzmann Machine（限制波兹曼机） 生成式模型是根据概率（需要求联合概率）乘出结果，而判别式模型是给出输入，由决策函数或者条件概率计算出结果。 175. 在Logistic Regression 中,如果同时加入L1和L2范数,会产生什么效果() 机器学习 ML基础 易 A.可以做特征选择,并在一定程度上防止过拟合 B.能解决维度灾难问题 C.能加快计算速度 D.可以获得更准确的结果 @刘炫320，本题题目及解析来源：机器学习习题集 L1范数具有稀疏解的特性，但是要注意的是，L1没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。它的优良性质是能产生稀疏性，导致W中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。所以能加快计算速度和缓解维数灾难. 所以BC正确. 在代价函数后面加上正则项，L1即是lasso回归，L2是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数是指向量各元素的平方和然后求平方根，用于防止过拟合，提升模型的泛化能力。因此选择A。 对于机器学习中的范数规则化，也就是L0,L1,L2范数的详细解答，请参阅《范数规则化》 176. 机器学习中L1正则化和L2正则化的区别是？ 机器学习 ML基础 易 A.使用L1可以得到稀疏的权值 B.使用L1可以得到平滑的权值 C.使用L2可以得到稀疏的权值 D.使用L2可以得到平滑的权值 177. 位势函数法的积累势函数K(x)的作用相当于Bayes判决中的() A.后验概率 B.先验概率 C.类概率密度 D.类概率密度与先验概率的乘积 178. 隐马尔可夫模型三个基本问题以及相应的算法说法正确的是（ ） A.评估—前向后向算法 B.解码—维特比算法 C.学习—Baum-Welch算法 D.学习—前向后向算法 解析：评估问题，可以使用前向算法、后向算法、前向后向算法。 179. 特征比数据量还大时，选择什么样的分类器？机器学习 ML基础 易 线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分 来自: 机器学习面试题大全 180. 下列属于无监督学习的是： 机器学习 ML基础 易 A.k-means B.SVM C.最大熵 D.CRF 解析： A是聚类，BC是分类，D是序列化标注，也是有监督学习。 181. 数据清理中，处理缺失值的方法是? 机器学习 ML基础 易 A.估算 B.整例删除 C.变量删除 D.成对删除 @刘炫320，本题题目及解析来源：机器学习习题集 由于调查、编码和录入误差，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法有：估算，整例删除，变量删除和成对删除。 估算(estimation)：最简单的办法就是用某个变量的样本均值、中位数或众数代替无效值和缺失值。这种办法简单，但没有充分考虑数据中已有的信息，误差可能较大。另一种办法就是根据调查对象对其他问题的答案，通过变量之间的相关分析或逻辑推论进行估计。例如，某一产品的拥有情况可能与家庭收入有关，可以根据调查对象的家庭收入推算拥有这一产品的可能性。 整例删除(casewise deletion)：是剔除含有缺失值的样本。由于很多问卷都可能存在缺失值，这种做法的结果可能导致有效样本量大大减少，无法充分利用已经收集到的数据。因此，只适合关键变量缺失，或者含有无效值或缺失值的样本比重很小的情况。 变量删除(variable deletion)：如果某一变量的无效值和缺失值很多，而且该变量对于所研究的问题不是特别重要，则可以考虑将该变量删除。这种做法减少了供分析用的变量数目，但没有改变样本量。 成对删除(pairwise deletion)：是用一个特殊码(通常是9、99、999等)代表无效值和缺失值，同时保留数据集中的全部变量和样本。但是，在具体计算时只采用有完整答案的样本，因而不同的分析因涉及的变量不同，其有效样本量也会有所不同。这是一种保守的处理方法，最大限度地保留了数据集中的可用信息。 采用不同的处理方法可能对分析结果产生影响，尤其是当缺失值的出现并非随机且变量之间明显相关时。因此，在调查中应当尽量避免出现无效值和缺失值，保证数据的完整性。 182. 关于线性回归的描述,以下正确的有（） 机器学习 ML基础 易 A.基本假设包括随机干扰项是均值为0,方差为1的标准正态分布 B.基本假设包括随机干扰下是均值为0的同方差正态分布 C.在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量 D.在违背基本假设时,模型不再可以估计 E.可以用DW检验残差是否存在序列相关性 F.多重共线性会使得参数估计值方差减小 解析：AB一元线性回归的基本假设有： 1、随机误差项是一个期望值或平均值为0的随机变量； 2、对于解释变量的所有观测值，随机误差项有相同的方差； 3、随机误差项彼此不相关； 4、解释变量是确定性变量，不是随机变量，与随机误差项彼此之间相互独立； 5、解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵； 6、随机误差项服从正态分布 CD违背基本假设的计量经济学模型还是可以估计的，只是不能使用普通最小二乘法进行估计。 当存在异方差时，普通最小二乘法估计存在以下问题： 参数估计值虽然是无偏的，但不是最小方差线性无偏估计。 E杜宾-瓦特森（DW）检验，计量经济，统计分析中常用的一种检验序列一阶 自相关 最常用的方法。 F所谓多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。影响 （1）完全共线性下参数估计量不存在 （2）近似共线性下OLS估计量非有效 多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀因子(Variance Inflation Factor, VIF) （3）参数估计量经济含义不合理 （4）变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外 （5）模型的预测功能失效。变大的方差容易使区间预测的“区间”变大，使预测失去意义。 对于线性回归模型,当响应变量服从正态分布,误差项满足高斯–马尔科夫条件（零均值、等方差、不相关）时,回归参数的最小二乘估计是一致最小方差无偏估计。 当然，该条件只是理想化的假定，为的是数学上有相应的较为成熟的结论。其实大多数实际问题都不完全满足这些理想化的假定。 线性回归模型理论的发展正是在不断克服理想化条件不被满足时得到许多新方法。如加权LSE、岭估计、压缩估计、BOX_COX变换等一系列段。做实际工作时一定是要超越书本上的理想化条件的。 183. 以下哪个是常见的时间序列算法模型（）机器学习 ML模型 易 A.RSI B.MACD C.ARMA D.KDJ 解析： C. 自回归滑动平均模型(ARMA) 其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。 其他三项都不是一个层次的： A.相对强弱指数 (RSI, Relative Strength Index) 是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 . B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence), 是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 . D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 (常为9日 ,9周等)内出现过的最高价, 最低价及最后一个计算周期的收盘价及这三者之间的比例关系, 来计算最后一个计算周期的未成熟随机值RSV, 然后根据平滑移动平均线的方法来计算K值 , D值与J值, 并绘成曲线图来研判股票走势 . 184. 下列不是SVM核函数的是（） 机器学习 ML模型 易 A.多项式核函数 B.logistic核函数 C.径向基核函数 D.Sigmoid核函数 @刘炫320，本题题目及解析来源：机器学习习题集 SVM核函数包括: 线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。 核函数的定义并不困难，根据泛函的有关理论，只要一种函数$K(x_i, x_j)$满足Mercer条件，它就对应某一变换空间的内积。对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型： (1)线性核函数 K(x, x_i) = x ⋅ x_i (2)多项式核 K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d (3)径向基核（RBF） K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 ) Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。 (4)傅里叶核 K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 ) (5)样条核 K ( x , x i ) = B 2 n + 1 ( x − x i ) (6)Sigmoid核函数 K ( x , x i ) = tanh ( κ ( x , x i ) − δ ) 采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。 核函数的选择：在选取核函数解决实际问题时，通常采用的方法有： 一是利用专家的先验知识预先选定核函数； 二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多． 三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想． 185. 解决隐马模型中预测问题的算法是（）机器学习 ML模型 中 A.前向算法 B.后向算法 C.Baum-Welch算法 D.维特比算法 @刘炫320，本题题目及解析来源：机器学习习题集 A、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。 C：Baum-Welch算法解决的是一个模型训练(学习)问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现； D：维特比算法解决的是给定一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测（解码）问题，通信中的解码问题。 186. 一般，k-NN最近邻方法在（）的情况下效果较好 机器学习 ML模型 易 A.样本较多但典型性不好 B.样本较少但典型性好 C.样本呈团状分布 D.样本呈链状分布 解析：K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B 样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。 187. 下列方法中，可以用于特征降维的方法包括（） 深度学习 DL模型 易 A.主成分分析PCA B.线性判别分析LDA C.深度学习SparseAutoEncoder D.矩阵奇异值分解SVD E.最小二乘法LeastSquares 解析：降维的3种常见方法ABD，都是线性的。深度学习是降维的方法这个就比较新鲜了，事实上，细细想来，也是降维的一种方法，因为如果隐藏层中的神经元数目要小于输入层，那就达到了降维，但如果隐藏层中的神经元如果多余输入层，那就不是降维了。最小二乘法是线性回归的一种解决方法，其实也是投影，但是并没有进行降维。 188. 下面哪些是基于核的机器学习算法? 机器学习 ML模型 易 A.Expectation Maximization（EM）（最大期望算法） B.Radial Basis Function（RBF）（径向基核函数） C.Linear Discrimimate Analysis（LDA）（线性判别分析法） D.Support Vector Machine（SVM）（支持向量机） 解析：径向基核函数是非常常用的核函数，而线性判别分析法的常规方法是线性的，但是当遇到非线性的时候，同样可以使用核方法使得非线性问题转化为线性问题。支持向量机处理非线性的问题的时候，核函数也是非常重要的。 189. 试证明样本空间中任意点$x$到超平面$(w, b)$的距离为$d = \frac{|w^Tx+b}{||w||}$ 机器学习 ML基础 易 (周志华《机器学习》课后习题) 190. 从网上下载或自己实现一个卷积神经网络，并在手写字体识别数据MNIST上进行实验测试。深度学习 DL模型 中 解析详见：周志华《机器学习》课后习题解答系列（六） 191. 神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的？深度学习 DL基础 中 @Hengkai Guo，本题解析来源：神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的？ 说说我对一个好的激活函数的理解吧，有些地方可能不太严谨，欢迎讨论。（部分参考了Activation function。） 1. 非线性：即导数不是常数。这个条件前面很多答主都提到了，是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。 2. 几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。 3. 计算简单：正如题主所说，非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。 4. 非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x&gt;0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。 8. 参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。 9. 归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。 参考文献： [1] Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016. [2] Lin M, Chen Q, Yan S. Network in network[J]. arXiv preprint arXiv:1312.4400, 2013. [3] Maas A L, Hannun A Y, Ng A Y. Rectifier nonlinearities improve neural network acoustic models[C]//Proc. ICML. 2013, 30(1). [4] He K, Zhang X, Ren S, et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1026-1034. [5] Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks[C]//Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. 2010: 249-256. [6] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778. [7] Goodfellow I J, Warde-Farley D, Mirza M, et al. Maxout networks[J]. arXiv preprint arXiv:1302.4389, 2013. [8] Klambauer G, Unterthiner T, Mayr A, et al. Self-Normalizing Neural Networks[J]. arXiv preprint arXiv:1706.02515, 2017. [9] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[C]//International Conference on Machine Learning. 2015: 448-456. 192. 梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？深度学习 DL基础 中 @李振华，梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？ 深度神经网络“容易收敛到局部最优”，很可能是一种想象，实际情况是，我们可能从来没有找到过“局部最优”，更别说全局最优了。很多人都有一种看法，就是“局部最优是神经网络优化的主要难点”。这来源于一维优化问题的直观想象。在单变量的情形下，优化问题最直观的困难就是有很多局部极值，如 人们直观的想象，高维的时候这样的局部极值会更多，指数级的增加，于是优化到全局最优就更难了。然而单变量到多变量一个重要差异是，单变量的时候，Hessian矩阵只有一个特征值，于是无论这个特征值的符号正负，一个临界点都是局部极值。但是在多变量的时候，Hessian有多个不同的特征值，这时候各个特征值就可能会有更复杂的分布，如有正有负的不定型和有多个退化特征值（零特征值）的半定型 在后两种情况下，是很难找到局部极值的，更别说全局最优了。 现在看来，神经网络的训练的困难主要是鞍点的问题。在实际中，我们很可能也从来没有真的遇到过局部极值。Bengio组这篇文章Eigenvalues of the Hessian in Deep Learning里面的实验研究给出以下的结论： • Training stops at a point that has a small gradient. The norm of the gradient is not zero, therefore it does not, technically speaking, converge to a critical point.• There are still negative eigenvalues even when they are small in magnitude 另一方面，一个好消息是，即使有局部极值，具有较差的loss的局部极值的吸引域也是很小的Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes。 For the landscape of loss function for deep networks, the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima. 所以，很可能我们实际上是在“什么也没找到”的情况下就停止了训练，然后拿到测试集上试试，“咦，效果还不错”。 补充说明，这些都是实验研究结果。理论方面，各种假设下，深度神经网络的Landscape的鞍点数目指数增加，而具有较差loss的局部极值非常少。 193. 请比较下EM算法、HMM、CRF。机器学习 ML模型 中 这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。 （1）EM算法 EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。 注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。 （2）HMM算法 隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。 马尔科夫三个基本问题： 概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法 学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。 预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径） （3）条件随机场CRF 给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。 之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。 （4）HMM和CRF对比 其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。 194. CNN常用的几个模型。深度学习 DL模型 中 名称 特点 LeNet5 没啥特点-不过是第一个CNN应该要知道 AlexNet 引入了ReLU和dropout，引入数据增强、池化相互之间有覆盖，三个卷积一个最大池化+三个全连接层 VGGNet 采用1*1和3*3的卷积核以及2*2的最大池化使得层数变得更深。常用VGGNet-16和VGGNet19 Google Inception Net 这个在控制了计算量和参数量的同时，获得了比较好的分类性能，和上面相比有几个大的改进：1、去除了最后的全连接层，而是用一个全局的平均池化来取代它；2、引入Inception Module，这是一个4个分支结合的结构。所有的分支都用到了1*1的卷积，这是因为1*1性价比很高，可以用很少的参数达到非线性和特征变换。 3、Inception V2第二版将所有的5*5变成2个3*3，而且提出来著名的Batch Normalization；4、Inception V3第三版就更变态了，把较大的二维卷积拆成了两个较小的一维卷积，加速运算、减少过拟合，同时还更改了Inception Module的结构。 微软ResNet残差神经网络(Residual Neural Network) 1、引入高速公路结构，可以让神经网络变得非常深。2、ResNet第二个版本将ReLU激活函数变成y=x的线性函数 195. 带核的SVM为什么能分类非线性问题？ 机器学习 SVM 中 核函数的本质是两个函数的內积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个內积 常用核函数及核函数的条件： 核函数选择的时候应该从线性核开始，而且在特征很多的情况下没有必要选择高斯核，应该从简单到难的选择模型。我们通常说的核函数指的是正定核函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。 RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。 线性核：主要用于线性可分的情况。 多项式核 196. Boosting和Bagging 机器学习 集成模型 难 (1). 随机森林 a). Boostrap从袋内有放回的抽取样本值 b). 每次随机抽取一定数量的特征（通常为sqr(n)）。 分类问题：采用Bagging投票的方式选择类别频次最高的。(投票法) 回归问题：直接取每颗树结果的平均值。（均值法） 常见参数 误差分析 优点 缺点 1、树最大深度2、树的个数3、节点上的最小样本数4、特征数(sqr(n)) oob(out-of-bag)将各个树的未采样样本作为预测样本统计误差作为误分率 可以并行计算不需要特征选择可以总结出特征重要性可以处理缺失数据不需要额外设计测试集 在回归上不能输出连续结果 (2). Boosting之AdaBoost Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。 (3). Boosting之GBDT 将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。 注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。 (4). Xgboost 1. 这个工具主要有以下几个特点： 2. 支持线性分类器 3. 可以自定义损失函数，并且可以用二阶偏导 4. 加入了正则化项：叶节点数、每个叶节点输出score的L2-norm 5. 支持特征抽样 6. 在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。 197. 逻辑回归相关问题。机器学习 LR 中 （1）公式推导一定要会 （2）逻辑回归的基本概念 这个最好从广义线性模型的角度分析，逻辑回归是假设y服从Bernoulli分布。 （3）L1-norm和L2-norm 其实稀疏的根本还是在于L0-norm也就是直接统计参数不为0的个数作为规则项，但实际上却不好执行于是引入了L1-norm；而L1-norm本质上是假设参数先验是服从Laplace分布的，而L2-norm是假设参数先验为Gaussian分布，我们在网上看到的通常用图像来解答这个问题的原理就在这。 但是L1-norm的求解比较困难，可以用坐标轴下降法或是最小角回归法求解。 （4）LR和SVM对比 首先，LR和SVM最大的区别在于损失函数的选择，LR的损失函数为Log损失（或者说是逻辑损失都可以）、而SVM的损失函数为hinge loss + L2 norm minw,b∑_{i}^{N}[1−y_i(w∗x_i+b)]+λ||w||2 其次，两者都是线性模型。 最后，SVM只考虑支持向量（也就是和分类相关的少数点） （5）LR和随机森林区别 随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。 （6）常用的优化方法 逻辑回归本身是可以用公式求解的，但是因为需要求逆的复杂度太高，所以才引入了梯度下降算法。 一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。 二阶方法：牛顿法、拟牛顿法： 这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。 拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有: DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。 198. 用贝叶斯机率说明Dropout的原理 机器学习 Dropout 难 @许韩，来源：深度学习岗位面试问题整理笔记 Dropout as a Bayesian Approximation: Insights and Applications 199. 为什么很多做人脸的Paper会最后加入一个Local Connected Conv？ @许韩，来源：深度学习岗位面试问题整理笔记 以FaceBook DeepFace DeepFace: Closing the Gap to Human-Level Performance in Face Verification为例： DeepFace先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层，这里是用Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。 200. 什么是共线性, 跟过拟合有什么关联? @抽象猴，来源：如果你是面试官，你怎么去判断一个面试者的深度学习水平？ 共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。 共线性会造成冗余，导致过拟合。 解决方法：排除变量的相关性／加入权重正则。 未完待续……]]></content>
      <tags>
        <tag>面试</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS31n学习笔记k-NN与线性分类器]]></title>
    <url>%2F2018%2F08%2F20%2FCS31n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C----k-NN%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1、前言 任务：图像分类问题，就是对于输入的图像数据，根据已有的分类标签集合，选出合适的标签对齐进行分类标记，属于监督学习的范畴。虽然该问题对人来说非常简单，但却是计算机视觉领域的核心问题，计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。 难点：对于计算机来说，一张图像是由[Hight,Width,Channel]组成的张量(Tensor)，张量中的元素为像素值大小，大小在0-255之间的整型，其中0表示全黑，255表示全白。因此图像分类的目标就是：把这些上百万的数字变成一个简单的标签(label)。可想而知，计算机视觉算法所应对的困难与挑战有多大。 1). 视角变化（Viewpoint variation） 2). 大小变化（Scale variation） 3). 形变（Deformation） 4). 遮挡（Occlusion） 5). 光照条件（Illumination conditions） 6). 背景干扰（Background clutter） 7). 类内差异（Intra-class variation） 算法及流程：要应对如此多的变化，采用单一的规则是不可能解决图像分类的问题。那我们不如给计算机“看”大量各种各样猫的照片，然后让计算机自己学习并识别。这种基于大量已标注数据，通过计算机学习来完成某种任务的方法，称为：数据驱动方法。其完整流程包含：输入训练集（已标注数据），学习分类器或者模型，评价分类器质量（验证集）。 2、K-NN K-NN没有显式训练过程，给定一个训练集，对于新输入的测试数据，在训练集中找到与该实例最近邻的K个实例，其中类别数最多的类为该实例的预测标签。 K-NN三要素：距离度量、K值选择以及分类决策。 2.1 距离度量 如何定义两个实例(图像)间的距离？是一种可选择的超参数。 L1 distance（曼哈顿距离）：d_1(I_1,I_2) = \sum_{p}|I_1^p-I_2^p| L2 distance（欧氏距离）：d_2(I_1,I_2) = \sqrt{\sum_{p}|I_1^p-I_2^p|^2} Lp distance（闵可夫斯基距离）：d_p(I_1,I_2) = (\sum_{p}|I_1^p-I_2^p|^p)^\frac{1}{p} $L_\infty$ distance（切比雪夫距离）：d_\infty(I_1,I_2) = \max_{p}|I_1^p-I_2^p| 马氏距离：d(\vec x,\vec y)=\sqrt{(\vec x-\vec y)^TS^{-1}(\vec x-\vec y)} 余弦距离：d(\vec x,\vec y)=1 - \frac{\vec x .\vec y}{|\vec x||\vec y|} 汉明距离：字符串x变成y所需要的最小的替换次数 数据集：CIFAR-10 10类 50000训练图片，10000测试图片 衡量两张图片的相似度，对于图像的L1距离计算： 选择L1距离的K-NN代码如下：123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred 2.2 k值的选择 另一个超参数(人为设定的参数)，k值选择会对算法结果产生非常大的影响。 如果k值较小，相当于用较小的领域中的训练实例进行预测，“学习”的近似误差会减小，只有与输入实例较近的训练实例才会对预测结果起作用。但缺点是估计误差会增大，即预测结果对近邻点实例非常敏感，若近邻实例为噪声就会导致错误的预测结果。极端地，若k=1，预测结果为最近领点所属类别。 如果k值较大，相当于用较大的领域中的训练实例进行预测，优点在于“学习”的估计误差会减小，但缺点是近似误差会增大，这时与输入实例较远的（不相似的）训练实例也会起作用。极端地，若k=N，模型过于简单，完全忽略了训练集中有用的信息，无论输入实例是什么，预测结果均为训练集中类别最多的类。 那么K值如何选择呢？简单的，尝试各种不同的参数，选择效果最好的。但是，如果只是在测试集评价效果最好是不可取的。一是因为测试集为最终模型泛化性能的评价而存在的；二是因为很可能会在测试集上过拟合了，无法在其他新数据达到很好泛化结果。 因此，常采用留一法或k折交叉验证法（5折/10折居多）来选取k值。 2.3 分类决策 常采用多数表决法，等价于经验风险最小化。 2.4 缺点 1). 速度慢，K-NN测试速度随着训练集规模线性增长，CNN相反：训练时间长，测试时间短（固定的计算）。 2). 复杂度高，如何降低最近邻分类器的计算复杂度也是研究的热门，在预处理阶段建立kd树或使用k-means聚类等，加速在数据集中查找最近邻的效率。近似最近邻算法（Approximate Nearest Neighbor (ANN) algorithms）如(FLANN)，牺牲了一定的最近邻精度来换取空间/时间复杂度的降低。 3). 不适用于维数大、容量大的数据，最近邻分类器在某些情况下可能会是一种好的选择，特别是数据维度比较低、数据量较小的时候，但是对于图像分类问题来说它基本不适合。原因之一是图像是高维物体（包含很多像素），在高维空间中进行距离运算通常不可靠。 3、线性分类器 相比于K-NN，线性分类器是基于参数化方法得到分类结果。 $score = f(x, W) = Wx+b$ Ｘ为输入图像3072个输入神经元[3072, 1]，Ｗ为权重[10, 3072]（学习的参数）$score$[10, 1]为预测得分。b[10, 1]为偏置。 具体计算例子： 将在CIFAIR数据集训练得到的W可视化，10类的特征权重如下图： a）马头和汽车存在左右方向，可看出对应参数出现两个头的情况。 问题： a）对于线分类器，最难分类的数据样本是哪些？ 实质为不同位置的颜色带权混合，因此针对不同位置、不同纹理难识别。]]></content>
      <tags>
        <tag>笔记</tag>
        <tag>cs231n</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS31n学习笔记计算机视觉发展史]]></title>
    <url>%2F2018%2F08%2F19%2FCS31n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80----%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%8F%91%E5%B1%95%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[1、前言 CS231n是斯坦福大学李飞飞团队2015年冬季学期开始开设的一门基于神经网络或者说卷积神经网络的计算机视觉课程，全称是CS231n: Convolutional Neural Networks for Visual Recognition，如今完全成为了一门明星课程，也是每一个CVer入门的必学课程之一。目前，已经有2015冬季版，2016冬季版，2017春季版，2018春季版（视频未公布），每学期的视频更新都会引起一波充电热潮。 互联网和传感器的发展，特别是视觉传感器，如：手机摄像头，数码相机，视频监控，行车记录仪等等，引发了视觉信息的大爆炸。不得不说，我们完全进入到了一个视觉时代，一个图片、视频信息爆炸的时代。但是这些信息也是最难以被利用的信息，我们称其为“互联网中的暗物质”。就像银河系中85%的质量属于暗物质和暗能量，难以被检测和利用。YouTube每60秒就会接受150小时以上的视频上传，我们根本不可能靠人眼给如此大量的数据进行标注、分类。那么如何对这些数据进行标记、分类、索引等工作，进而利用这些数据来做广告、检索或者其他操作。唯一的希望就是利用计算机去帮我们完成这项工作，运用计算机视觉技术对图片进行标签、分类，对视频进行理解。如何更好地利用这些海量数据，如何应对“暗物质”的挑战，如何让计算机更好地理解这个世界，这就是计算机视觉要解决的问题。 计算机视觉是一门跨学科的课程，所以，我们面对的问题，建模方式也必将是跨学科的，像工程学、物理学、生物学、心理学、计算机科学和数学都有着密切关系。 一些资源：http://cs231n.github.io/classification/ 2、计算机视觉简史2.1 计算机视觉的诞生 第一只眼的诞生：在五亿四千三百万年前，单细胞生物就已在地球上出没，当时的动物物种只有三个动物门。而在短短的五百万年后(这段时期仅占生物演化史的千分之一)，这三个动物门的物种突然演化成三十八个动物门，几乎现今所有动物的祖先，全在一瞬间蜂涌而出。这个物种快速暴增的现象发生在寒武纪初期，称为寒武纪大爆发，其原因至今仍是未解之谜。其中，Andrew Parker的“光开关理论”最具说服力，他研究发现，当时由于一个偶然的原因，浅海和大气中的化学物质发生了变化，使得浅海和大气的透明度大大增加。随着大量的光线进入海洋，于是动物开始进化出接收光的器官——眼睛。约5.4３亿年前出现的三叶虫正是第一批演化出真正眼睛的动物，刚进化的眼睛非常非常的简陋就像针孔相机，甚至没有晶体状，只能接收一点光线和感知环境信息。该理论认为正是眼睛的进化，引发了生命体之间的进化竞赛，使得那些具备最佳视觉的生物才最有可能生存下来。眼睛的出现，可能是地球上寒武纪生命大爆发的主要原因。 达芬奇的相机：16世纪文艺复兴时期，达芬奇发明了“照相暗盒”（camera obscura），开始用来获取真实世界中的图像信息，这种早期的照相设备并不能产生出照片，只是运用小孔成像原理，方便艺术家描绘物体的比例。但这便是现代视觉工程技术的开端了，人们开始复制我们所看到的信息，不过这并没有涉及到试图去理解所看到的信息。从此之后，视觉技术取得一定的进步，如：电影技术、柯达商用相机产品、摄像机产品等等。 大脑视觉处理:1959年，Huber&amp;Wiesel研究生物的大脑是如何处理视觉信息的。他们发现，大脑并不是对整体目标进行处理，而是从简单的形状（例如，边缘）开始处理视觉信息。在视觉处理的第一步，基础视觉区的神经元按一列一列组织起来，每一列神经元只“喜欢”某一种特定的形状,某种线条的简单组合。1981年两位科学家凭借这个贡献获得了诺贝尔奖，这是视觉领域一项极其重要的成就。 方块世界：现代计算机视觉领域的先驱是Lary Roberts在1963年的论文，名为“方块世界”Block World，也是计算机视觉第一篇博士论文。视觉信息处理是基于边缘好形状，是边缘决定了形状。 计算机视觉的诞生：计算机视觉诞生于1966年夏季，MIT成立了人工智能实验室，其中一名教授开始着手研究计算机视觉问题。至此，计算机视觉成为AI领域增长最快的一个领域，计算机视觉顶级大会CVPR和ICCV每年都有来及自全世界的进行这方面研究的研究人员。 2.2 近现代发展 VISION：David Marr写了一本非常有影响力的书籍VISION，并给了我们第二个非常重要的观点（第一个为Huber&amp;Wiesel认为的视觉处理是从一些简单的形状开始，而不是整体目标）：视觉是分层的。复杂的视觉处理可以基于这两个观点进行，即从简单形状开始，并建立一个分层模型。David Marr认为图像结构可以分为多层，第一层为简单的边缘结构（同Huber&amp;Wiesel）他叫其为原始草图，接下来他称之为2.5D草图，将表层、深度信息或者场景的不连续性拼凑在一起。这里将2D图像信息调整为包含真实世界的3D信息。第三层为3D真实世界的模型。这是一个高度抽象的架构，并不能指引建立相应数学模型，但这是一个非常重要的、宏观的概念思想。基于David Marr的思考方式，开始涌现出一波视觉识别算法，即重建3D模型，以便于进一步识别。其中，Generalized Cylinder模型（1979年）认为世界由简单形状组合，通过不同角度观察而来。Pictorial Structure模型（1973年），其认为物体有简单的部分组成，如人脸由眼镜鼻子和嘴组成，各部分之间通过“弹簧”连接，允许之间出现一定形变。 Normalized Cut：（Shi &amp; Malik，1997年）第一次使用现实世界照片解决彩色图像分割问题。感知分组：视觉领域最为重要的问题。 VJ人脸检测器：第一个转化为智能人脸检测的产品，应用于富士康相机2006年的数码相机产品中。基于haar特征+adboost分类器。也是第一项在计算机上可以实时运行的计算机视觉方面的研究成果。虽然这并不是这个时期的唯一成果，但这件成果反应了计算机视觉领域研究聚焦的一次变迁，不再是David Marr的重建3D模型再识别，而是直接识别“物体是什么”。该成果将研究的焦点聚焦到识别领域，这个趋势，将计算机视觉带回了人工智能领域。 Feature： a）SIFT（David Lowe，1999年）：尺度不变特征转换(Scale-invariant feature transform)，步骤：尺度空间极点检测—关键点精确定位—关键点的方向确定—特征向量的生成 b）SPM（Lazebnik, Schmid &amp; Ponce, 2006年）：空间金字塔匹配（Spatial Pyramid Matching)，Spatial：将图像分成若干块(sub-regions)，分别统计每一子块的特征，最后将所有块的特征拼接起来，形成完整的特征。Pyramid：在分块的细节上，采用了一种多尺度的分块方法，即分块的粒度越大越细(increasingly fine)，呈现出一种层次金字塔的结构。Matching：匹配 c）HOG（Dalal &amp; Triggs, 2005年）：梯度方向直方图(Histogram of Oriented Gradient)，步骤：全局图像归一化—计算图像梯度—统计局部图像梯度信息—归一化—生成特征描述向量， d）DPM（Felzenswalb, McAllester, Ramanan,2009年）：可变形的组件模型（Deformable Part Model），88分辨率的根滤波器（Root filter）+ 44分辨率的组件滤波器（Part filter）。 2.3 深度学习的时代 数据： ａ）MNIST(Modified National Institute of Standards and Technology, Yann LeCun 1998年),70,000张扫描的手写数字字体照片（每张为0-9中的一个数字）。 ｂ）Pascal VOC(PASCAL Visual Object Challenge,Everingham et al. 2006-2012年),包含20类的目标检测数据集，10000级别的照片数量。 ｃ）ImageNet(Deng, Dong, Socher, Li, Li, &amp; Fei-Fei, 2009年)，包含2万分类，千万级别（5000万）的识别检测数据集。从2010年开始，每年举行图像识别比赛：1,000 类目标，1,431,167张图像，被称为计算机视觉领域的奥铃匹克竞赛。2012年卷积神经网络（CNN）以巨大优势取得冠军，深度学习再次掀起研究热潮，成为深度学习革命的开端。 模型： ａ）LeNet:(Yann LeCun 1998年)，结构：conv+pooling+conv+pooling+conv+FC+FC，激活函数为sigmoid，用于MNIST手写字体识别。 ｂ）AlexNet:(Krizhevsky et al. 2012年)，结构：conv1+pooling+conv2+pooling+conv3+conv4+conv5+fc6+fc7+fc8，激活函数Relu，运用多个GPU训练。 ｃ）ZFNet：(Matthew D. Zeiler et al. 2013年)，结构：AlexNet一些参数的改动，对CNN提取特征进行可视化。 ｄ）GoogLeNet：(Szegedy et al. 2014年)，结构：inception结构。 ｅ）VGG：(Simonyan 2014年)，结构：（VGG16）conv1+conv2+pooling+conv3+conv4+pooling+conv5+conv6+conv7+pooling+conv8+conv9。+conv10+pooling+conv11+conv12+conv13+pooling+FC14+FC15+FC16。 ｆ）ResNet：(Kaiming He et al. 2015年)，结构：residual block。 ｇ）DenseNet：(Huang et al. 2017年)，结构：ResNet的密集链接。 硬件： 摩尔定律，NVIDIA GPU计算能力。 3、课程主题 CS231n关注视觉识别问题—图像分类(image classification)，以及大量相关问题，如：目标检测(object detection), 图像描述(image captioning)…。 计算机视觉解决的问题远不止识别问题，还有一大堆亟待解决的问题，如：密集标记、感知分组、3D场景识别，动作理解等等。计算机视觉的终极目的：理解，让计算机像人一样理解这个世界，并让世界变得更美好。]]></content>
      <tags>
        <tag>笔记</tag>
        <tag>cs231n</tag>
        <tag>课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode刷题(1-100)]]></title>
    <url>%2F2018%2F08%2F01%2FLeetcode%E5%88%B7%E9%A2%98%EF%BC%881-100%EF%BC%89%2F</url>
    <content type="text"><![CDATA[LeetCode是一个准备面试非常有用的网站，是每个程序员求职面试必刷的题库之一，中文版本网站领扣。其中题目按照难易程度分为易，中，难三等，同时也按照专题进行了分类。本博客记录本人的刷题过程，参考了很多大神的优质实现，权当笔记，供自己以后复习之用。 1. Two Sum（求两个数的和） 题目：给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 方法一：双重循环，时间按复杂度:O(n) 方法二：通过哈希表/字典存储数组元素及位置，判断target - nums[i]是否在字典中,同样元素不能重复：下标不能一样。时间复杂度：O(n),空间复杂度：O(n) 方法三：排序后，左右“指针”对应数字求和，若小于target，小指针往后移动(low++),若大于target，大指针往前移动(high—)，相等，返回结果。时间复杂度：O(n),空间复杂度：O(1)12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ d = dict() res = [] for ind, i in enumerate(nums): d[i] = ind for ind, i in enumerate(nums): tmp = target - i if tmp in d and ind != d[tmp]: res.append(ind) res.append(d[tmp]) return res return res def twoSum2(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ res = [] nums = [(k,v) for v, k in enumerate(nums)] nums = sorted(nums, key=lambda d:d[0]) low = 0 high = len(nums) -1 while low &lt; high: sum2 = nums[low][0] + nums[high][0] if sum2 == target: res.append(nums[low][1]) res.append(nums[high][1]) return res elif sum2 &lt; target: low += 1 else: high -= 1 return res 2. Add Two Numbers（单链表表示的两个数相加） 题目：给定两个非空链表来表示两个非负整数。位数按照逆序方式存储，它们的每个节点只存储单个数字。将两数相加返回一个新的链表。 你可以假设除了数字 0 之外，这两个数字都不会以零开头。 示例： 输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807 方法：遍历链表相加sum(链表为空、进位)，新节点数字为（sum%10），进位为sum/10(python中使用//整除)，注意考虑最终是否还有进位。123456789101112131415161718192021222324252627282930# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ jinwei = 0 res = ListNode(0) cur1 = l1 cur2 = l2 cur = res while cur1 != None or cur2 != None: x = cur1.val if cur1 != None else 0 y = cur2.val if cur2 != None else 0 sum2 = jinwei + x + y jinwei = sum2 // 10 cur.next = ListNode(sum2 % 10) cur = cur.next if cur1 != None:cur1 = cur1.next if cur2 != None:cur2 = cur2.next if jinwei &gt; 0: cur.next = ListNode(jinwei) return res.next 3. Longest Substring Without Repeating Characters（最长非重复子字符串） 题目：给定一个字符串，找出不含有重复字符的最长子串的长度。 示例： 输入: “pwwkew”输出: 3解释: 无重复字符的最长子串是 “wke”，其长度为 3。请注意，答案必须是一个子串，”pwke” 是一个子序列 而不是子串。 方法：用字典charindex存储每个字符的位置，start表示子串开始位置，更新策略为：如果当前字符已经在字典存在，并且start小于charindex[char] + 1,同时维护最长不重复子串长度res。123456789101112131415class Solution: def lengthOfLongestSubstring(self, s): """ :type s: str :rtype: int """ charindex = &#123;&#125; res = 0 start = 0 for ind, char in enumerate(s): if char in charindex: start = max(charindex[char] + 1, start) res = max(res, ind-start+1) charindex[char] = ind return res 4. Median of Two Sorted Arrays（两个排序数组的中位数） 题目：给定两个大小为 m 和 n 的有序数组 nums1 和 nums2 。请找出这两个有序数组的中位数。要求算法的时间复杂度为 O(log (m+n)) 。你可以假设 nums1 和 nums2 不同时为空。 示例： nums1 = [1, 3]，nums2 = [2]，中位数是 2.0nums1 = [1, 2]，nums2 = [3, 4]，中位数是 (2 + 3)/2 = 2.5 方法：参考[here],1234567891011121314151617181920class Solution: def findMedianSortedArrays(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: float """ a, b = sorted((nums1, nums2), key=len) m, n = len(a), len(b) after = (m + n - 1) // 2 lo, hi = 0, m while lo &lt; hi: i = (lo + hi) // 2 if after-i-1 &lt; 0 or a[i] &gt;= b[after-i-1]: hi = i else: lo = i + 1 i = lo nextfew = sorted(a[i:i+2] + b[after-i:after-i+2]) return (nextfew[0] + nextfew[1 - (m+n)%2]) / 2.0 5. Longest Palindromic Substring（最长回文子串） 题目：给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为1000。 示例： 输入: “babad”输出: “bab”注意: “aba”也是一个有效答案。 方法一：暴力枚举，枚举所有子串（最长子串开始）+判断子串是否为回文，时间复杂度为：O(n^3) 方法一：动态规划，时间复杂度为：O(n^2) 方法二：Manacher算法，时间复杂度为：O(n),分析参考最长回文子串（Longest Palindromic Substring）——三种时间复杂度的解法123456789101112131415161718192021222324252627282930313233class Solution: def longestPalindrome(self, s): """ :type s: str :rtype: str """ s='#'+'#'.join(s)+'#' RL=[0]*len(s) MaxRight=0 Pos=0 Maxlen=0 for i in range(len(s)): if i &lt; MaxRight: RL[i]=min(RL[2*Pos-i],MaxRight-i) else: #i在maxright右边，以i为中心的回文串还没扫到，此时，以i为中心向两边扩展 RL[i]=1 #RL=1：只有自己 #以i为中心扩展，直到左 != 右or达到边界(先判断边界) while i-RL[i]&gt;=0 and i+RL[i]&lt;len(s) and s[i-RL[i]]==s[i+RL[i]]: RL[i]+=1 #更新Maxright pos: if RL[i]+i-1&gt;MaxRight: MaxRight=RL[i]+i-1 Pos=i #更新最长回文子串的长; Maxlen=max(RL) s=s[RL.index(Maxlen)-(Maxlen-1):RL.index(Maxlen)+(Maxlen-1)] s=s.replace('#','') return s 6. 006-ZigZag Conversion（Z字型转换） 题目：将字符串 “PAYPALISHIRING” 以Z字形排列成给定的行数： P A H N A P L S I I G Y I R 示例： 输入: s = “PAYPALISHIRING”, numRows = 4输出: “PINALSIGYAHRPI” 方法：数学规律， 第一行和最后一行的字母下标数（下标数从0开始）可以看成一个等差数列，中间的几行也有规律。把满列的和单列的分成两部分来看，满列的按等差数列来做，单列的用单列的规律加进去。12345678910111213141516171819class Solution(object): def convert(self, s, numRows): """ :type s: str :type numRows: int :rtype: str """ if numRows &lt;= 1: return s n = len(s) ans = [] step = 2 * numRows - 2 for i in range(numRows): for j in range(i, n, step): ans.append(s[j]) if i != 0 and i != numRows - 1 and j - 2*i + step &lt; n: ans.append(s[j - 2*i + step]) return "".join(ans) 7. Reverse Integer（翻转整数） 题目：给定一个 32 位有符号整数，将整数中的数字进行反转。 示例：输入: 123输出: 321输入: -123 输出: -321 方法：通过求余数求商法进行操作。 class Solution: def reverse(self, x): &quot;&quot;&quot; :type x: int :rtype: int &quot;&quot;&quot; res= 0 tmp = x if x &gt; 0 else -x while tmp != 0: res = res * 10 + tmp % 10 tmp = tmp // 10 if res &gt; 2**31-1 or res &lt; (-2)**31: res = 0 res = res if x &gt; 0 else -res return res 7. String to Integer (atoi) （字符串转成整数）]]></content>
      <tags>
        <tag>面试</tag>
        <tag>笔记</tag>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BAT机器学习面试题库(1-100)]]></title>
    <url>%2F2018%2F07%2F20%2FBAT%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E5%BA%93%EF%BC%881-100%EF%BC%89%2F</url>
    <content type="text"><![CDATA[为了求职笔试面试，需恶补基础、算法原理，于是仔细研读了七月在线发布的BAT机器学习面试1000题系列，也添加了一些自己的理解或来自其他博客的答案，以下内容均来自BAT机器学习面试1000题系列。该文为本人的阅读笔记，主要是为了记忆和自查。 前言July我又回来了。 之前本博客整理过数千道微软等公司的面试题，侧重数据结构、算法、海量数据处理，详见：微软面试100题系列，今17年，近期和团队整理BAT机器学习面试1000题系列，侧重机器学习、深度学习。我们将通过这个系列索引绝大部分机器学习和深度学习的笔试面试题、知识点，它将更是一个足够庞大的机器学习和深度学习面试库/知识库，通俗成体系且循序渐进。 此外，有四点得强调下： 虽然本系列主要是机器学习、深度学习相关的考题，其他类型的题不多，但不代表应聘机器学习或深度学习的岗位时，公司或面试官就只问这两项，虽说是做数据或AI相关，但基本的语言（比如Python）、编码coding能力（对于开发，编码coding能力怎么强调都不过分，比如最简单的手写快速排序、手写二分查找）、数据结构、算法、计算机体系结构、操作系统、概率统计等等也必须掌握。对于数据结构和算法，一者 重点推荐前面说的微软面试100题系列（后来这个系列整理成了新书《编程之法：面试和算法心得》），二者 多刷leetcode，看1000道题不如实际动手刷100道。 本系列会尽量让考察同一个部分（比如同是模型/算法相关的）、同一个方向（比如同是属于最优化的算法）的题整理到一块，为的是让大家做到举一反三、构建完整知识体系，在准备笔试面试的过程中，通过懂一题懂一片。 本系列每一道题的答案都会确保逻辑清晰、通俗易懂（当你学习某个知识点感觉学不懂时，十有八九不是你不够聪明，十有八九是你所看的资料不够通俗、不够易懂），如有更好意见，欢迎在评论下共同探讨。 关于如何学习机器学习，最推荐机器学习集训营系列。从Python基础、数据分析、爬虫，到数据可视化、spark大数据，最后实战机器学习、深度学习等一应俱全。 另，本系列会长久更新，直到上千道、甚至数千道题，欢迎各位于评论下留言分享你在自己笔试面试中遇到的题，或你在网上看到或收藏的题，共同分享帮助全球更多人，thanks。 BAT机器学习面试1000题系列 1. 请简要介绍下SVM。 机器学习 ML模型 易 SVM，全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据（特征空间）的分类算法，它的目标是为确定一个分类超平面（间隔最大化），从而将不同的数据分隔开。 线性可分支持向量机：训练数据线性可分，通过硬间隔最大化，训练得到线性分类器。 线性支持向量机：训练数据近似线性可分，通过软间隔最大化，也可以学习得到线性分类器。 线性不可分支持向量机：训练数据线性不可分，通过核技巧 + 软间隔最大化，学习非线性支持向量机。 《统计学习方法》$P_{96}$-$p_{135}$ 支持向量机通俗导论（理解SVM的三层境界） 机器学习之深入理解SVM 纯白板手推SVM 2. 请简要介绍下Tensorflow的计算图。 深度学习 DL框架 中 @寒小阳：Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个计算（op）都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。 Tensorflow一般分为两部分：构造计算流图部分 + 通过session输入数据执行构造图中的计算。 TF python库中默认有一个计算图（default graph, tf.get_default_graph() 调用），也可以自定义计算图（tf.Graph()）1a=x*y;b=a+z;c=tf.reduce_sum(b) CS 20SI: Tensorflow for Deep Learning Research tensorflow学习笔记（1）如何高效地学习TensorFlow（附链接） 3. 在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别？ 机器学习 ML模型 中 欧氏距离，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量、L2距离，它定义于欧几里得空间中，如点 x = (x1,…,xn) 和 y = (y1,…,yn) 之间的距离为： d(x,y) =\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ...+(x_n - y_n)^2} = \sqrt{\sum_{i=1}^n(x_i-y_i)^2} 欧氏距离虽然很有用，但也有明显的缺点。它将样品的不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。例如，在教育研究中，经常遇到对人的分析和判别，个体的不同属性对于区分个体有着不同的重要性。因此，欧氏距离适用于向量各分量的度量标准统一的情况。欧氏距离可用于任何空间的距离计算问题(无坐标空间限制)。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。 曼哈顿距离，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：$|x_1 - x_2| + |y_1 - y_2|$，要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。当坐标轴变动时，点间的距离就会不同。通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。 曼哈顿距离和欧式距离一般用途不同，无相互替代性。k-means或kNN中，距离的选择也是一个可选超参数。另，关于各种距离的比较参看《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》。 4. CNN的卷积核是单层的还是多层的？ 深度学习 DL模型 中 @AntZ：卷积运算的定义和理解可以看下这篇文章《CNN笔记：通俗理解卷积神经网络》，在CNN中,卷积计算属于离散卷积, 本来需要卷积核的权重矩阵旋转180度(相关与卷积), 但我们并不需要旋转前的权重矩阵形式, 故直接用旋转后权重矩阵作为卷积核表达, 这样的好处就离散卷积运算变成了矩阵点积运算。 一般而言，深度卷积网络是一层又一层的。层的本质是特征图, 存贮输入数据或其中间表示值。一组卷积核则是联系前后两层的网络参数表达体, 训练的目标就是每个卷积核的权重参数组。 描述网络模型中某层的厚度，通常用名词通道channel数或者特征图feature map数。不过人们更习惯把作为数据输入的前层的厚度称之为通道数（比如RGB三色图层称为输入通道数为3），把作为卷积输出的后层的厚度称之为特征图数。 卷积核(filter)一般是3D多层的，除了面积参数, 比如3x3之外, 还有厚度参数H（2D的视为厚度1). 还有一个属性是卷积核的个数N。 卷积核的厚度H, 一般等于前层厚度M(输入通道数或feature map数). 特殊情况M &gt; H。 卷积核的个数N, 一般等于后层厚度(后层feature maps数，因为相等所以也用N表示)。 卷积核的大小K, 一般为奇数大小（３, ５, ７）的正方形模板。 卷积核通常从属于后层，为后层提供了各种查看前层特征的视角，这个视角是自动形成的。一层的参数为：H x K x K x N个，输出N个feature map 卷积核厚度等于1时为2D卷积，也就是平面对应点分别相乘然后把结果加起来，相当于点积运算. 各种2D卷积动图可以看这里 卷积核厚度大于1时为3D卷积(depth-wise)，每片平面分别求2D卷积，然后把每片卷积结果加起来，作为3D卷积结果；1x1卷积属于3D卷积的一个特例(point-wise)，有厚度无面积, 直接把每层单个点相乘再相加。 归纳之，卷积的意思就是把一个区域，不管是一维线段，二维方阵，还是三维长方块，全部按照卷积核的维度形状，从输入挖出同样维度形状, 对应逐点相乘后求和，浓缩成一个标量值也就是降到零维度，作为输出到一个特征图的一个点的值. 这个很像渔夫收网。 可以比喻一群渔夫坐一个渔船撒网打鱼，鱼塘是多层水域，每层鱼儿不同。 船每次移位一个stride到一个地方，每个渔夫撒一网，得到收获，然后换一个距离stride再撒，如此重复直到遍历鱼塘。 A渔夫盯着鱼的品种，遍历鱼塘后该渔夫描绘了鱼塘的鱼品种分布； B渔夫盯着鱼的重量，遍历鱼塘后该渔夫描绘了鱼塘的鱼重量分布； 还有N-2个渔夫，各自兴趣各干各的； 最后得到N个特征图，描述了鱼塘的一切！ 2D卷积表示渔夫的网就是带一圈浮标的渔网，只打上面一层水体的鱼； 3D卷积表示渔夫的网是多层嵌套的渔网，上中下层水体的鱼儿都跑不掉； 1x1卷积可以视为每次移位stride，甩钩钓鱼代替了撒网； 下面解释一下特殊情况的 M &gt; H： 实际上，除了输入数据的通道数比较少之外，中间层的feature map数很多，这样中间层算卷积会累死计算机（鱼塘太深，每层鱼都打，需要的鱼网太重了）。所以很多深度卷积网络把全部通道/特征图划分一下，每个卷积核只看其中一部分（渔夫A的渔网只打捞深水段，渔夫B的渔网只打捞浅水段）。这样整个深度网络架构是横向开始分道扬镳了，到最后才又融合。这样看来，很多网络模型的架构不完全是突发奇想，而是是被参数计算量逼得。特别是现在需要在移动设备上进行AI应用计算(也叫推断), 模型参数规模必须更小, 所以出现很多减少握手规模的卷积形式, 现在主流网络架构大都如此。比如AlexNet： 另，附百度2015校招机器学习笔试题 5. 关于LR。 机器学习 ML模型 难 @rickjin：把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。 另外，下面资料可作学习参考：《统计学习方法$P_{77}-P_{94}$》、Logistic Regression 的前世今生（理论篇）、机器学习算法与Python实践之（七）逻辑回归（Logistic Regression）。 6. （过拟合）overfitting怎么解决？机器学习 ML基础 中 @AntZ: overfitting就是过拟合, 也就是随着训练过程的进行，模型复杂度增加，在training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集, 对训练集外的数据却不work, 这称之为泛化(generalization)性能不好。泛化性能是训练的效果评价中的首要目标，没有良好的泛化，就等于南辕北辙, 一切都是无用功。 产生原因：算法的学习能力过强；一些假设条件（如样本独立同分布）可能是不成立的；训练样本过少不能对整个空间进行分布估计。 解决方法：dropout、regularization、batch normalizatin、Early stopping、数据扩增（Data augmentatio）、模型融合、交叉验证、特征选择/特征降维 7. LR和SVM的联系与区别。机器学习 ML模型 中 @朝阳在望，联系： 1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。 3、均属于有监督判别模型。 区别： 1、LR是参数模型，SVM是非参数模型。 2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss + L2，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 3、SVM的处理方法是只考虑support vectors（边界线局部的点），也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。 5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做（核方法）的logic有的做不了。来源：机器学习常见面试问题（一） 6、LR输出为概率，SVM输出为类别。 8. 说说你知道的核函数。机器学习 ML基础 易 通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如： - 多项式核：$K(x, z) = (x\cdot z + 1)^p$，对应的SVM是一个p次多项式分类器。分类决策函数为：$f(x) = sign(\sum_{i=1}^{N_S}a_{i}^{\ast}y_i(x_i \cdot x+1)^p + b^\ast)$ - 高斯核:$K(x, z) = exp(-\frac{||x-z||^2}{2\sigma^2})$，对应的SVM是高斯径向基函数分类器，分类决策函数为：$f(x) = sign(\sum_{i=1}^{N_S}a_{i}^{\ast}y_iexp(-\frac{||x-z||^2}{2\sigma^2}) + b^\ast)$，这个核就是最可以将原始空间映射为无穷维空间的那个家伙。不过，如果$\sigma$选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果$\sigma$选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数$\sigma$，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间： - 线性核:$K(x_1,x_2) = (x_1, x_2)$，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了(意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的)。 9. LR与线性回归的区别与联系。机器学习 ML模型 中等 @AntZ: LR工业上一般指Logistic Regression(逻辑回归)而不是Linear Regression(线性回归). LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围, 其目标函数也因此从差平方和函数变为对数损失函数, 以提供最优化所需导数（sigmoid函数是softmax函数的二元特例, 其导数均为函数值的$f(x)\cdot (1-f(x))$形式）。请注意, LR往往是解决二元0/1分类问题的, 只是它和线性回归耦合太紧, 不自觉也冠了个回归的名字(马甲无处不在). 若要求多元分类,就要把sigmoid换成大名鼎鼎的softmax了。 @nishizhen：个人感觉逻辑回归和线性回归首先都是广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 @乖乖癞皮狗：逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。 10. 请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么？机器学习 ML模型 难 @AntZ:集成学习的集成对象是学习器. Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器, 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器, 迭代集成(平滑加权)。 决策树属于最常用的学习器, 其学习过程是从根建立树, 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂, CART决策树用基尼指数计算最优分裂, xgboost每颗决策树模型使用二阶泰勒展开系数计算最优分裂。 下面所提到的学习器都是决策树: Bagging方法: 降低方差（variance） 学习器间不存在强依赖关系, 学习器可并行训练生成, 集成方式一般为投票; Random Forest属于Bagging的代表, 放回抽样, 每个学习器随机选择部分特征去优化; Boosting方法: 降低偏差（bias） 学习器之间存在强依赖关系、必须串行生成, 集成方式为加权和; Adaboost属于Boosting, 采用指数损失函数替代原本分类任务的0/1损失函数; 区别：样本量、样本权重、决策方法、可并行性 xgboost属于Boosting的集大成者, 对函数残差近似值进行梯度下降, 迭代时利用了二阶梯度信息, 集成模型可分类也可回归. 由于它可在特征粒度上并行计算, 结构风险和工程实现都做了很多优化, 泛化, 性能和扩展性都比GBDT要好。 关于决策树，这里有篇《决策树算法》。而随机森林Random Forest是一个包含多个决策树的分类器。至于AdaBoost，则是英文”Adaptive Boosting”（自适应增强）的缩写，关于AdaBoost可以看下这篇文章《Adaboost 算法的原理与推导》。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。 @Xijun LI：xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有： 1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数 2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性 3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的 更多详见：集成学习总结 11. 为什么xgboost要用泰勒展开，优势在哪里？机器学习 ML模型 难 @AntZ：xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。 12. xgboost如何寻找最优特征？是又放回还是无放回的呢？机器学习 ML模型 难 @AntZ：xgboost在训练的过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据, 从而记忆了每个特征对在模型训练时的重要性 — 从根到叶子中间节点涉及某特征的次数作为该特征重要性排序. xgboost属于boosting集成学习方法, 样本是不放回的, 因而每轮计算样本不重复. 另一方面, xgboost支持子采样, 也就是每轮计算可以不使用全部样本, 以减少过拟合. 进一步地, xgboost还有列采样, 每轮计算按百分比随机采样一部分特征, 既提高计算速度又减少过拟合。 13. 谈谈判别式模型和生成式模型？机器学习 ML基础 易 判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。 生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。 由生成模型可以得到判别模型，但由判别模型得不到生成模型。 常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场（CRF） 常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机 14. L1和L2的区别。机器学习 ML基础 易 L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|. 简单总结一下就是： L1范数: 为x向量各个元素绝对值之和。 L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或者Frobenius范数 Lp范数: 为x向量各个元素绝对值p次方和的1/p次方. 在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。 L1范数可以使权值稀疏，方便特征提取。 L2范数可以防止过拟合，提升模型的泛化能力。 @AntZ: L1和L2的差别，为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢？看导数一个是1一个是w便知, 在靠进零附近, L1以匀速下降到零, 而L2则完全停下来了. 这说明L1是将不重要的特征(或者说, 重要性不在一个数量级上)尽快剔除, L2则是把特征贡献尽量压缩最小但不至于为零。 两者一起作用（Elastic Net）, 就是把重要性在一个数量级(重要性最高的)的那些特征一起平等共事(简言之, 不养闲人也不要超人)。 15. L1和L2正则先验分别服从什么分布。机器学习 ML基础 易 @齐同学：面试中遇到的，L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。 @AntZ: 先验就是优化的起跑线, 有先验的好处就是可以在较小的数据集中有良好的泛化性能，当然这是在先验分布是接近真实分布的情况下得到的了，从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。 对参数引入高斯正态先验分布相当于L2正则化, 这个大家都熟悉，如下图(左)：$f(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$,对参数引入拉普拉斯先验等价于 L1正则化, 如下图(右)：$f(x| \mu, b) = \frac{1}{2b}e^{-\frac{|x-\mu|}{b}}$ 从上面两图可以看出, L2先验趋向零周围, L1先验趋向零本身。 16. CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？深度学习 DL应用 难 @许韩，来源：深度学习岗位面试问题整理笔记 - 以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的。 - CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。 - CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。 局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图： 17. 说一下Adaboost，权值更新公式。当弱分类器是$G_m$时，每个样本的的权重是$w_1，w_2…$，请写出最终的决策公式。机器学习 ML模型 难 给定一个训练数据集$T={(x_1,y_1), (x_2,y_2)…(x_N,y_N)}$，其中实例$x \in \mathcal{X}$，而实例空间$\mathcal{X} \subset \mathbb{R}^n$，$y_i$属于标记集合{-1,+1}，Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成(权值求和)一个强分类器 主要做法是增加难分类样本权重，减小易分类样本权值；加大错误率小的弱分类权值，使其在表决中其较大的作用，减小分类错误率大的弱分类器权值，使其在表决中其较小作用。 Adaboost的算法流程如下： 步骤1. 初始化样本权重。首先，初始化训练数据的权值分布。每一个训练样本最开始时都被赋予相同的权值：1/N。 D_1 = (w_{11},..., w_{1i}, ..., w_{1N}), w_{1i} = \frac{1}{N}, i=1,2,...,N 步骤2. 进行多轮迭代，用$m = 1,2, …, m$表示迭代的第多少轮 a).使用具有权值分布$D_m$的训练数据集学习，得到基本分类器（选取让误差率最低的阈值来设计基本分类器）： G_m(x):\mathcal{X} \to \{-1,1\} b).计算$G_m(x)$在训练集上的分类误差。 e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^{N}w_{mi}I(G_m(x_i) \neq y_i) 由上述式子可知，$G_m(x)$在训练数据集上的误差率em就是被Gm(x)误分类样本的权值之和。 c).计算$G_m(x)$的系数，$a_m$表示$G_m(x)$在最终分类器中的重要程度（目的：得到基本分类器在最终分类器中所占的权重）： a_m = \frac{1}{2}\log{\frac{1-e_m}{e_m}} 由上述式子可知，$e_m &lt;= 1/2$时，$a_m &gt;= 0$，且$a_m$随着$e_m$的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。 d).更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代 D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1, N}), w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp (-a_my_iG_m(x_i)), i=1,2,...,N 使得被基本分类器$G_m(x)$误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过这样的方式，AdaBoost方法能“重点关注”或“聚焦于”那些较难分的样本上 其中，$Z＿m$是规范化因子，使得$D_{m+1}$成为一个概率分布： Z_m = \sum_{i=1}^{N}w_{mi} \exp (-a_my_iG_m(x_i)) 步骤3. 组合各个弱分类器。 f(x) = \sum_{i=1}^{M}a_mG_m(x) 从而得到最终分类器，如下： G(x) = sign(f(x)) = sign(\sum_{i=1}^{M}a_mG_m(x)) 更多请查看此文：《Adaboost算法的原理与推导》 18. LSTM结构推导，为什么比RNN好？ 深度学习DL模型 难 推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。 《深度理解LSTM》 19. 根据谷歌一员工写的How to Write a Spelling Corrector显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现”拼写检查”的功能。机器学习 ML应用 难 用户输入一个单词时，可能拼写正确，也可能拼写错误。如果把拼写正确的情况记做c（代表correct），拼写错误的情况记做w（代表wrong），那么”拼写检查”要做的事情就是：在发生w的情况下，试图推断出c。换言之：已知w，然后在若干个备选方案中，找出可能性最大的那个c，也就是求$P(c|w)$的最大值。而根据贝叶斯定理，有： P(c|w)=\frac{P(w|c)P(c)}{P(w)} 由于对于所有备选的c来说，对应的都是同一个w，所以它们的P(w)是相同的，因此我们只要最大化$P(w|c)P(c)$即可。其中: P(c)表示某个正确的词的出现”概率”，它可以用”频率”代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，P(c)就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。 P(w|c)表示在试图拼写c的情况下，出现拼写错误w的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，P(w|c)就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。值得一提的是，一般把这种问题称为“编辑距离”，参见:]程序员编程艺术第二十八~二十九章：最大连续乘积子串、字符串编辑距离。 所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。具体的计算过程及此方法的缺陷请参见How to Write a Spelling Corrector 20. 为什么朴素贝叶斯如此“朴素”？机器学习 ML模型 易 因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。 21. 请大致对比下plsa和LDA的区别。机器学习 ML模型 中等 pLSA:主题分布和词分布确定后，以一定的概率$(P(z_k|d_i), P(w_j|z_k))$分别选取具体的主题和词项，生成好文档。而后根据生成好的文档反推其主题分布、词分布时，最终用EM算法（极大似然估计思想）求解出了两个未知但固定的参数的值：(由$P(w_j|z_k)$转换而来)和(由$P(z_k|d_i)$转换而来)。 文档d产生主题z的概率，主题z产生单词w的概率都是两个固定的值 举个文档d产生主题z的例子。给定一篇文档d，主题分布是一定的，比如${P(z_i|d), i = 1,2,3 }$可能就是{0.4,0.5,0.1}，表示z1、z2、z3，这3个主题被文档d选中的概率都是个固定的值：$P(z_1|d) = 0.4、P(z_2|d) = 0.5、P(z_3|d) = 0.1$，如下图所示（图截取自沈博PPT上）： LDA:但在贝叶斯框架下的LDA中，我们不再认为主题分布（各个主题在文档中出现的概率分布）和词分布（各个词语在某个主题下出现的概率分布）是唯一确定的（而是随机变量），而是有很多种可能。但一篇文档总得对应一个主题分布和一个词分布吧，怎么办呢？LDA为它们弄了两个Dirichlet先验参数，这个Dirichlet先验为某篇文档随机抽取出某个主题分布和词分布。 文档d产生主题z（准确的说，其实是Dirichlet先验为文档d生成主题分布Θ，然后根据主题分布Θ产生主题z）的概率，主题z产生单词 w的概率都不再是某两个确定的值，而是随机变量。 还是再次举下文档d具体产生主题z的例子。给定一篇文档d，现在有多个主题z1、z2、z3，它们的主题分布${ P(z_i|d), i = 1,2,3 }$可能是{0.4,0.5,0.1}，也可能是{0.2,0.2,0.6}，即这些主题被d选中的概率都不再认为是确定的值，可能是$P(z_1|d) = 0.4、P(z_2|d) = 0.5、P(z_3|d) = 0.1$，也有可能是$P(z_1|d) = 0.2、P(z_2|d) = 0.2、P(z_3|d) = 0.6$等等，而主题分布到底是哪个取值集合我们不确定（为什么？这就是贝叶斯派的核心思想，把未知参数当作是随机变量，不再认为是某一个确定的值），但其先验分布是dirichlet分布，所以可以从无穷多个主题分布中按照dirichlet 先验随机抽取出某个主题分布出来。如下图所示（图截取自沈博PPT上）： 换言之，LDA在pLSA的基础上给这两参数$(P(z_k|d_i), P(w_j|z_k))$加了两个先验分布的参数（贝叶斯化）：一个主题分布的先验分布Dirichlet分布$\alpha$，和一个词语分布的先验分布Dirichlet分布$\beta$。 更多请参见：《通俗理解LDA主题模型》22. 请简要说说EM算法。机器学习 ML模型 中等 @tornadomeet，本题解析来源：机器学习&amp;数据挖掘笔记_16（常见面试之机器学习算法思想简单梳理） 有时候因为样本的产生和隐含变量有关（隐含变量是不能观察的），而求模型的参数时一般采用最大似然估计，由于含有了隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数的（对应模型参数个数可能有多个），EM算法一般分为2步： E步：选取一组参数，求出在该参数下隐含变量的条件概率值； M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。 重复上面2步直至收敛。公式如下所示：E-step:\\ For\ each\ i, set\ Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)}; \theta) \\ M-step:\\ set\ \theta:=argmax_\theta \sum_i \sum_{z^{(i)}}Q_i(z^{(i)})log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} M步公式中下界函数的推导过程： EM算法一个常见的例子就是高斯混合模型(Gaussian Mixed Model, GMM)模型，每个样本都有可能由k个高斯产生，只不过由每个高斯产生的概率不同而已，因此每个样本都有对应的高斯分布（k个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。 GMM的E步公式如下（计算每个样本对应每个高斯的概率）： 更具体的计算公式为： M步公式如下(计算每个高斯的比重，均值，方差这3个参数)： 23. KNN中的K如何选取的？机器学习 ML模型 易 关于什么是KNN，可以查看此文：《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》。KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说： a). 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合； b). 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。 K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。 在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。 24. 机器学习中，为何要经常对数据做归一化。机器学习 ML基础 中等 @zhanlijun，参考链接：为什么一些机器学习模型需要对数据进行归一化？ 1）归一化后加快了梯度下降求最优解的速度；2）归一化有可能提高精度。 常见归一化类型：线性归一化、标准差标准化、非线性归一化。 25. 谈谈深度学习中的归一化问题。深度学习 DL基础 易 详情参见此视频：深度学习中的归一化 26. 哪些机器学习算法不需要做归一化处理？机器学习 ML基础 易 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少（加快了梯度下降求最优解的速度）。 另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。 @管博士：我理解归一化和标准化主要是为了使计算更方便 比如两个变量的量纲不同 可能一个的数值远大于另一个那么他们同时作为变量的时候 可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确 或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能 量纲也需要调整 所以我估计lr 和 knn 标准化一下应该有好处。至于其他的算法 我也觉得如果变量量纲差距很大的话 先标准化一下会有好处。 @寒小阳：一般我习惯说树形模型，这里说的概率模型可能是差不多的意思。 27. 请简要说说一个完整机器学习项目的流程。机器学习 ML应用 中 a). 抽象成数学问题 明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。 这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如何划归为其中的某类问题。 b). 获取数据 数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。 数据要有代表性，否则必然会过拟合。 而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。(类别均衡) 而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。 c). 特征预处理与特征选择（特征工程） 良好的数据要能够提取出良好的特征才能真正发挥效力。 特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。 特征选择: 筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。 d). 训练模型与调优 直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数（调参），使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。 e). 模型诊断 如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。 过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。 误差分析也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题…… 诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。 f). 模型融合 一般来说，模型融合后都能使得效果有一定提升。而且效果很好。 工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫【特征工程 + 模型融合】。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。 g).上线运行 这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。 这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。 故，基于此，七月在线每一期ML算法班都特此增加特征工程、模型调优等相关课。比如，这里有个公开课视频《特征处理与特征选择》。 28. 逻辑斯特回归为什么要对特征进行离散化。机器学习 ML模型 中等 @严林，本题解析来源：https://www.zhihu.com/question/31989952 在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 0. 离散特征的增加和减少都很容易，易于模型的快速迭代； 1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。 李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。 29. 下列哪个不属于CRF模型对于HMM和MEMM模型的优势（B） 机器学习 ML模型 中等 A. 特征灵活 B. 速度快 C. 可容纳较多上下文信息 D. 全局最优 CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢 首先，CRF(条件随机场)，HMM(隐马模型)，MEMM(最大熵隐马模型)都常用来做序列标注的建模. 隐马模型一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择 最大熵隐马模型则解决了隐马的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉 条件随机场则很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。 此外《机器学习工程师第八期》里有讲概率图模型。 30. 什么是熵。机器学习 ML基础 易 熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量。 1948年，香农Claude E. Shannon引入信息（熵），将其定义为离散随机事件的出现概率。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。 信息熵计算公式：$H(x) = -\sum P(x_i)\log(p(x_i)), (i=1,2,…n)$ 更多请查看《最大熵模型中的数学推导》。 31. 熵、联合熵、条件熵、相对熵、互信息的定义。机器学习 ML基础 中等 为了更好的理解，需要了解的概率必备知识有： a). 大写字母X表示随机变量，小写字母x表示随机变量X的某个具体的取值； b). P(X)表示随机变量X的概率分布，P(X,Y)表示随机变量X、Y的联合概率分布，P(Y|X)表示已知随机变量X的情况下随机变量Y的条件概率分布； c). p(X = x)表示随机变量X取某个具体值的概率，简记为p(x)； d). p(X = x, Y = y) 表示联合概率，简记为p(x,y)，p(Y = y|X = x)表示条件概率，简记为p(y|x)，且有：p(x,y) = p(x) p(y|x)。 *熵：如果一个随机变量X的可能取值为$X = \{x_1, x_2,…, x_k\}$，其概率分布为$P(X = x_i) = p_i（i = 1,2, …, n），则随机变量X的熵定义为： H(x) = -\sum P(x_i)\log(p(x_i)), (i=1,2,...n) 联合熵：两个随机变量X，Y的联合分布，可以形成联合熵Joint Entropy，用H(X,Y)表示。 条件熵：在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用H(Y|X)表示，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。且有此式子成立：$H(Y|X) = H(X,Y) – H(X)$，整个式子表示(X,Y)发生所包含的熵减去X单独发生包含的熵。至于怎么得来的请看推导： 简单解释下上面的推导过程。整个式子共6行，其中: 第二行推到第三行的依据是边缘分布p(x)等于联合分布p(x,y)的和； 第三行推到第四行的依据是把公因子logp(x)乘进去，然后把x,y写在一起； 第四行推到第五行的依据是：因为两个sigma都有p(x,y)，故提取公因子p(x,y)放到外边，然后把里边的-（log p(x,y) - log p(x)）写成- log (p(x,y)/p(x) ) ； 第五行推到第六行的依据是：p(x,y) = p(x) p(y|x)，故p(x,y) / p(x) = p(y|x)。 *相对熵：又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度等。设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是： D(p||q) = \sump(x)\log \frac{p(x)}{q(x)} = E_{p(x)}\log \frac{p(x)}{q(x)} 在一定程度上，相对熵可以度量两个随机变量的“距离”，且有D(p||q) ≠D(q||p)。另外，值得一提的是，D(p||q)是必然大于等于0的。 互信息：两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵，用I(X,Y)表示： I(X, Y) = \sum_{x, y}p(x, y)\log \frac{p(x, y)}{p(x)p(y)} 且有I(X,Y)=D(P(X,Y) || P(X)P(Y))。下面，咱们来计算下H(Y)-I(X,Y)的结果，如下： 通过上面的计算过程，我们发现竟然有H(Y)-I(X,Y) = H(Y|X)。故通过条件熵的定义，有：H(Y|X) = H(X,Y) - H(X)，而根据互信息定义展开得到H(Y|X) = H(Y) - I(X,Y)，把前者跟后者结合起来，便有I(X,Y)= H(X) + H(Y) - H(X,Y)，此结论被多数文献作为互信息的定义。更多请查看《最大熵模型中的数学推导》。 32. 什么是最大熵。机器学习 ML基础 易 熵是随机变量不确定性的度量，不确定性越大，熵值越大；若随机变量退化成定值，熵为0。如果没有外界干扰，随机变量总是趋向于无序，在经过足够时间的稳定演化，它应该能够达到的最大程度的熵。 为了准确的估计随机变量的状态，我们一般习惯性最大化熵，认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。换言之，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，其原则是承认已知事物（知识），且对未知事物不做任何假设，没有任何偏见。 例如，投掷一个骰子，如果问”每个面朝上的概率分别是多少”，你会说是等概率，即各点出现的概率均为1/6。因为对这个”一无所知”的色子，什么都不确定，而假定它每一个朝上概率均等则是最合理的做法。从投资的角度来看，这是风险最小的做法，而从信息论的角度讲，就是保留了最大的不确定性，也就是说让熵达到最大。 33. 了解正则化么。机器学习 ML基础 易 正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。 奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。 34. 协方差和相关性有什么区别？机器学习 ML基础 易 相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。 \sum_{ij} = cov(X_i, X_i) = E[(X_i - \mu_i)(X_j - \mu_j)] = E[X_iX_j]-\mu_i \mu_j 为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。 \rol{X, Y} = \frac{cov(X, Y)}{\sigma_X \sigma_Y}35. 线性分类器与非线性分类器的区别以及优劣。机器学习 ML基础 易 @伟祺，线性和非线性是针对，模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2那么就是非线性模型，如果输入是x和X^2则模型是线性的。 线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。 非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。 常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归（） 常见的非线性分类器：决策树、RF、GBDT、多层感知机(非线性激活函数) SVM两种都有（看线性核还是高斯核） 36. 简单说说贝叶斯定理。机器学习 ML模型 易 在引出贝叶斯定理之前，先学习几个定义： 条件概率（又称后验概率）就是事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P(A|B)，读作“在B条件下A的概率”。 比如，在同一个样本空间Ω中的事件或者子集A与B，如果随机从Ω中选出的一个元素属于B，那么这个随机选择的元素还属于A的概率就定义为在B的前提下A的条件概率，所以：P(A|B) = |A∩B|/|B|，接着分子、分母都除以|Ω|得到 P(A|B)=\frac{P(A \cap B)}{P(B)} 联合概率表示两个事件共同发生的概率。A与B的联合概率表示为$P(A∩B)$或者$P(A，B)$。 边缘概率（又称先验概率）是某个事件发生的概率。边缘概率是这样得到的：在联合概率中，把最终结果中那些不需要的事件通过合并成它们的全概率，而消去它们（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率），这称为边缘化（marginalization），比如A的边缘概率表示为P(A)，B的边缘概率表示为P(B)。 接着，考虑一个问题：P(A|B)是在B发生的情况下A发生的可能性。 首先，事件B发生之前，我们对事件A的发生有一个基本的概率判断，称为A的先验概率，用P(A)表示； 其次，事件B发生之后，我们对事件A的发生概率重新评估，称为A的后验概率，用P(A|B)表示； 类似的，事件A发生之前，我们对事件B的发生有一个基本的概率判断，称为B的先验概率，用P(B)表示； 同样，事件A发生之后，我们对事件B的发生概率重新评估，称为B的后验概率，用P(B|A)表示。 贝叶斯定理的公式表达式： P(A|B)=\frac{P(B|A)P(A)}{P(B)} 所以，贝叶斯公式可以直接根据条件概率的定义直接推出。即因为P(A,B) = P(A)P(B|A) = P(B)P(A|B)，所以P(A|B) = P(A)P(B|A) / P(B)。更多请参见此文：《从贝叶斯方法谈到贝叶斯网络》。 37. 某超市研究销售纪录数据后发现，买啤酒的人很大概率也会购买尿布，这种属于数据挖掘的哪类问题？(A) 数据挖掘 DM模型 易 A. 关联规则发现 B. 聚类 C. 分类 D. 自然语言处理 38. 将原始数据进行集成、变换、维度规约、数值规约是在以下哪个步骤的任务？(C) 数据挖掘 DM基础 易 A. 频繁模式挖掘 B. 分类和预测 C. 数据预处理 D. 数据流挖掘 39. 下面哪种不属于数据预处理的方法？ (D) 数据挖掘 DM基础 易 A. 变量代换 B. 离散化 C. 聚集 D. 估计遗漏值 40. 什么是KDD？ (A) 数据挖掘 DM基础 易 A. 数据挖掘与知识发现 B. 领域知识发现 C. 文档知识发现 D. 动态知识发现 41. 当不知道数据所带标签时，可以使用哪种技术促使带同类标签的数据与带其他标签的数据相分离？(B) 数据挖掘 DM模型 易 A. 分类 B. 聚类 C. 关联分析 D. 隐马尔可夫链 42. 建立一个模型，通过这个模型根据已知的变量值来预测其他某个变量值属于数据挖掘的哪一类任务？(C) 数据挖掘 DM基础 易 A. 根据内容检索 B. 建模描述 C. 预测建模 D. 寻找模式和规则 43. 以下哪种方法不属于特征选择的标准方法： (D) 数据挖掘 DM基础 易 A. 嵌入 B. 过滤 C. 包装 D. 抽样 44. 请用python编写函数find_string，从文本中搜索并打印内容，要求支持通配符星号和问号。Python Python语言 易12345678910find_string('hello\nworld\n','wor')['wor']find_string('hello\nworld\n','l*d')['ld']find_string('hello\nworld\n','o.')['or']# 答案import redef find_string(str,pat): return re.findall(pat,str,re.I) 45. 简单说下sigmoid激活函数。深度学习 DL基础 易 常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。这里先简要介绍下最基础的sigmoid函数（btw，在本博客中SVM那篇文章开头有提过）。 sigmoid的函数表达式如下： g(z) = \frac{1}{1 + e^{-z}} 其中z是一个线性组合，比如z可以等于：$z = wx + b$。通过代入很大的正数或很小的负数到g(z)函数中可知，其结果趋近于0或1。 因此，sigmoid函数g(z)的图形表示如下（ 横轴表示定义域z，纵轴表示值域g(z) ）： 也就是说，sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0。 压缩至0到1有何用处呢？用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。 举个例子，如下图（图引自Stanford机器学习公开课） $z = b + w_1 \cdot x_1 + w_2 \cdot x_2$，其中b为偏置项，假定取-30，$w_1,w_2$都取为20 - 如果 = 0 = 0，则z = -30，g(z) = 1/( 1 + e^-z )趋近于0。此外，从上图sigmoid函数的图形上也可以看出，当z=-30的时候，g(z)的值趋近于0 - 如果 = 0 = 1，或 =1 = 0，则z = b + + = -30 + 20 = -10，同样，g(z)的值趋近于0 - 如果 = 1 = 1，则z = b + + = -30 + 201 + 201 = 10，此时，g(z)趋近于1。 - 换言之，只有和都取1的时候，g(z)→1，判定为正样本；或取0的时候，g(z)→0，判定为负样本，如此达到分类的目的。 综上，sigmod函数，是逻辑斯蒂回归的压缩函数，它的性质是可以把分隔平面压缩到[0,1]区间一个数（向量），在线性分割平面值为0时候正好对应sigmod值为0.5，大于0对应sigmod值大于0.5、小于0对应sigmod值小于0.5；0.5可以作为分类的阀值；exp的形式最值求解时候比较方便，用相乘形式作为logistic损失函数，使得损失函数是凸函数；不足之处是sigmod函数在y趋于0或1时候有死区，控制不好在bp形式传递loss时候容易造成梯度弥撒。 导数：g(x)(1 - g(x)) 46. 什么是卷积。深度学习 DL基础 易 对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。 非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。 OK，举个具体的例子。比如下图中，图中左边部分是原始输入数据(三通道)，图中中间部分是滤波器filter（shape：3，3,3,2），图中右边是输出的feature map。 中间滤波器filter与数据窗口做内积，对应位置相乘相加（二维空间 + 通道），卷积核大小若为1x1，就相当于通道同一位置乘一个系数相加 47. 什么是CNN的池化pool层。深度学习 DL模型 易 池化，简言之，即取区域平均或最大，如下图所示（图引自cs231n） *CNN网络的pooling层有什么用： 1. invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度) 2. 保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，防止过拟合，提高模型泛化能力 *缺陷：丢失信息 上图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4。很简单不是？ 48. 简述下什么是生成对抗网络（GAN）。深度学习 DL扩展 中 GAN之所以是对抗的，是因为GAN的内部是竞争关系(引入博弈论的思想)，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。 更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。discriminator就是为了尽可能的区分假币与真币。 如下图中的左右两个场景： 48. 学梵高作画的原理是啥？深度学习 DL应用 难 这里有篇如何做梵高风格画的实验教程《教你从头到尾利用DL学梵高作画：GTX 1070 cuda 8.0 tensorflow gpu版》，至于其原理请看这个视频：NeuralStyle艺术化图片（学梵高作画背后的原理）。 风格迁移论文：《A Neural Algorithm of Artistic Style》 《Perceptual Losses for Real-Time Style Transfer and Super-Resolution》 《Learning Linear Transformations for Fast Arbitrary Style Transfer》 49. 现在有 a 到 z 26 个元素， 编写程序打印 a 到 z 中任取 3 个元素的组合（比如 打印 a b c ，d y z等） 数理逻辑 排列组合 中 解析参考：一道百度机器学习工程师职位的面试题 50. 说说梯度下降法。机器学习 ML基础 中 @LeftNotEasy，本题解析来源：机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)，下面是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。 我们用$X_1，X_2…X_n$去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向等等，我们可以做出一个估计函数： h(x) = h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 θ在这儿称为参数，在这儿的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。如果我们令$x_0 = 1$，就可以用向量的方式来表示了： h_\theta(x) = \theta^TX 我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个进行评估的函数称为损失函数（loss function），描述h函数不好的程度，在下面，我们称这个函数为J函数 在这儿我们可以做出下面的一个损失函数（MSE）: J(\theta)_{min_\theta J_\theta}=\frac{1}{2}\sum_{i=1}^m{(h_\theta(x^{(i)}) - y^{(i)})^2} 换言之，我们把对$x_i$的估计值$h(x_i)$与真实值$y_i$差的平方和作为损失函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了(便于求导)。 如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，是一种完全是数学描述的方法，另外一种就是梯度下降法。 梯度下降法的算法流程如下： 1）初始化：首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。 2）更新：改变θ的值，使得J(θ)按梯度下降的方向进行减少。 为了描述的更清楚，给出下面的图： 这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低，也就是达到深蓝色的部分。$θ_0，θ_1$表示θ向量的两个维度。 在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。 然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如下图（左）所示，算法的结束将是在θ下降到无法继续下降为止。 当然，可能梯度下降的最终点并非是全局最小点，即也可能是一个局部最小点，如上图（右）所示。这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。 下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J： 下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。 一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。 用更简单的数学语言进行描述步骤2）是这样的： 51. 梯度下降法找到的一定是下降最快的方向么？机器学习 ML基础 中 梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（当然高维问题不能叫平面）上下降最快的方向。在practical implementation中，牛顿方向（考虑海森矩阵）才一般被认为是下降最快的方向，可以达到superlinear的收敛速度。梯度下降类的算法的收敛速度一般是linear甚至sublinear的（在某些带复杂约束的问题）。by林小溪。一般解释梯度下降，会用下山来举例。假设你现在在山顶处，必须抵达山脚下（也就是山谷最低处）的湖泊。但让人头疼的是，你的双眼被蒙上了无法辨别前进方向。换句话说，你不再能够一眼看出哪条路径是最快的下山路径，如下图: 最好的办法就是走一步算一步，先用脚向四周各个方向都迈出一步，试探一下周围的地势，用脚感觉下哪个方向是下降最大的方向。换言之，每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向（当前最陡峭的位置向下）走一步。就这样，每要走一步都根据上一步所在的位置选择当前最陡峭最快下山的方向走下一步，一步步走下去，一直走到我们感觉已经到了山脚。 当然这样走下去，我们走到的可能并不一定是真正的山脚，而只是走到了某一个局部的山峰低处。换句话说，梯度下降不一定能够找到全局的最优解，也有可能只是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。 参考：一文清晰讲解机器学习中梯度下降算法（包括其变式算法） 52. 随机梯度下降 普通的梯度下降算法在更新回归系数时要遍历整个数据集，是一种批处理方法，这样训练数据特别忙庞大时，可能出现如下问题： 1）收敛过程可能非常慢； 2）如果误差曲面上有多个局极小值，那么不能保证这个过程会找到全局最小值。 为了解决上面的问题，实际中我们应用的是梯度下降的一种变体被称为随机梯度下降。 上面公式中的误差是针对于所有训练样本而得到的，而随机梯度下降的思想是根据每个单独的训练样本来更新权值，这样我们上面的梯度公式就变成了： 参考：梯度下降与随机梯度下降 53. 牛顿法和梯度下降法有什么不同。机器学习 ML基础 中 1）牛顿法（Newton’s method） 牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f(x)的泰勒级数的前面几项来寻找方程f(x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。 具体步骤： 首先，选择一个接近函数f(x)零点的x0，计算相应的f(x0)和切线斜率f’(x0)（这里f’表示函数f的导数）。然后我们计算穿过点(x0, f(x0))并且斜率为f ‘(x0)的直线和x轴的交点的x坐标，也就是求如下方程的解： x \cdot f'(x_0) + f(x_0) - x_0 \cdot f'(x_0) = 0 我们将新求得的点的ｘ坐标命名为x1，通常x1会比x0更接近方程f(x)=0的解。因此我们现在可以利用x1开始下一轮迭代。迭代公式可化简为如下所示： x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} 已经证明，如果f’是连续的，并且待求的零点x是孤立的，那么在零点x周围存在一个区域，只要初始值x0位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果f’(x)不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。 由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是”切线法”。牛顿法的搜索路径（二维情况）如下图所示： 关于牛顿法和梯度下降法的效率对比： a). 从收敛速度上看 ，牛顿法是二阶收敛，梯度下降是一阶收敛，前者牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是在局部上看的更细致，梯度法仅考虑方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。 b). 根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。 牛顿法的优缺点总结： 优点：二阶收敛，收敛速度快； 缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。 54. 什么是拟牛顿法（Quasi-Newton Methods）？机器学习 ML基础 中 @wtq1993，机器学习中常见的最优化算法 拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。 拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。 具体步骤： 拟牛顿法的基本思想如下。首先构造目标函数在当前迭代$x_k$的二次模型： 这里$B_K$是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点： x_{k+1} = X_k + \alpha_kp_k 其中我们要求步长$\alpha_k$，满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hessian矩阵Bk代替真实的Hessian矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵Bk的更新。现在假设得到一个新的迭代xk+1，并得到一个新的二次模型： m_{k+1}(p) = f(x_k + 1) + \delta f(x_{k + 1})^Tp + \frac{p^TB_{K+1}p}{2} 我们尽可能地利用上一步的信息来选取Bk。具体地，我们要求 \delta f(x_k + 1) - \delta f(x_k) = \alpha_kB_{k +1}p_k 从而得到 B_{k+1}(x_{k+1} - x_k) = \delta f(x_k + 1) - \delta f(x_k) 这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。 55. 请说说随机梯度下降法的问题和挑战？机器学习 ML基础 中 那到底如何优化随机梯度法呢？详情请点击：论文公开课第一期：详解梯度下降等各类优化算法（含视频和PPT下载）。 56. 说说共轭梯度法？机器学习 ML基础 中 共轭梯度法是介于梯度下降法（最速下降法）与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有逐步收敛性，稳定性高，而且不需要任何外来参数。 下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图： 注：绿色为梯度下降法，红色代表共轭梯度法 57. 对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法？机器学习 ML基础 中 @抽象猴，来源如果你是面试官，你怎么去判断一个面试者的深度学习水平？ 没有免费的午餐定理： 对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。 也就是说：对于所有问题，无论学习算法A多聪明，学习算法B多笨拙，它们的期望性能相同。 但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。 58. 什么最小二乘法？机器学习 ML基础 中 我们口头中经常说：一般来说，平均来说。如平均来说，不吸烟的健康优于吸烟者，之所以要加“平均”二字，是因为凡事皆有例外，总存在某个特别的人他吸烟但由于经常锻炼所以他的健康状况可能会优于他身边不吸烟的朋友。而最小二乘法的一个最简单的例子便是算术平均。 最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。用函数表示为： min_\vec(x) \sum_{i =1}^{n}(y_M - y_I)^2 由于算术平均是一个历经考验的方法，而以上的推理说明，算术平均是最小二乘的一个特例，所以从另一个角度说明了最小二乘方法的优良性，使我们对最小二乘法更加有信心。 最小二乘法发表之后很快得到了大家的认可接受，并迅速的在数据分析实践中被广泛使用。不过历史上又有人把最小二乘法的发明归功于高斯，这又是怎么一回事呢。高斯在1809年也发表了最小二乘法，并且声称自己已经使用这个方法多年。高斯发明了小行星定位的数学方法，并在数据分析中使用最小二乘方法进行计算，准确的预测了谷神星的位置。 对了，最小二乘法跟SVM有什么联系呢？请参见支持向量机通俗导论（理解SVM的三层境界）。 59. 看你T恤上印着：人生苦短，我用Python，你可否说说Python到底是什么样的语言？你可以比较其他技术或者语言来回答你的问题。Python Python语言 易 15个重要Python面试题 测测你适不适合做Python？ 这里是一些关键点： Python是解释型语言。这意味着不像C和其他语言，Python运行前不需要编译。其他解释型语言包括PHP和Ruby。 Python是动态类型的，这意味着你不需要在声明变量时指定类型。你可以先定义x=111，然后 x=”I’m a string”。 Python是面向对象语言，所有允许定义类并且可以继承和组合。Python没有访问访问标识如在C++中的public, private, 这就非常信任程序员的素质，相信每个程序员都是“成人”了~ 在Python中，函数是一等公民。这就意味着它们可以被赋值，从其他函数返回值，并且传递函数对象。类不是一等公民。 写Python代码很快，但是跑起来会比编译型语言慢。幸运的是，Python允许使用C扩展写程序，所以瓶颈可以得到处理。Numpy库就是一个很好例子，因为很多代码不是Python直接写的，所以运行很快。 Python使用场景很多 – web应用开发、大数据应用、数据科学、人工智能等等。它也经常被看做“胶水”语言，使得不同语言间可以衔接上。 Python能够简化工作，使得程序员能够关心如何重写代码而不是详细看一遍底层实现。 60. Python是如何进行内存管理的？ Python Python基础 中 @Tom_junsong，来源：2017 Python最新面试题及答案16道题 答:从三个方面来说,一对象的引用计数机制,二垃圾回收机制,三内存池机制 一、对象的引用计数机制 Python内部使用引用计数，来保持追踪内存中的对象，所有对象都有引用计数。 引用计数增加的情况： 1，一个对象分配一个新名称 2，将其放入一个容器中（如列表、元组或字典） 引用计数减少的情况： 1，使用del语句对对象别名显示的销毁 2，引用超出作用域或被重新赋值 sys.getrefcount( )函数可以获得对象的当前引用计数 多数情况下，引用计数比你猜测得要大得多。对于不可变数据（如数字和字符串），解释器会在程序的不同部分共享内存，以便节约内存。 二、垃圾回收 1，当一个对象的引用计数归零时，它将被垃圾收集机制处理掉。 2，当两个对象a和b相互引用时，del语句可以减少a和b的引用计数，并销毁用于引用底层对象的名称。然而由于每个对象都包含一个对其他对象的应用，因此引用计数不会归零，对象也不会销毁。（从而导致内存泄露）。为解决这一问题，解释器会定期执行一个循环检测器，搜索不可访问对象的循环并删除它们。 三、内存池机制 Python提供了对内存的垃圾收集机制，但是它将不用的内存放到内存池而不是返回给操作系统。 1，Pymalloc机制。为了加速Python的执行效率，Python引入了一个内存池机制，用于管理对小块内存的申请和释放。 2，Python中所有小于256个字节的对象都使用pymalloc实现的分配器，而大的对象则使用系统的malloc。 3，对于Python对象，如整数，浮点数和List，都有其独立的私有内存池，对象间不共享他们的内存池。也就是说如果你分配又释放了大量的整数，用于缓存这些整数的内存就不能再分配给浮点数。 61. 请写出一段Python代码实现删除一个list里面的重复元素。Python Python开发 中 1、使用set函数，set(list)； 2、使用字典函数：12345a=[1,2,4,2,4,5,6,5,7,8,9,0]b=&#123;&#125;b=b.fromkeys(a)c=list(b.keys())c 62. 编程用sort进行排序，然后从最后一个元素开始判断？Python Python开发 中12345678910a=[1,2,4,2,4,5,7,10,5,5,7,8,9,0,3]a.sort()last=a[-1]for i in range(len(a)-2,-1,-1): if last==a[i]: del a[i] else: last=a[i]print(a) 63. Python里面如何生成随机数？ ？Python Python开发 中 @Tom_junsong，2017 Python最新面试题及答案16道题 答：random模块 随机整数：random.randint(a,b)：返回随机整数x,a&lt;=x&lt;=b random.randrange(start,stop,[,step])：返回一个范围在(start,stop,step)之间的随机整数，不包括结束值。 随机实数：random.random( ):返回0到1之间的浮点数 random.uniform(a,b):返回指定范围内的浮点数。更多Python笔试面试题请看：很全的 Python 面试题 64. 说说常见的损失函数？机器学习 ML基础 易 对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致（要知道，有时损失或误差是不可避免的），用一个损失函数来度量预测错误的程度。损失函数记为L(Y, f(X))。 常用的损失函数有以下几种（基本引用自《统计学习方法》）： 65. 简单介绍下logistics回归？机器学习 ML模型 易 Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。 假设函数：h_{\theta }(x)=g(\theta ^T x)=\frac{1}{1+e^{-\theta ^{T}x}} 其中x是n维特征向量，函数g就是Logistic函数。而：$g(z)=\frac{1}{1+e^{-z}}$的图像是： 可以看到，将无穷映射到了(0,1)。 而假设函数就是特征属于y=1的概率。 P(y=1|x ;\theta )=h_{\theta }(x)；P(y=0|x ;\theta )=1-h_{\theta }(x) 66. 看你是搞视觉的，熟悉哪些CV框架，顺带聊聊CV最近五年的发展史如何？深度学习 DL应用 难 原英文：adeshpande3.github.io 作者：Adit Deshpande，UCLA CS研究生 译者：新智元闻菲、胡祥杰计算机视觉和 CNN 发展十一座里程碑 原论文：《CNN十篇经典论文》 本文结构如下： AlexNet（2012年） ZF Net（2013年） VGG Net（2014年） GoogLeNet （2014年） 微软 ResNet （2015年） 区域 CNN（R-CNN - 2013年，Fast R-CNN - 2015年，Faster R-CNN - 2015年 Mask R-CNN - 2017年） 生成对抗网络（2014年） 生成图像描述（2014年） 空间转化器网络（2015年） 67. 深度学习在视觉领域有何前沿进展？深度学习 DL应用 难 @元峰 本题解析来源：深度学习在计算机视觉领域的前沿进展 引言 在今年的神经网络顶级会议NIPS2016上，深度学习三大牛之一的Yann Lecun教授给出了一个关于机器学习中的有监督学习、无监督学习和增强学习的一个有趣的比喻，他说：如果把智能（Intelligence）比作一个蛋糕，那么无监督学习就是蛋糕本体，增强学习是蛋糕上的樱桃，那么监督学习，仅仅能算作蛋糕上的糖霜（图1）。 图1. Yann LeCun 对监督学习，增强学习和无监督学习的价值的形象比喻 1. 深度有监督学习在计算机视觉领域的进展 1.1 图像分类（Image Classification） 自从Alex和他的导师Hinton（深度学习鼻祖）在2012年的ImageNet大规模图像识别竞赛（ILSVRC2012）中以超过第二名10个百分点的成绩(83.6%的Top5精度)碾压第二名（74.2%，使用传统的计算机视觉方法）后，深度学习真正开始火热，卷积神经网络（CNN）开始成为家喻户晓的名字，从12年的AlexNet（83.6%），到2013年ImageNet 大规模图像识别竞赛冠军的ZFnet88.8%，再到2014年VGG的92.7%和同年的冠军GoogLeNet的93.3%，终于，到了2015年，在1000类的图像识别中，微软提出的残差网（ResNet）以96.43%的Top5正确率，达到了超过人类的水平（人类的正确率也只有94.9%）. Top5精度是指在给出一张图片，模型给出5个最有可能的标签，只要在预测的5个结果中包含正确标签，即为正确 图２. 2010-2015年ILSVRC竞赛图像识别错误率演进趋势 1.2 图像检测（Image Dection） 伴随着图像分类任务，还有另外一个更加有挑战的任务–图像检测，图像检测是指在分类图像的同时把物体用矩形框给圈起来。从14年到16年，先后涌现出R-CNN,Fast R-CNN, Faster R-CNN, YOLO, SSD等知名框架，其检测平均精度（mAP），在计算机视觉一个知名数据集上PASCAL VOC上的检测平均精度（mAP），也从R-CNN的53.3%，到Fast RCNN的68.4%，再到Faster R-CNN的75.9%，最新实验显示，Faster RCNN结合残差网（Resnet-101），其检测精度可以达到83.8%。深度学习检测速度也越来越快，从最初的RCNN模型，处理一张图片要用2秒多，到Faster RCNN的198毫秒/张，再到YOLO的155帧/秒（其缺陷是精度较低，只有52.7%），最后出来了精度和速度都较高的SSD，精度75.1%，速度23帧/秒。 图3. 图像检测示例 1.3 图像分割（Semantic Segmentation） 图像分割也是一项有意思的研究领域，它的目的是把图像中各种不同物体给用不同颜色分割出来（实例分割/语义分割），如下图所示，其平均精度（mIoU，即预测区域和实际区域交集除以预测区域和实际区域的并集），也从最开始的FCN模型（图像语义分割全连接网络，该论文获得计算机视觉顶会CVPR2015的最佳论文的）的62.2%，到DeepLab框架的72.7%，再到牛津大学的CRF as RNN的74.7%。该领域是一个仍在进展的领域，仍旧有很大的进步空间。 图4. 图像分割的例子 1.4 图像标注–看图说话（Image Captioning） 图像标注是一项引人注目的研究领域，它的研究目的是给出一张图片，你给我用一段文字描述它，如图中所示，图片中第一个图，程序自动给出的描述是“一个人在尘土飞扬的土路上骑摩托车”，第二个图片是“两只狗在草地上玩耍”。由于该研究巨大的商业价值（例如图片搜索），近几年，工业界的百度，谷歌和微软 以及学术界的加大伯克利，深度学习研究重地多伦多大学都在做相应的研究。 图5.图像标注，根据图片生成描述文字 1.5 图像生成–文字转图像（Image Generator） 图片标注任务本来是一个半圆，既然我们可以从图片产生描述文字，那么我们也能从文字来生成图片。如图6所示，第一列“一架大客机在蓝天飞翔”，模型自动根据文字生成了16张图片，第三列比较有意思，“一群大象在干燥草地行走”（这个有点违背常识，因为大象一般在雨林，不会在干燥草地上行走），模型也相应的生成了对应图片，虽然生成的质量还不算太好，但也已经中规中矩。 图6.根据文字生成图片 2. 强化学习（Reinforcement Learning） 在监督学习任务中，我们都是给定样本一个固定标签，然后去训练模型，可是，在真实环境中，我们很难给出所有样本的标签，这时候，强化学习就派上了用场。简单来说，我们给定一些奖励或惩罚，强化学习就是让模型自己去试错，模型自己去优化怎么才能得到更多的分数。2016年大火的AlphaGo就是利用了强化学习去训练，它在不断的自我试错和博弈中掌握了最优的策略。利用强化学习去玩flyppy bird，已经能够玩到几万分了。 图７. 强化学习玩flappy bird 谷歌DeepMind发表的使用增强学习来玩Atari游戏，其中一个经典的游戏是打砖块（breakout），DeepMind提出的模型仅仅使用像素作为输入，没有任何其他先验知识，换句话说，模型并不认识球是什么，它玩的是什么，令人惊讶的是，在经过240分钟的训练后，它不光学会了正确的接球，击打砖块，它甚至学会了持续击打同一个位置，游戏就胜利的越快（它的奖励也越高）。视频链接:Youtbe(需翻墙),优酷 强化学习在机器人领域和自动驾驶领域有极大的应用价值，当前arxiv上基本上每隔几天就会有相应的论文出现。机器人去学习试错来学习最优的表现，这或许是人工智能进化的最优途径，估计也是通向强人工智能的必经之路。 3. 深度无监督学习（Deep Unsupervised Learning）–预测学习 相比有限的监督学习数据，自然界有无穷无尽的未标注数据。试想，如果人工智能可以从庞大的自然界自动去学习，那岂不是开启了一个新纪元？当前，最有前景的研究领域或许应属无监督学习，这也正是Yann Lecun教授把无监督学习比喻成人工智能大蛋糕的原因吧。 深度学习牛人Ian Goodfellow在2014年提出生成对抗网络(GAN)后，该领域越来越火，成为16年研究最火热的一个领域之一。大牛Yann LeCun曾说：“对抗网络是切片面包发明以来最令人激动的事情。”这句话足以说明生成对抗网络有多重要。 生成对抗网络的一个简单解释如下：假设有两个模型，一个是生成模型（Generative Model，下文简写为G），一个是判别模型（Discriminative Model，下文简写为D），判别模型(D)的任务就是判断一个实例是真实的还是由模型生成的，生成模型(G)的任务是生成一个实例来骗过判别模型（D），两个模型互相对抗，发展下去就会达到一个平衡，生成模型生成的实例与真实的没有区别，判别模型无法区分自然的还是模型生成的。以赝品商人为例，赝品商人（生成模型）制作出假的毕加索画作来欺骗行家（判别模型D），赝品商人一直提升他的高仿水平来区分行家，行家也一直学习真的假的毕加索画作来提升自己的辨识能力，两个人一直博弈，最后赝品商人高仿的毕加索画作达到了以假乱真的水平，行家最后也很难区分正品和赝品了。下图是Goodfellow在发表生成对抗网络论文中的一些生成图片，可以看出，模型生成的模型与真实的还是有大差别，但这是14年的论文了，16年这个领域进展非常快，相继出现了条件生成对抗网络（Conditional Generative Adversarial Nets）和信息生成对抗网络（InfoGAN），深度卷积生成对抗网络（Deep Convolutional Generative Adversarial Network, DCGAN），更重要的是，当前生成对抗网络把触角伸到了视频预测领域，众所周知，人类主要是靠视频序列来理解自然界的，图片只占非常小的一部分，当人工智能学会理解视频后，它也真正开始显现出威力了。 图9 生成对抗网络生成的一些图片，最后边一列是与训练集中图片最相近的生产图片 这里推荐一篇2017年初Ian GoodFellow结合他在NIPS2016的演讲写出的综述性论文NIPS 2016 Tutorial: Generative Adversarial Networks 3.1条件生成对抗网络（Conditional Generative Adversarial Nets，CGAN） 生成对抗网络一般是根据随机噪声来生成特定类型的图像等实例，条件生成对抗网络则是根据一定的输入来限定输出，例如根据几个描述名词来生成特定的实例，这有点类似1.5节介绍的由文字生成图像，下图是Conditioanal Generative Adversarial Nets论文中的一张图片，根据特定的名词描述来生成图片。（注意：左边的一列图片的描述文字是训练集中不存在的，也就是说是模型根据没有见过的描述来生成的图片，右边的一列图片的描述是训练集中存在的） 图10. 根据文字来生成图片 条件生成对抗网络的另一篇有意思的论文是图像到图像的翻译pix2pix，该论文提出的模型能够根据一张输入图片，然后给出模型生成的图片，下图是论文中的一张图，其中左上角第一对非常有意思，模型输入图像分割的结果，给出了生成的真实场景的结果，这类似于图像分割的反向工程。 图11. 根据特定输入来生成一些有意思的输出图片 生成对抗网络也用在了图像超分辨率上，2016年有人提出SRGAN模型，它把原高清图下采样后，试图用生成对抗网络模型来还原图片来生成更为自然的，更逼近原图像的图像。下图中最右边是原图，把他降采样后采用三次差值（Bicubic Interpolation）得到的图像比较模糊，采用残差网络的版本（SRResNet）已经干净了很多，我们可以看到SRGAN生成的图片更为真实一些。 图12.生成对抗网络做超分辨率的例子，最右边是原始图像 生成对抗网络的另一篇有影响力的论文是深度卷积生成对抗网络DCGAN,作者把卷积神经网络和生成对抗网络结合起来，作者指出该框架可以很好的学习事物的特征，论文在图像生成和图像操作上给出了很有意思的结果，例如图13，带眼睛的男人-不戴眼镜的男人+不带眼睛的女人=带眼睛的女人,该模型给出了图片的类似向量化操作。 图13. DCGAN论文中的例图 生成对抗网络的发展是在是太火爆，一篇文章难以罗列完全，对此感兴趣的朋友们可以自己在网络搜素相关论文来研究 openAI的一篇描述生成对抗网络的博客非常棒，因为Ian Goodfellow就在OpenAI工作，所以这篇博客的质量还是相当有保障的。链接为：Open AI 生成对抗网络博客 3.2 视频预测 该方向是笔者自己最感兴趣的方向，Yann LeCun也提出，“用预测学习来替代无监督学习”,预测学习通过观察和理解这个世界是如何运作的，然后对世界的变化做出预测，机器学会了感知世界的变化，然后对世界的状态进行了推断。 今年的NIPS上，MIT的学者Vondrick等人发表了一篇名为Generating Videos with Scene Dynamics的论文,该论文提出了基于一幅静态的图片，模型自动推测接下来的场景，例如给出一张人站在沙滩的图片，模型自动给出一段接下来的海浪涌动的小视频。该模型是以无监督的方式，在大量的视频上训练而来的。该模型表明它可以自动学习到视频中有用的特征。下图是作者的官方主页上给出的图，是动态图，如果无法正常查看，请转入官方网站 MIT的CSAIL实验室也放出了一篇博客，题目是《教会机器去预测未来》,该模型在youtube视频和电视剧上（例如The Office和《绝望主妇》）训练，训练好以后，如果你给该模型一个亲吻之前的图片，该模型能自动推测出加下来拥抱亲吻的动作，具体的例子见下图。 图14. 给出一张静态图，模型自动推测接下来的动作 哈佛大学的Lotter等人提出了PredNet，该模型也是在KITTI数据集上训练,然后该模型就可以根据前面的视频，预测行车记录仪接下来几帧的图像，模型是用长短期记忆神经网络（LSTM）训练得到的。具体例子见下图,给出行车记录仪前几张的图片，自动预测接下来的五帧场景，模型输入几帧图像后，预测接下来的5帧，由图可知，越往后，模型预测的越是模糊,但模型已经可以给出有参加价值的预测结果了。图片是动图，如果无法正常查看，请访问论文作者的博客 图18. 给出行车记录仪前几张的图片，自动预测接下来的五帧场景,该图为动图 4. 总结 生成对抗网络，无监督学习视频预测的论文实在是太多，本人精力实在有限，对此感兴趣的读者可以每天刷一下arxiv的计算机视觉版块的计算机视觉和模型识别，神经网络和进化计算和人工智能等相应版块，基本上每天都有这方面新论文出现。图像检测和分割，增强学习，生成对抗网络，预测学习都是人工智能发展火热的方向，希望对深度学习感兴趣的我们在这方面能做出来点成果。谢谢朋友们的阅读，对深度无监督学习感兴趣的朋友，欢迎一起学习交流，请私信我。 5. 参考文献 在写本文的过程中，我尽量把论文网址以链接的形式附着在正文中.本文参考的大部分博客和论文整理如下，方便大家和自己以后研究查看。 参考博客: 【NIPS 主旨演讲】Yann LeCun：用预测学习替代无监督学习 计算机视觉和 CNN 发展十一座里程碑 Generative Models Generating Videos with Scene Dynamics Teaching machines to predict the future 参考论文: Resnet模型，图像分类，超过人类的计算机识别水平。Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification 图像检测 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks 图像分割Conditional Random Fields as Recurrent Neural Networks 图像标注，看图说话 Show and Tell: A Neural Image Caption Generator 文字生成图像Generative Adversarial Text to Image Synthesis 强化学习玩flyppy bird Using Deep Q-Network to Learn How To Play Flappy Bird 强化学习玩Atari游戏 Playing Atari with Deep Reinforcement Learning 生成对抗网络 Generative Adversarial Networks 条件生成对抗网络Conditional Generative Adversarial Nets 生成对抗网络做图像超分辨率Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network 深度卷积生成对抗网络Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks 由图片推演视频Generating Videos with Scene Dynamics 视频预测和无监督学习Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning 68. 在分类问题中，我们经常会遇到正负样本数据量不等的情况，比如正样本为10w条数据，负样本只有1w条数据，以下最合适的处理方法是( )？ 机器学习 ML基础 中 A 将负样本重复10次，生成10w样本量，打乱顺序参与分类 B 直接进行分类，可以最大限度利用数据 C 从10w正样本中随机抽取1w参与分类 D 将负样本每个权重设置为10，正样本权重为1，参与训练过程 数据不均衡处理方法： 1. 重采样 A可视作重采样的变形。改变数据分布消除不平衡，可能导致过拟合。 2. 欠采样 C的方案 提高少数类的分类性能，可能丢失多数类的重要信息。 如果1：10算是均匀的话，可以将多数类分割成为1000份。然后将每一份跟少数类的样本组合进行训练得到分类器。而后将这1000个分类器用assemble的方法组合位一个分类器。A选项可以看作此方式，因而相对比较合理。 另：如果目标是 预测的分布 跟训练的分布一致，那就加大对分布不一致的惩罚系数。 3. 权值调整 D方案也是其中一种方式。 ４. 损失函数 @管博士：准确的说，其实选项中的这些方法各有优缺点，需要具体问题具体分析，有篇文章对各种方法的优缺点进行了分析，讲的不错 感兴趣的同学可以参考一下：How to handle Imbalanced Classification Problems in machine learning?。 69. 深度学习是当前很热门的机器学习算法，在深度学习中，涉及到大量的矩阵相乘，现在需要计算三个稠密矩阵A,B,C的乘积ABC,假设三个矩阵的尺寸分别为m*n，n*p，p*q，且m&lt;n&lt;p&lt;q，以下计算顺序效率最高的是（）？深度学习 DL基础 中 A.(AB)C B.AC(B) C.A(BC) D.所以效率都相同 正确答案：A @BlackEyes_SGC： m*n*p&lt;m*n*q，m*p*q&lt; n*p*q, 所以 (AB)C 最小 首先，根据简单的矩阵知识，因为 A*B ， A 的列数必须和B的行数相等。因此，可以排除 B 选项， 然后，再看 A、C 选项。在A选项中，m*n矩阵A和n*p的矩阵B的乘积，得到m*p的矩阵 A*B ，而的每行n个元素需要n次乘法和n-1次加法，忽略加法，共需要n次乘法运算。同样情况分析 A*B之后再乘以C时的情况，共需要p次乘法运算。因此， A 选项 (AB)C 需要的乘法次数是n*p。同理分析， C选项 A (BC) 需要的乘法次数是 。由于，显然 A 运算次数更少，故选 A 。 70. Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是（）机器学习 ML模型 中 A.各类别的先验概率P(C)是相等的 B.以0为均值，sqr(2)/2为标准差的正态分布 C.特征变量X的各个维度是类别条件独立随机变量 D.P(X|C)是高斯分布 正确答案：C @BlackEyes_SGC：朴素贝叶斯的基本假设就是每个变量相互独立。 70. 关于支持向量机SVM,下列说法错误的是（） 机器学习 ML模型 中 A.L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力 B.Hinge损失函数，作用是最小化经验分类错误 C.分类间隔为1/||w||，||w||代表向量的模 D.当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习 正确答案：C @BlackEyes_SGC： A正确。考虑加入正则化项的原因：想象一个完美的数据集，y&gt;1是正类，y&lt;-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。 B正确。 C错误。间隔应该是2/||w||才对，后半句应该没错，向量的模通常指的就是其二范数。 D正确。考虑软间隔的时候，C对优化问题的影响就在于把a的范围从[0，+inf]限制到了[0,C]。C越小，那么a就会越小，目标函数拉格朗日函数导数为0可以求出w=求和ai∗yi∗xi，a变小使得w变小，因此间隔2/||w||变大 71. 在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计（）机器学习 ML模型 中 A.EM算法 B.维特比算法 C.前向后向算法 D.极大似然估计 正确答案：D @BlackEyes_SGC： EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法 维特比算法： 用动态规划解决HMM的预测问题，不是参数估计 前向后向算法：用来算概率 极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数 注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。 72. 假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是？机器学习 ML模型 中 A.这个被重复的特征在模型中的决定作用会被加强 B.模型效果相比无重复特征的情况下精确度会降低 C.如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样。 D.当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题 E.NB可以用来做最小二乘回归 F.以上说法都不正确 正确答案：BD @BlackEyes_SGC：NB的核心在于它假设向量的所有分量之间是独立的。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分 73. 以下哪些方法不可以直接来对文本分类？机器学习 ML模型 易 A、Kmeans B、决策树 C、支持向量机 D、KNN 正确答案: A分类不同于聚类。 @BlackEyes_SGC：A：Kmeans是聚类方法，典型的无监督学习方法。分类是监督学习方法，BCD都是常见的分类方法。 74. 已知一组数据的协方差矩阵P,下面关于主分量说法错误的是（）机器学习 ML基础 易 A、主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小 B、在经主分量分解后,协方差矩阵成为对角矩阵 C、主分量分析就是K-L变换 D、主分量是通过求协方差矩阵的特征值得到 解析：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。 @BlackEyes_SGC：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。 75. kmeans的复杂度？机器学习 ML模型 易 算法流程：选择k个质心作为初始质心 repeat 将每个点指派到最近的质心，形成k个簇 重新计算每个簇的质心 util 簇不再更新或者达到最大迭代次数 时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数，空间复杂度：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数,具体参考：机器学习之深入理解K-means、与KNN算法区别及其代码实现 76. 关于logit回归和SVM不正确的是（A） 机器学习 ML模型 中 A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。A错误 B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确 C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。 D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。 @BlackEyes_SGC：Logit回归目标函数是最小化后验概率，Logit回归可以用于预测事件发生概率的大小，SVM目标是结构风险最小化，SVM可以有效避免模型过拟合。的数目，m为记录数，n为维数 77. 输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为（） 深度学习 DL基础 中 A 95 B 96 C 97 D 98 E 99 F 100 正确答案：C @BlackEyes_SGC：计算尺寸不被整除只在GoogLeNet中遇到过。卷积向下取整，池化向上取整。 本题 （200-5+2 * 1）/2+1 为99.5，取99 （99-3）/1+1 为97 （97-3+2 * 1）/1+1 为97 研究过网络的话看到stride为1的时候，当kernel为 3 padding为1或者kernel为5 padding为2 一看就是卷积前后尺寸不变。 计算GoogLeNet全过程的尺寸也一样。 78. 影响聚类算法结果的主要因素有？ 机器学习 ML模型 易 A.已知类别的样本质量； B.分类准则； C.特征选取； D.模式相似性测度 79. 模式识别中，马式距离较之于欧式距离的优点是（C、D） 机器学习 ML模型 易 A.平移不变性； B.旋转不变性； C.尺度不变性； D.考虑了模式的分布 79. 影响基本K-均值算法的主要因素有(BD） 机器学习 ML模型 易 A.样本输入顺序； B.模式相似性测度； C.聚类准则； D.初始类中心的选取 80. 在统计模式分类问题中，当先验概率未知时，可以使用（BD） 机器学习 ML模型 易 A. 最小损失准则； B. 最小最大损失准则； C. 最小误判概率准则； D. N-P判决； @刘炫320，本题题目及解析来源：机器学习习题集 选项A，最小损失准则中需要用到先验概率 选项B，而最大最小损失规则主要就是使用解决最小损失规则时先验概率未知或难以计算的问题的。 选项D，在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。 1. p(y)已知，直接使用贝叶斯公式求后验概率即可； 2. p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。 聂曼-皮尔逊决策（N-P判决）可以归结为找阈值a，即： 如果，则x属于w1； 如果，则x属于w2； 81. 如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（BC） 机器学习 ML模型 易 A. 已知类别样本质量； B. 分类准则； C. 特征选取； D. 量纲； 82. 欧式距离具有（AB ）；马式距离具有（ABCD ）。机器学习 ML基础 易 A. 平移不变性； B. 旋转不变性； C. 尺度缩放不变性； D. 不受量纲影响的特性 83. 你有哪些deep learning（rnn、cnn）调参的经验？ 深度学习 DL基础 中 @萧瑟，来源你有哪些deep learning（rnn、cnn）调参的经验？ 参数初始化 下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。 下面的n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5 Xavier初始法论文：Understanding the difficulty of training deep feedforward neural networks He初始化论文：Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification 1. uniform均匀分布初始化：w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out]) Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n) He初始化，适用于ReLU：scale = np.sqrt(6/n) 2. normal高斯分布初始化：w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0 Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n) He初始化，适用于ReLU：stdev = np.sqrt(2/n) 3. svd初始化：对RNN有比较好的效果。参考论文：Exact solutions to the nonlinear dynamics of learning in deep linear neural networks 数据预处理方式 1. zero-center ,这个挺常用的.X -= np.mean(X, axis = 0) # zero-centerX /= np.std(X, axis = 0) # normalize 2. PCA whitening,这个用的比较少. 训练技巧 1. 要做梯度归一化,即算出来的梯度除以minibatch size 2. clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15 3. dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置.关于RNN如何用dropout,可以参考这篇论文:Recurrent Neural Network Regularization 4. adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。 5. 除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。 6. rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好. 7. word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果. 8. 尽量对数据做shuffle 9. LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:An Empirical Exploration of Recurrent Network Architectures, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值. 10. Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift 11. 如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: Highway Networks 12. 来自@张馨宇的技巧：一轮加正则，一轮不加正则，反复进行。 Ensemble Ensemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式 同样的参数,不同的初始化方式 不同的参数,通过cross-validation,选取最好的几组 同样的参数,模型训练的不同阶段，即不同迭代次数的模型。 不同的模型,进行线性融合. 例如RNN和传统模型. 更多深度学习技巧，请参见专栏：炼丹实验室 - 知乎专栏 84. 简单说说RNN的原理？深度学习 DL模型 中 我们升学到高三准备高考时，此时的知识是由高二及高二之前所学的知识加上高三所学的知识合成得来，即我们的知识是由前序铺垫，是有记忆的，好比当电影字幕上出现：“我是”时，你会很自然的联想到：“我是中国人”。 85. 什么是RNN？深度学习 DL模型 中 @一只鸟的天空，本题解析来源：循环神经网络(RNN, Recurrent Neural Networks)介绍 RNNs的目的使用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。RNNs之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。理论上，RNNs能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关，下图便是一个典型的RNNs： From Nature RNNs包含输入单元(Input units)，输入集标记为${x_0,x_1,…,x_t,x_{t+1},…}$，而输出单元(Output units)的输出集则被标记为${y_0,y_1,…,y_t,y_{t+1}.,..}$。RNNs还包含隐藏单元(Hidden units)，我们将其输出集标记为${s_0,s_1,…,s_t,s_{t+1},…}，这些隐藏单元完成了最为主要的工作。你会发现，在图中：有一条单向流动的信息流是从输入单元到达隐藏单元的，与此同时另一条单向流动的信息流从隐藏单元到达输出单元。在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”，并且隐藏层的输入还包括上一隐藏层的状态，即隐藏层内的节点可以自连也可以互连。 上图将循环神经网络进行展开成一个全神经网络。例如，对一个包含5个单词的语句，那么展开的网络便是一个五层的神经网络，每一层代表一个单词。对于该网络的计算过程如下： a). $x_t$表示第$t,t=1,2,3…t$步(step)的输入。比如，$x_1$为第二个词的one-hot向量(根据上图，$x_0￥为第一个词)； PS：使用计算机对自然语言进行处理，便需要将自然语言处理成为机器能够识别的符号，加上在机器学习过程中，需要将其进行数值化。而词是自然语言理解与处理的基础，因此需要对词进行数值化，词向量(Word Representation，Word embeding)[1]便是一种可行又有效的方法。何为词向量，即使用一个指定长度的实数向量v来表示一个词。有一种种最简单的表示方法，就是使用One-hot vector表示单词，即根据单词的数量|V|生成一个|V| * 1的向量，当某一位为一的时候其他位都为零，然后这个向量就代表一个单词。缺点也很明显： 1. 由于向量长度是根据单词个数来的，如果有新词出现，这个向量还得增加，麻烦！(Impossible to keep up to date); 2. 主观性太强(subjective) 3. 这么多单词，还得人工打labor并且adapt，想想就恐 4. 最不能忍受的一点便是很难计算单词之间的相似性。 现在有一种更加有效的词向量模式，该模式是通过神经网或者深度学习对词进行训练，输出一个指定维度的向量，该向量便是输入词的表达。如word2vec。 b). $s_t$为隐藏层的第t步的状态，它是网络的记忆单元。 $s_t$根据当前输入层的输出与上一步隐藏层的状态进行计算。$st=f(U_{x_t}+W_{s_t−1)}，其中$f$一般是非线性的激活函数，如tanh或ReLU，在计算$s_0$时，即第一个单词的隐藏层状态，需要用到s_{−1}，但是其并不存在，在实现中一般置为0向量； c). $o_t$是第t步的输出，如下个单词的向量表示，$o_t=softmax(V_{s_t}). 更多请看此文：循环神经网络(RNN, Recurrent Neural Networks)介绍。 86. RNN是怎么从单层网络一步一步构造的？深度学习 DL模型 难 @何之源，本题解析来源：完全图解RNN、RNN变体、Seq2Seq、Attention机制 [译] 理解 LSTM 网络 87. RNN中只能采用tanh而不是ReLu作为激活函数么？深度学习 DL模型 中 RNN中为什么要采用tanh而不是ReLu作为激活函数？ 88. 深度学习（CNN RNN Attention）解决大规模文本分类问题。深度学习 DL应用 难 用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践 89. 如何解决RNN梯度爆炸和弥散的问题？深度学习 DL模型 难 本题解析来源：深度学习与自然语言处理(7)_斯坦福cs224d 语言模型，RNN，LSTM与GRU 为了解决梯度爆炸问题，Thomas Mikolov首先提出了一个简单的启发性的解决方案，就是当梯度大于一定阈值的的时候，将它截断为一个较小的数。具体如算法1所述： 算法1：当梯度爆炸时截断梯度（伪代码） \hat{g} \leftarrow \frac{\partial E}{\partial W} \\ if \ \left \| \hat{g} \right \| \geq threshold \ then \\ \hat{g} \leftarrow \frac{threashold}{ \left\| \hat{g} \right\|}\hat{g} 下图可视化了梯度截断的效果。它展示了一个小的rnn（其中W为权值矩阵，b为bias项）的决策面。这个模型是一个一小段时间的rnn单元组成；实心箭头表明每步梯度下降的训练过程。当梯度下降过程中，模型的目标函数取得了较高的误差时，梯度将被送到远离决策面的位置。截断模型产生了一个虚线，它将误差梯度拉回到离原始梯度接近的位置。 为了解决梯度弥散的问题，我们介绍了两种方法。第一种方法是将随机初始化$W^{(hh)}改为一个有关联的矩阵初始化。第二种方法是使用ReLU（Rectified Linear Units）代替sigmoid函数。ReLU的导数不是0就是1.因此，神经元的梯度将始终为1，而不会当梯度传播了一定时间之后变小。 90. 如何理解LSTM网络。深度学习 DL模型 难 @Not_GOD，本题解析来源：[译] 理解 LSTM 网络 91. 当机器学习性能遭遇瓶颈时，你会如何优化的？机器学习 ML应用 难 可以从这4个方面进行尝试：、基于数据、借助算法、用算法调参、借助模型融合。当然能谈多细多深入就看你的经验心得了。这里有一份参考清单：机器学习性能改善备忘单。 92. 如何提高深度学习的性能？深度学习 DL应用 难 机器学习系列(10)_如何提高深度学习(和机器学习)的性能 1. 通过数据提升性能 a). 获取更多数据 b). 创造更多数据 c). 重放缩你的数据 d). 转换你的数据 e). 特征选取 f). 重架构你的问题 2. 通过算法提升性能 a). 对算法进行抽样调查 b). 借鉴已有文献 c). 重采样方法 3. 通过算法调参提升性能 a). 诊断（Diagnostics） b). 权重初始化（Weight Initialization） c). 学习速率（Learning Rate） d). 激活函数 e). 网络拓扑（Network Topology） f). 批次和周期（Batches and Epochs） g). 正则化 h). 优化和损失 i). 早停法 4. 通过嵌套模型提升性能 a). 组合模型 b). 组合视角 c). 堆叠（Stacking） 93. 什麽样的资料集不适合用深度学习？深度学习 DL应用 难 @抽象猴，来源：如果你是面试官，你怎么去判断一个面试者的深度学习水平？ 1. 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。 2. 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。 94. 广义线性模型是怎被应用在深度学习中？深度学习 DL模型 中 @许韩，来源：如果你是面试官，你怎么去判断一个面试者的深度学习水平？ 1. A Statistical View of Deep Learning (I): Recursive GLMs 2. 深度学习从统计学角度，可以看做递归的广义线性模型。 3. 广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。 4. 深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。下图是一个对照表： 95. 准备机器学习面试应该了解哪些理论知识？机器学习 ML模型 中 @穆文，来源：如果你是面试官，你怎么去判断一个面试者的深度学习水平？ 1. 【理论功底】主要考察对机器学习模型的理解，选择性提问（如果遇到面试者的研究方向自己不了解但感兴趣，会很欣喜，可以趁机学习一个哈哈）这块儿的问题会比较细碎，都是我自己深入思考过的（背书是没用的，这里任何一个点我都可以给你展开问下去），在此全部手敲 a). 过拟合欠拟合（举几个例子让判断下，顺便问问交叉验证的目的、超参数搜索方法、EarlyStopping）、L1正则和L2正则的做法、正则化背后的思想（顺便问问BatchNorm、Covariance Shift）、L1正则产生稀疏解原理、逻辑回归为何线性模型（顺便问问LR如何解决低维不可分、从图模型角度看LR和朴素贝叶斯和无监督）、几种参数估计方法MLE/MAP/贝叶斯的联系和区别、简单说下SVM的支持向量（顺便问问KKT条件、为何对偶、核的通俗理解）、 GBDT随机森林能否并行（顺便问问bagging boosting）、 生成模型判别模型举个例子、聚类方法的掌握（顺便问问Kmeans的EM推导思路、谱聚类和Graph-cut的理解）、梯度下降类方法和牛顿类方法的区别（顺便问问Adam、L-BFGS的思路）、半监督的思想（顺便问问一些特定半监督算法是如何利用无标签数据的、从MAP角度看半监督）、常见的分类模型的评价指标（顺便问问交叉熵、ROC如何绘制、AUC的物理含义、类别不均衡样本） b). CNN中卷积操作和卷积核作用、maxpooling作用、卷积层与全连接层的联系、梯度爆炸和消失的概念（顺便问问神经网络权值初始化的方法、为何能减缓梯度爆炸消失、CNN中有哪些解决办法、LSTM如何解决的、如何梯度裁剪、dropout如何用在RNN系列网络中、dropout防止过拟合）、为何卷积可以用在图像/语音/语句上（顺便问问channel在不同类型数据源中的含义） c). 如果面试者跟我一样做NLP、推荐系统，我会继续追问 CRF跟逻辑回归 最大熵模型的关系、CRF的优化方法、CRF和MRF的联系、HMM和CRF的关系（顺便问问 朴素贝叶斯和HMM的联系、LSTM+CRF 用于序列标注的原理、CRF的点函数和边函数、CRF的经验分布）、WordEmbedding的几种常用方法和原理（顺便问问language model、perplexity评价指标、word2vec跟Glove的异同）、topic model说一说、为何CNN能用在文本分类、syntactic和semantic问题举例、常见Sentence embedding方法、注意力机制（顺便问问注意力机制的几种不同情形、为何引入、seq2seq原理）、序列标注的评价指标、语义消歧的做法、常见的跟word有关的特征、factorization machine、常见矩阵分解模型、如何把分类模型用于商品推荐（包括数据集划分、模型验证等）、序列学习、wide&amp;deep model（顺便问问为何wide和deep) 2. 【代码能力】主要考察实现算法和优化代码的能力，我一般会先看面试者的github repo（如果简历给出来），看其代码风格、架构能力（遇到大神会认真学习一个哈哈），如果没有github，我会避免问典型的应试题，而是问一些我本人从实际问题中抽象出的小算法题，比如： a). 给出节点的矩阵和边的矩阵，求路径和最大的路径（来源于Viterbi算法，本质就是个动态规划），至少给个思路和伪代码（顺便聊聊前向传播和反向传播） b). 给出一数组，数组元素是pair对儿，表示一个有向无环图的&lt;父亲节点, 孩子节点&gt;，用最优的方法，将其变成一个新的有序数组，数组元素是该有向无环图所有节点，数组的有序性体现在：父亲节点在孩子节点前面（来源于 贝叶斯网络实现时的小trick） 3. 【项目能力】主要考察解决实际问题的思路、填坑能力，这部分最考验面试官功底，要能从面试者浮夸的描述中寻找有意义的点，并一步步深挖。另外很多dirty work(数据预处理、文本清洗、调参经验、算法复杂度优化、Bad case分析、修改损失函数等)也是在这步深挖。 96. 标准化与归一化的区别？机器学习 ML基础 易 @艾华丰，本题解析来源：标准化和归一化什么区别？ 归一化方法： 1、把数变为（0，1）之间的小数主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。 2、把有量纲表达式变为无量纲表达式 归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。 标准化方法： 数据的标准化是将数据按比例缩放，使之落入一个小的特定区间。由于信用指标体系的各个指标度量单位是不同的，为了能够将指标参与评价计算，需要对指标进行规范化处理，通过函数变换将其数值映射到某个数值区间。 97. 随机森林如何处理缺失值？机器学习 ML模型 中 方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。 方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似12。 98. 随机森林如何评估特征重要性？机器学习 ML模型 中 衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy： 1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。 2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。 99. 优化Kmeans？机器学习 ML模型 中 使用kd树或者ball tree 将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。 100. KMeans初始类簇中心点的选取。机器学习 ML模型 中 k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。 1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心 2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) 3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 4. 重复2和3直到k个聚类中心被选出来 5. 利用这k个初始的聚类中心来运行标准的k-means算法 见下一篇：BAT机器学习面试题库(101-200)]]></content>
      <tags>
        <tag>面试</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018搜狐内容识别算法大赛总结]]></title>
    <url>%2F2018%2F06%2F20%2F2018%E6%90%9C%E7%8B%90%E5%86%85%E5%AE%B9%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一. 前言 由于参加了搜狐的第一届算法大赛，所以对第二届的竞赛还是比较关注和期待的。第二届是关于新闻内容（文本和图片）识别：是否为营销文章（0，非营销；1，部分营销；2，完全营销）。营销大部分根据文本判定，少部分根据图片判别。具体可见官网。该方案只考虑了文本的识别，最终名次：第七名。 二. 代码说明 针对该比赛，我们使用的是python3和java8的语言环境，代码中主要涉及的包有：numpy、pandas、sklearn、gensim、tensorflow、keras、xgboost、HanLP。代码中主要分为特征提取、常用工具与模型构建的三类文件，下面对各代码文件作简要说明： data目录： 数据及模型存储路径。 keras_retinanet目录：目标检测算法retinanet代码目录（最终未采用）。 utils目录：HanLP工具java代码，包含分词代码与词性特征提取代码，由于java代码分词速度远比pyhanlp快，最终采用java代码做分词和词性特征提取。 preprocess.py：主要实现对数据进行预处理：过滤只有图片没有片段的1类新闻、数据清洗、文本分词（jieba和HanLP）。 feat_TF-IDF_stack.py：TF-IDF stacking特征生成，包含TF-IDF特征提取，各基础模型（LogisticRegression、MultinomialNB、BernoulliNB）的5折训练及训练集特征提取，测试集stacking特征提取。每个模型输出[num_samples x 3]的特征。 feat_nature_stack.py：词性stacking特征生成 ，根据java代码输出的词性统计特征，运用各基础模型（LogisticRegression，MultinomialNB，BernoulliNB）对训练集做词性stacking特征提取，测试集stacking特征提取。每个模型输出[num_samples x 3]的特征。 feat_w2v_model.py：词向量特征提取提取，输出包括：[num_samples x 300]的求和词向量，[num_samples x 300]的均值词向量以及TF-IDF与词向量的权值词向量。最终采用均值词向量作为文本的词向量特征。 feat_d2v_model.py：Doc2vec stacking特征，主要提取了Doc2vec两种训练方法的特征，运用神经网络分别做5折stacking，为训练集和测试集提取两类[num_samples x 3]的stacking特征。 feat_statistic.py：统计特征提取，主要包含：标题及文本长度、图片数量、文本中关于钱出现的次数、网站出现次数、电话出现次数、日期出现次数、营销词汇出现次数等各类统计特征。 get_segs.py： 为预测为1类的新闻添加营销片段。 predict.py：加载模型预测并输出提交结果文件。 tools_py3.py：主办方提供的tool.py对应的python3版本，来及群内分享。 param.py：各类参数配置代码：jieba、hanlp分词工具选择，是否去停词，是否训练，各类数据、模型保存路径。 utils.py：主要工具代码：日志打印、数据读取、去除BOM、sklearn模型的加载与保存、评估函数 STFIWF.py：改进的TF-IDF特征提取代码，考虑标签与词频的关系，主要参考自[here]。 detect.py：图片二维码、小程序、电话、网址检测代码（最终未采用）。 xgb_blending.py模型线下验证、模型训练代码。 三. 最优结果生成文档1). 模型说明 分类模型：我们主要运用了四类基础模型对部分特征做stacking，输出若干个[num_samples x 3]的stacking特征，拼接 上均值词向量特征与文本统计特征，用于最后xgb模型训练。针对TF-IDF与词性这类稀疏特征，我们采用LogisticRegression、MultinomialNB和BernoulliNB的基础模型，而对于Doc2vec的dbow与dm特征，我们使用keras搭建的神经网络模型。最后参数抖动训练五个xgb模型，将5个xgb模型的输出结果求均值作为最后的输出结果，按照概率最大得到最终类别预测结果。 片段提取：对于营销片段提取，我们采用规则匹配的方法。即：针对，预测为1类的新闻文本，直接判断测试集中新闻文本是否包含训练集中的营销片段，若能匹配到，则提取作为营销片段的预测输出。 模型结构如下：模型结构图 2). 特征说明 我们从 ５个方面对新闻文本进行特征构建，分别是：基于TF-IDF的特征、基于词性的特征、基于 Word2Vec 的特征、基于 Doc2Vec 的特征和文本统计特征。 基于TF-IDF的特征 TF-IDF（term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf 是一种统计方法，用以评估一字词对于一个文档集或一个语料库中的其中一份文档的重要程度。字词的重要性随着它在文档中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 TF-IDF的不足在于：单词在文档中的出现次数TF对于整体权重影响过大；没有考虑到单词在不同类间的分布差异。于是，我们没有采用sklearn中的TfidfVectorizer 提取特征，而是参考[here]使用改进后的TfidfVectorizer 提取文本TF-IDF特征。但是TF-IDF属于高维稀疏向量，直接使用树模型进行训练不仅速度较慢且效果不一定很好。于是，我们采用了一种比较流行的做法，就是做一层 stacking，将高维稀疏特征转化为低维稠密向量。在这里我们尝试了很多个基础模型，最终使用了LogisticRegression，MultinomialNB和BernoulliNB。 基于词性的特征 我们使用了HanLP汉语言处理工具包提取了文本的词性特征，共148类，并统计各类词性出现的次数与比例、以及整体的信息熵。词性统计特征同样拥有TF-IDF稀疏的特性，于是我们采用了同样的三个基模型对词性统计特征做了一层stacking。 基于 Word2Vec 的特征 Word2Vec 方法，可以将词语直接表示成一个固定长度的向量。对于Word2Vec特征，我们选定的维数为300维，并将训练数据（打标数据与未打标数据）中词频低于5的词语过滤掉。一篇文档的词向量特征，我们尝试了对每个词求和、求均值、基于TF-IDF加权的操作。最终由于词向量求和效果不好，加权操作计算量过大而选择均值词向量作为文档的Word2Vec 特征向量。 基于 Doc2Vec 的特征 使用 Doc2Vec 方法，可以将文档直接表示成一个固定长度的向量。根据训练文档向量的网络结构的不同，可以分为 Distributed Memory（DM）与 Distributed Bag of Words（DBOW）两种模型。其中 DM 模型不仅考虑了词的上下文语义特征，还考虑到了词序信息。DBOW 模型则忽略了上下文词序信息，而专注于文档中的各个词的语义信息。我们同时采用了 DBOW 和 DM 这两种模型构建文档向量，希望能够保留文档中完整的信息。 我们选择Doc2Vec的维数为300，并采用迭代的训练方法，进行多次重复训练，每一次都需要对训练数据（打标数据与未打标数据）重新打乱，以提高分类精度，DBOW 模型的迭代次数为 5，DM 模型的迭代次数为 10。我们对这两类文档向量分别做了一层 stacking，使用了一个简易的神经网络模型，只有一层 300 维的隐含层，进行训练并构建下一层模型需要的特征。 文本统计特征 本次竞赛要求尽可能识别营销类别的文章，于是营销词汇如：“优惠”、“特惠”、“加盟”、“利润空间大”、“免费”、“实惠””……；同时，我们发现1类文章多为转载文章，片段中的词汇如：“来源”、“转载”、“出自”，“版权”……；这些词汇对文本分类会起着重要的作用。我们基于训练数据建立了两个字典，并统计标题、文本中出现第一类词汇的次数，内容文本最后一句中第二类词汇出现的次数。另外还统计了文本长度、标题长度、图片数量、关于钱的内容、时间、日期、微信、QQ、ID等各类特征。3). 提升改进 更加细致的数据清洗工作，运用正则表达式做各种文本替换:时间、日期、电话、网址、微信、QQ、ID、关于钱的内容、打折信息、【】《》里的内容等。 选择更好的基础模型，在若干个基础模型中选择了较优的几个模型。 修改、增加部分统计信息。 四. 值得学习的方案 第一名开源方案 2017搜狐图文匹配算法大赛精彩回顾直击搜狐内容识别算法总决赛]]></content>
      <tags>
        <tag>比赛</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+github搭建博客及相关坑位]]></title>
    <url>%2F2018%2F05%2F21%2Fhexo%2Bgithub%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%8F%8A%E7%9B%B8%E5%85%B3%E5%9D%91%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[一. 前言 CS专业七年了，学得很多东西没怎么记录，一晃就忘得差不多了。 觉得很不错的大佬文章这里收藏一点，那里收藏一点，也没有一个统一的记录方式。 想通过这个个人博客写点东西，记录自己的成长过程和一些收获。 浏览了网上很多资源，学着用hexo和github搭建自己的博客，该文记录本博客的搭建过程。 参考链接：我是如何利用Github Pages搭建起我的博客，细数一路的坑Hexo + yilia 搭建博客可能会遇到的所有疑问Hexo+Github实现相册功能 Hexo + yilia 主题实现文章目录号外号外！解决github+hexo+yilia评论插件的问题！！！hexo 博客小功能添加-评论、萌妹纸、相册、字数统计… 二、所需要的环境安装Node.js()http://nodejs.cn/download/配置环境变量，windows下安装配置环境，一键完成很简单。 安装git（必须）可以查看廖雪峰老师的git教程https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000学习安装下载地址：https://git-for-windows.github.io/注册github账号:https://github.com/配置SSH-key:https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/001374385852170d9c7adf13c30429b9660d0eb689dd43a000。创建名为userName.github.io的仓库,userName是你申请的用户名。 三.安装hexo安装Hexo: 右键git bash,输入npm install -g hexo初始化Hexo: 自定义创建一个文件夹（用来放置你的博客）,点击该文件夹,输入 &gt; hexo init3.然后生成部署文件，启动本地服务 hexo g 或者hexo generate hexo s 或者hexo server，可以在http://localhost:4000/ 查看 至此hexo已经安装完毕了,我安装过程中出现了以下问题： 在hexo install过程中可能出现npm WARN deprecated swig@1.4.2: This package is no longer maintained只是说这个包不再维护了，后面好像并不影响使用，当然你也可以npm -g install npm，更新安装。 四、布置博客熟悉git的简单操作选中你的文件夹 git init ，创建仓库git add.``git commit -m”标记”，提交文件修改 git remote add origin https://github.com/Scyzin/scyzin.github.io.git 与你的github仓库连接git push -u origin master，将你的修改上传到github上。博客主题更换hexo默认的主题是landscape，在themes文件夹下，可以使用别人开发好的主题，这里有很多，我使用的是这一个: https://github.com/litten/hexo-theme-yilia下载之后将文件夹整个复制到D:\blog\themes目录下，再修改D:\blog 的下_config.yml文件里的theme：theme: hexo-theme-yilia。部署配置配置到github对应的仓库中: 1.为hexo安装git插件:npm install —save hexo-deployer-git，否则会出现 hexo d时会出现 ERROR Deployer not found: git2.修改D:\blog 的下_config.yml文件 deploy:type: gitrepo: https://github.com/Scyzin/scyzin.github.io.gitbranch: master加入如下配置：冒号后面都必须有空格。 Hexo配置文件说明:D:\blog`的下_config.yml常用的配置熟悉下就会了，这里可以选择学习一个扩展配置。http://blog.csdn.net/u013082989/article/details/70144934?locationNum=3&amp;fps=1hexo d，将博客发布在github.io上。至此你就可以直接通过scyzin.github.io访问你的博客。当然你也可以选择购买域名，绑定自己的地址访问。 五、 hexo常用命令hexo new “postName” #新建文章其中my new post为文章标题，执行命令后，会在项目\source_posts中生成my new post.md，用markdown编辑器打开编辑就行了。 当然，也可以直接在\source_posts中新建一个md文件，我就是这么做的。** hexo new page “pageName” 新建页面hexo generate 生成静态页面至public目录hexo server 开启预览访问端口（默认端口4000，’ctrl + c’关闭server）hexo deploy #将.deploy目录部署到GitHubhexo help 查看帮助简写:hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deployexo version #查看Hexo的版本 六、 些遇到的问题支持公式：在Hexo中渲染MathJax数学公式添加音乐： var ap = new APlayer({ element: document.getElementById("aplayer-Ihqtbjah"), narrow: false, autoplay: false, showlrc: false, music: { title: "她的睫毛", author: "周杰伦", url: "http://home.ustc.edu.cn/~mmmwhy/%d6%dc%bd%dc%c2%d7%20-%20%cb%fd%b5%c4%bd%de%c3%ab.mp3", pic: "http://home.ustc.edu.cn/~mmmwhy/jay.jpg", lrc: "" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap);]]></content>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马上金融---图像去污比赛总结]]></title>
    <url>%2F2018%2F05%2F18%2F%E9%A9%AC%E4%B8%8A%E9%87%91%E8%9E%8D---%E5%9B%BE%E5%83%8F%E5%8E%BB%E6%B1%A1%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、前言 前段时间参加了一个图像的比赛，起因于实验室老师在群里发了关于马上金融AI算法大赛的链接，从而得知马上金融这个比赛（后来公司也来学校宣讲了）。由于本人研究方向是图像处理与深度学习这一块，加上之前看过深度学习去雨的相关内容，想来也应该有很多相同之处，于是决定尝试一波。最后很意外地得到了第一名。 二、参赛方法与模型简介 图像修复是目前图像算法领域的重要问题之一，也是其他很多图像应用有效的预处理手段。图像修复的效果在某些场景下极大地影响着最终的图像识别结果，比如人脸识别，目标分类等图像应用。图像修复包括缺失图像补全，图像污迹去除，图像超分辨率等。本赛题要求针对图像污迹去除这一类问题，设计有效的算法，能够尽可能的去除图像上的网纹类污迹，还原真实的图像。 2012年，Hinton课题组运用构建的CNN网络AlexNet，首次参加ImageNet图像识别比赛，一举夺得冠军，且碾压第二名（SVM方法）的分类性能。从2012年至2016年，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件都在不断进步。正是由于该比赛，CNN吸引到了众多研究者的注意，再一次掀起了深度学习和人工智能的浪潮。同时，在计算机视觉及图像处理等领域，深度学习特别是CNN的运用，使得图像分类、目标检测、人脸识别等各项任务上都取得了突破性进展。 目前，在学术界，图像修复的主流研究方向仍是采用深度学习的方法。其中比较经典的算法包括：图像超分辨率方法SRCNN及基于上面改进的FSRCNN、ESPCN、VDSR等，图像着色方法Colorization，图像去噪方法DNCNN、IRCNN等，图像去马赛克方法，图像去模糊方法Robust Motion Deblurring、DeblurGAN等，缺失图像补全方法context-encoder、High-Res-Neural-Inpainting等。深度学习的方法可以运用GPU硬件加速，其处理速度也可能远比非深度学习方法快。由此可见，运用深度学习解决这类传统的图像处理问题不论在性能还是速度上仍然有一个很好的效果。 基于上述背景，我们主要采用深度学习的方法处理本赛题的图像污迹去除任务，运用多层卷积神经网络结构自动提取图像特征，建立网纹污迹与真实图像之间的映射关系。主要的技术创新点包括：（1）使用细节信息学习网纹图像与真实图像的残差；（2）通过图像预处理获得网纹先验知识；（3）结合先验信息，采用多任务联合训练的方式，优化网络参数。 三、参赛方法介绍 1. 数据预处理方法 根据主办方提供的数据集，我们将数据集分为了有网纹的图像集并放入input文件夹，干净图像并放入label文件夹，网纹图像与干净图像的mask文件放入map文件夹，于是得到了一张图片的三个文件存于三个文件夹。其中input与label由主办方提供，map中的mask文件由预处理所得，具体方法如下： a). 读取input与label中对应的图片文件，并将灰度图统一为三通道图像； b). 计算两者的差值； c). 二值化所得到的差值图像； d). 中值滤波去掉噪声。 经处理后，三个文件夹下分别包含10087张图片，其中的具体文件如图1所示。图1 训练集中样本 我们从10087组照片中，随机选择1500组（资源限制），每一组随机生成500个64x64的patch并存入h5文件用于模型的输入。 2. 参赛方法的理论原理介绍 a). 卷积神经网络 卷积神经网络(CNN)是为了识别二维形状而设计的多层感知器，具有局部感受、层次结构、特征提取和分类过程结合的全局训练的特点。这种结构的设计使其在图像发生移动、偏转、尺度缩放或者任何变化时能够保持高度不变性。现在卷积神经网络的主要应用领域有车牌识别、手写字体识别，人脸关键点的检测，文本分类等方向。 图2为经典的LeNet-5 模型的结构，可以对CNN 网络的整体架构有一个全局的了解，CNN 的网络搭建方式类型繁多，但都是由基本的卷积层(Convolutional Layer)、池化层(Pooling Layer)、激活层(Activation Layer)、全连接层(Fully Connected Layer)，目标函数(Objective Function)组合而成的。当然，我们并未使用到池化层与全连接层，而是加入了批规范化(Batch Normalization) 加速训练过程，下面详细地介绍参赛中所使用的技术原理。图2 LeNet-5网络结构 卷积层：卷积是图像处理领域中较为常见的一种操作，它可以看作是空间的一种线性操作，公式如式(1)所示： f(x,y)ⓧw(x,y)=∑_{s=-a}^a∑_{t=-b}^b w(s,t)f(x-s,y-t) 其中f(x,y)表示一幅图像的灰度值， w(s,t)表示一个滤波器，在CNN中被称为卷积核。假设m和n为奇数，则 a=(m-1)/2，b=(n-1)/2。图3展示了图像在CNN中的卷积操作。图3 卷积操作 CNN 网络中的每个卷积层通常是由很多个参数值不同的卷积核组成，通过卷积操作得到若干卷积特征图。在网络的同一层中，计算得到的特征图尺寸大小是一样的，而该特征图的大小Sout主要受上层的输出结果I、该层的卷积核尺寸K、滑动步长S和边界填0的数量P共4个因素共同影响。计算公式如下： S_{out}=(I-K+(2*P))/S+1 激活层：激活层主要使用激活函数将线性结果做非线性映射，提高模型非线性建模能力。激活函数必须满足非线性、可微性、单调性、有限输出值的范围的特性。常见的激活函数有sigmoid、tanh、ReLU等，其各自的函数形式及函数图像如下表所示：表1 常见激活函数 ReLU(Rectified Linear Unit)函数是目前比较常用的一个激活函数，相比于sigmod函数和tanh函数，它有以下几个优点： a). 在输入为正数的时候，不存在梯度饱和问题。 b). 计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多（sigmod和tanh要计算指数，计算速度会比较慢）。 批规范化：Batch Normalization的提出是为了克服深度神经网络难以训练的弊病，同时可以起到防止梯度弥散，加快训练速度，提高模型精度的作用。Batch Normalization算法流程如下： 首先，BN算法在每一次迭代中的每一层输入都进行了归一化，将输入数据的分布归一化为均值为0,方差为1的分布，如下式： \hat{x}^{(k)}=\frac {(x^k-E[x^k])} {\sqrt{Var[x^k]}} 其中，$x^k$为输入数据的第k维，$E[x^k]$表示该维度的平均值，$Var[x^k]$表示该维度的方差。 BN算法在第二步中设置了两个可学习的变量γ和β，然后用这两个可学习的变量去还原上一层应该学到的数据分布，使得第一步的归一化破坏掉的特征通过学习的参数γ和β去纠正从而得到真正学习的特征，保持模型的表达能力。 y^{(k)}=γ^k \hat{x}̂^{(k)}+β^{(k)} 这样BN就把原来不固定的数据分布全部转换为固定的数据分布，而这种数据分布恰恰就是要学习到的分布，从而加速了网络的训练表2 Batch Normalization算法的流程 b). 引导滤波 引导滤波(guided filter)算法属于可以保持边缘的一种滤波算法，算法在进行滤波时需要一幅引导图像，引导图像可以是另外单独的图像，也可以是输入图像本身，当引导图为输入图像本身时，引导滤波就成为一个保持边缘的滤波操作。引导滤波可以用于降噪、细节平滑、HDR压缩、抠图、去雾等方面。 引导滤波定义某像素点的输出结果为： q_i=a_k I_i+b_k 其中，q为输出图像，I为引导图像，a和b是当窗口中心位于k时该线性函数的不变系数。即该方法假定：q与I在以像素k为中心的窗口中存在局部线性关系。具体的推导过程可参考原论文。算法整体实现步骤如下： （1）利用boxFilter滤波器完成相关系数参数，其中均值包括引导图像均值、原始待滤波图像均值、互相关均值及自相关均值。 （2）根据均值计算相关系数参数，包括自相关方差var，互相关协方差cov。 （3）计算窗口线性变换参数系数a、b。 （4）根据公式计算参数a、b的均值。 （5）利用参数得到引导滤波输出图像q。 c). 参赛方法的具体实现 1) 模型采用的框架 我们采用google深度学习开源框架TensorFlow搭建网络模型，TensorFlow使用数据流图的形式进行计算，每个节点代表一个操作，节点之间通过多维数组（tensor）进行交互，是目前最受欢迎的深度学习开源框架之一。 2) 模型结构与分析 本次参赛模型主要用到的方法有：运用引导滤波获取网纹图像边缘细节信息，减少对于去网纹任务作用较少的信息；使用深度学习技术建立细节层与残差信息的关系映射；加入粗糙的二值mask信息，多任务联合训练判别条纹信息；参赛的模型结构如图4所示：图4 模型结构图 3) 模型的超参数设定与分析 该模型的深度为26，卷积核大小为3，优化算法使用Adam，batch size大小为20。训练过程中，我们设置的初始学习率为0.1，在每一轮迭代完之后，学习率乘0.1，总共epoch设置为3，即：第一个epoch，学习率为0.1，第二个epoch学习率为0.01，第三个epoch，学习率为0.001。对于两个分支的损失权重，我们尝试过2:1，或者初始权重2:1，然后每一轮迭代后不断降低mask分支的权重，但最终结果都没有刚开始的权重比1:1好。引导滤波中，滤波半径大小被设置为5，epsilon设置为1。 4) 模型的训练方式等 该模型训练过程中输入数据包括：网纹图像X，真实图像Y，mask图像M。对于真实图像输出分支，目标函数使用均方误差loss1；针对mask输出分支，我们使用交叉熵损失loss2。两者采用1:1的比例相加得到最后需要优化的损失函数loss，具体公式如式(5)： loss1 = \frac{1}{N}∑_{i=1}^N \| f(X_{i, detail},W,b) + X_i - Y_i \|^2 其中，N为训练图片数量，f(•)为卷积神经网络，W,b为网络要学习的参数，除了两个分支上的权重不同以外，其余权重两个分支权重共享，X_(i,detail)表示为第i张图片的细节层，通过引导滤波所得。 loss2=-\frac{1}{N} ∑_{i=1}^N M_i \log ⁡f(X_{i,detail},W,b) +(1-M_i) \log⁡ (1-f(X_{i,detail},W,b)) 模型的损失函数： loss=loss1+loss2四、参赛方法评估 1). 参赛模型的评测结果 评价指标：平均PSNR增益率计算公式如下： score= \frac{1}{N} ∑_{i=1}^N \frac{去网纹图PSNR-网纹图PSNR} {网纹图PSNR} 其中，PSNR计算公式如下： PSNR = 20 \cdot \log_{10}⁡ \frac{MAX_I}{\sqrt{MSE}} 其中，MAXI 图像像素最大值，MSE为处理后图像K与原图I的均方误差，大小为MxN，计算公式： MSE=\frac{1}{M×N} ∑_{i=0}^{M-1}∑_{j=0}^{N-1}‖I(i,j)-K(i,j)‖^2 评测结果：线上成绩: 1.20287 2). 参赛方法的创新性 a). 使用边缘细节信息作为网络的输入，而不是原图，减少了学习内容。 b). 学习图像残差信息，使得学习内容减少，模型收敛较快，易于训练。 c). 残差模块，充分利用各层的特征信息。 d). 加入mask信息，相当于增加条纹判断分支，多任务训练使网络学习更多特征。 3). 参赛模型的鲁棒性等 实验结果证明，该模型能够有效地去除网状条纹污迹，还原真实的图像，针对各类场景(背景单一，背景复杂、黑色背景等)都有很好的鲁棒性，以下为各类场景下的网纹去除结果样例。 五、参考文献 Removing rain from single images via a deep detail network DeMeshNet: Blind Face Inpainting for Deep MeshFace Verification Multi-task ConvNet for blind face inpainting with application to face verification]]></content>
      <tags>
        <tag>比赛</tag>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017搜狐图文匹配算法大赛总结]]></title>
    <url>%2F2017%2F06%2F18%2F2017%E6%90%9C%E7%8B%90%E5%9B%BE%E6%96%87%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、前言 爱搞事儿的师姐(”▽“)突然发来一条链接，问我要不要一起搞事儿参加个比赛，怀着初生牛犊不怕虎的想法，折腾就折腾吧，于是参加了搜狐图文匹配这个比赛，这也是搜狐第一届算法大赛。运气还不错，得了一个周冠军，全实验室吃了一顿大餐，最后也拿到一个还不错的名次：第4名 二、赛题介绍 (1). 组委会给定的搜狐新闻文本内容和相应的新闻配图等数据集来训练模型（数据集规模为10万条新闻和10万张新闻配图）。比赛要求在给定新的新闻内容集合和新的图片集合之后（数据集规模为2万条新闻和2万张新闻配图），参赛队伍能为每一篇新闻找到匹配度最高的10张图片，并且给出相应的排序,如下图：详情见官网。 (2). 评价指标为：NDCG 根据提供的答案，计算每条数据i的ndcg值ndcg（i），得分为: score = \frac{\sum_i^n dcg(i)}{n} 因此这里就需要： a.尽可能的将返回的10张图片为相关图片 b.保证每张图片得分最高 这样才能保证精准匹配，得分高。 三、解题思路 针对该题，我们采用推荐的思想。首先利用测试文本来去训练集找“最近”文本，采用TFIDF余弦距离计算文本距离，找出相似新闻的Top10，然后利用最相似训练样本的图片去搜索测试集中的候选图片，其中图片特征提取使用VGG16,图像相似度同样利用余弦距离。最后将搜索到的图片取Top1为测试文本的匹配图片。 (1). 简单预处理： 去除训练集、验证集、测试集中的无效图片。 (2). 处理过程 a.对文本进行分词，去停词等操作，构建词典，并计算文本的TFIDF向量； b.利用开源VGG16权重对训练集和测试集的图片进行特征提取，获取fc2层作为图片特征； c.计算文本Top1相似文本； d.计算相似文本对应图片的Top10图片作为预测结果。 四、其他方案 其他队伍方案：SOHU图文匹配竞赛-方案分享 四、总结展望 由于刚接触深度学习，自然语言处理这些新的方向，很多东西做的不够完善，没有尝试端到端的建模方法。同时，由于自身能力不足，很多想法没能实现。不过，能到决赛现场跟很多优秀的同学学习交流，知道了很多其他的方法，如：OCR、各种文本编码（TF-IDF、LSI、LDA）；对了，还有师兄强大的spark，能把我几十个小时的代码加速到1.5h。也见识了帝都的风貌（很遗憾有事儿先回了o(╥﹏╥)o），感觉运气很不错，很满意了。希望以后好好学习学习这方面的东西，不愧于研究生这三年。最后，附上视频与合照，以作纪念！！！ 2017搜狐图文匹配算法大赛精彩回顾 决赛合影留念]]></content>
      <tags>
        <tag>比赛</tag>
        <tag>总结</tag>
      </tags>
  </entry>
</search>
